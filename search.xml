<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>1.阿里云服务器搭建部署</title>
    <url>/2020/04/07/1.%E9%98%BF%E9%87%8C%E4%BA%91%E6%9C%8D%E5%8A%A1%E5%99%A8%E6%90%AD%E5%BB%BA%E9%83%A8%E7%BD%B2/</url>
    <content><![CDATA[<h2 id="4-1-阿里云-ECS-服务器准备"><a href="#4-1-阿里云-ECS-服务器准备" class="headerlink" title="4.1 阿里云 ECS 服务器准备"></a>4.1 阿里云 ECS 服务器准备</h2><h3 id="4-1-1-注册阿里云账户"><a href="#4-1-1-注册阿里云账户" class="headerlink" title="4.1.1 注册阿里云账户"></a>4.1.1 注册阿里云账户</h3><p>1）登录阿里云官方网站：<a href="https://cn.aliyun.com/" target="_blank" rel="noopener">https://cn.aliyun.com/</a></p>
<p><img src="https://leinuo-blog.oss-cn-beijing.aliyuncs.com/images/typora/1594018865359.png" alt="1594018865359"></p>
<p>2）点击免费注册，提示支付宝快捷注册和账号密码注册两种方式，任选其一。</p>
<h3 id="4-1-2-购买-ECS-云服务器"><a href="#4-1-2-购买-ECS-云服务器" class="headerlink" title="4.1.2 购买 ECS 云服务器"></a>4.1.2 购买 ECS 云服务器</h3><p>1） 进入阿里云首页</p>
<p><img src="https://leinuo-blog.oss-cn-beijing.aliyuncs.com/images/typora/1594019015393.png" alt="1594019015393"></p>
<p>2）打开侧边栏，点击云服务器 ECS</p>
<p><img src="https://leinuo-blog.oss-cn-beijing.aliyuncs.com/images/typora/1594019029747.png" alt="1594019029747"></p>
<p>3）点击立即购买</p>
<p><img src="https://leinuo-blog.oss-cn-beijing.aliyuncs.com/images/typora/1594019063204.png" alt="1594019063204"></p>
<p>4）选择计费方式、服务器区域</p>
<p><img src="https://leinuo-blog.oss-cn-beijing.aliyuncs.com/images/typora/1594019073788.png" alt="1594019073788"></p>
<p>5）选定服务器配置</p>
<p><img src="https://leinuo-blog.oss-cn-beijing.aliyuncs.com/images/typora/1594019083393.png" alt="1594019083393"></p>
<p>6）选定服务器系统</p>
<p><img src="https://leinuo-blog.oss-cn-beijing.aliyuncs.com/images/typora/1594019091359.png" alt="1594019091359"></p>
<p>7）选定磁盘类型及大小</p>
<p><img src="https://leinuo-blog.oss-cn-beijing.aliyuncs.com/images/typora/1594019098518.png" alt="1594019098518"></p>
<p>8）网络和安全组配置</p>
<p><img src="https://leinuo-blog.oss-cn-beijing.aliyuncs.com/images/typora/1594019105824.png" alt="1594019105824"></p>
<p><img src="https://leinuo-blog.oss-cn-beijing.aliyuncs.com/images/typora/1594020372395.png" alt="1594020372395"></p>
<p>9）系统配置</p>
<p><img src="https://leinuo-blog.oss-cn-beijing.aliyuncs.com/images/typora/1594020401191.png" alt="1594020401191"></p>
<p>10）分组设置，默认即可</p>
<p><img src="https://leinuo-blog.oss-cn-beijing.aliyuncs.com/images/typora/1594020426315.png" alt="1594020426315"></p>
<p>11）确认订单</p>
<p><img src="https://leinuo-blog.oss-cn-beijing.aliyuncs.com/images/typora/1594020437115.png" alt="1594020437115"></p>
<p>12）在控制台上查看创建的实例列表</p>
<p><img src="https://leinuo-blog.oss-cn-beijing.aliyuncs.com/images/typora/1594020799882.png" alt="1594020799882"></p>
<h3 id="4-1-3-配置防火墙（可选）"><a href="#4-1-3-配置防火墙（可选）" class="headerlink" title="4.1.3 配置防火墙（可选）"></a>4.1.3 配置防火墙（可选）</h3><p>我们需要将开发程序的端口允许外部访问。比如 HDFS 的 50070 端口、YARN 的 8088<br>端口等</p>
<p>1）进入设置页面</p>
<p><img src="https://leinuo-blog.oss-cn-beijing.aliyuncs.com/images/typora/1594020828875.png" alt="1594020828875"></p>
<p>2）点击配置规则</p>
<p><img src="https://leinuo-blog.oss-cn-beijing.aliyuncs.com/images/typora/1594020841321.png" alt="1594020841321"></p>
<p>3）点击添加安全组规则</p>
<p><img src="https://leinuo-blog.oss-cn-beijing.aliyuncs.com/images/typora/1594020852052.png" alt="1594020852052"></p>
<p>4）配置规则</p>
<p><img src="https://leinuo-blog.oss-cn-beijing.aliyuncs.com/images/typora/1594020864178.png" alt="1594020864178"></p>
<p>50070：代表允许外部访问 50070 端口。<br>0.0.0.0/0 代表所有主机都可以访问，实际应添加本机 IP。</p>
<p>5）需要添加的端口如下：</p>
<p><img src="https://leinuo-blog.oss-cn-beijing.aliyuncs.com/images/typora/1594020895835.png" alt="1594020895835"></p>
<p>说明：</p>
<p><img src="https://leinuo-blog.oss-cn-beijing.aliyuncs.com/images/typora/1594020943333.png" alt="1594020943333"></p>
<p>6）配置好后，返回控制台</p>
<p>点击左侧导航栏的 ECS-&gt;点击实例</p>
<p><img src="https://leinuo-blog.oss-cn-beijing.aliyuncs.com/images/typora/1594020961191.png" alt="1594020961191"></p>
<p><img src="https://leinuo-blog.oss-cn-beijing.aliyuncs.com/images/typora/1594020966336.png" alt="1594020966336"></p>
<h3 id="4-1-4-ECS-配置升级（可选）"><a href="#4-1-4-ECS-配置升级（可选）" class="headerlink" title="4.1.4 ECS 配置升级（可选）"></a>4.1.4 ECS 配置升级（可选）</h3><p>1）升级 hadoop102 配置（可选，如果配置够，就不需要升级）</p>
<p>（1）停止实例</p>
<p><img src="https://leinuo-blog.oss-cn-beijing.aliyuncs.com/images/typora/1594020990645.png" alt="1594020990645"></p>
<p>（2）更改实例规格</p>
<p><img src="https://leinuo-blog.oss-cn-beijing.aliyuncs.com/images/typora/1594020997912.png" alt="1594020997912"></p>
<p><img src="https://leinuo-blog.oss-cn-beijing.aliyuncs.com/images/typora/1594021001334.png" alt="1594021001334"></p>
<h3 id="4-1-5-阿里云服务器连接"><a href="#4-1-5-阿里云服务器连接" class="headerlink" title="4.1.5 阿里云服务器连接"></a>4.1.5 阿里云服务器连接</h3><p>打开远程连接工具进行配置，这里以 CRT 为例。</p>
<p>1）新建一个 Session</p>
<p><img src="https://leinuo-blog.oss-cn-beijing.aliyuncs.com/images/typora/1594021030824.png" alt="1594021030824"></p>
<p>2）根据阿里云公网 IP 地址，填写 HostName</p>
<p><img src="https://leinuo-blog.oss-cn-beijing.aliyuncs.com/images/typora/1594021054966.png" alt="1594021054966"></p>
<p><img src="https://leinuo-blog.oss-cn-beijing.aliyuncs.com/images/typora/1594021063661.png" alt="1594021063661"></p>
<p><img src="https://leinuo-blog.oss-cn-beijing.aliyuncs.com/images/typora/1594021068225.png" alt="1594021068225"></p>
<p><img src="https://leinuo-blog.oss-cn-beijing.aliyuncs.com/images/typora/1594021072522.png" alt="1594021072522"></p>
<p>3）输入密码，连接阿里云服务器</p>
<p><img src="https://leinuo-blog.oss-cn-beijing.aliyuncs.com/images/typora/1594021083677.png" alt="1594021083677"></p>
<p><img src="https://leinuo-blog.oss-cn-beijing.aliyuncs.com/images/typora/1594021088414.png" alt="1594021088414"></p>
<p><img src="https://leinuo-blog.oss-cn-beijing.aliyuncs.com/images/typora/1594021096773.png" alt="1594021096773"></p>
<p>4）重复以上 1-3 步，分别连接 hadoop103 和 hadoop104 两台服务器。<br>5）修改 CecureCRT 的 Session 编码方式（可选）<br>默认 CecureCRT 的 Session 编码方式为 default，当查看中文文件时，会显示乱码。需<br>要将编码方式修改为 UTF-8。</p>
<p><img src="https://leinuo-blog.oss-cn-beijing.aliyuncs.com/images/typora/1594021122798.png" alt="1594021122798"></p>
<p><img src="https://leinuo-blog.oss-cn-beijing.aliyuncs.com/images/typora/1594021126315.png" alt="1594021126315"></p>
<h3 id="4-2-基础-环境准备"><a href="#4-2-基础-环境准备" class="headerlink" title="4.2 基础 环境准备"></a>4.2 基础 环境准备</h3><h4 id="4-2-1-配置主机名称（可选）"><a href="#4-2-1-配置主机名称（可选）" class="headerlink" title="4.2.1 配置主机名称（可选）"></a>4.2.1 配置主机名称（可选）</h4><p>1）进入 Linux 系统查看本机的主机名。通过 hostname 命令查看</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@hadoop102 ~]# hostname</span><br><span class="line">hadoop102</span><br></pre></td></tr></table></figure>

<p>2）如果感觉此主机名不合适，我们可以修改。编辑/etc/sysconfig/network 文件</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@hadoop102 ~]# vi /etc/sysconfig/network</span><br><span class="line">文件中内容</span><br><span class="line">NETWORKING=yes</span><br><span class="line">HOSTNAME= hadoop102</span><br></pre></td></tr></table></figure>

<p>注意：主机名称不要有“_”下划线<br>3）修改此主机名为我们想要修改的主机名，如修改为 hadoop102 后，保存退出。<br>4）修改主机名称后，需要重启服务器，主机名才能生效。</p>
<h4 id="4-2-2-配置主机名称-映射"><a href="#4-2-2-配置主机名称-映射" class="headerlink" title="4.2.2 配置主机名称 映射"></a>4.2.2 配置主机名称 映射</h4><p>1）查看阿里云分配的私有 IP 地址</p>
<p><img src="https://leinuo-blog.oss-cn-beijing.aliyuncs.com/images/typora/1594021285020.png" alt="1594021285020"></p>
<p>2）根据阿里云分配的私有 IP 地址，配置主机名称映射，打开/etc/hosts</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@hadoop102 ~]# vim /etc/hosts</span><br></pre></td></tr></table></figure>

<p>添加如下内容后，保存并退出。<br>172.26.74.102 hadoop104 hadoop104<br>172.26.74.103 hadoop102 hadoop102<br>172.26.74.104 hadoop103 hadoop103</p>
<p>注意：这里每台服务器 IP 不一样，IP 填写的都是私有 IP。</p>
<p>3）分别在 hadoop103 和 hadoop104 两台服务器上配置主机名称映射</p>
<p>4.2.3 创建户 普通用户 atguigu<br>1）添加一个用户</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@hadoop102 ~]# useradd atguigu</span><br></pre></td></tr></table></figure>

<p>2）设置用户密码</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@hadoop102 ~]# passwd atguigu</span><br><span class="line">Changing password for user atguigu.</span><br><span class="line">New password:</span><br><span class="line">Retype new password:</span><br></pre></td></tr></table></figure>

<p>3）配置 atguigu 具有 root 权限</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@hadoop102 ~]# vim /etc/sudoers</span><br><span class="line">添加如下内容</span><br><span class="line">atguigu ALL=(ALL) ALL</span><br><span class="line">保存并退出 wq!</span><br></pre></td></tr></table></figure>

<p>4）重复步骤 1-3，分别在 hadoop103 和 hadoop104 服务器上添加新用户 atguigu，并初始化密码和配置 sudoers</p>
<h4 id="4-2-4-本-集群分发脚本-xsync"><a href="#4-2-4-本-集群分发脚本-xsync" class="headerlink" title="4.2.4 本 集群分发脚本 xsync"></a>4.2.4 本 集群分发脚本 xsync</h4><p>1）在/usr/local/bin 目录下，创建 xsync 文件</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[atguigu@hadoop102 bin]$ vi xsync</span><br></pre></td></tr></table></figure>

<p>在该文件中编写如下代码</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">!/bin/bash</span></span><br><span class="line"><span class="meta">#</span><span class="bash">1 获取输入参数个数，如果没有参数，直接退出</span></span><br><span class="line">pcount=$#</span><br><span class="line">if((pcount==0)); then</span><br><span class="line">echo no args;</span><br><span class="line">exit;</span><br><span class="line">fi</span><br><span class="line"><span class="meta">#</span><span class="bash">2 获取文件名称</span></span><br><span class="line">p1=$1</span><br><span class="line">fname=`basename $p1`</span><br><span class="line">echo fname=$fname</span><br><span class="line"><span class="meta">#</span><span class="bash">3 获取上级目录到绝对路径</span></span><br><span class="line">pdir=`cd -P $(dirname $p1); pwd`</span><br><span class="line">echo pdir=$pdir</span><br><span class="line"><span class="meta">#</span><span class="bash">4 获取当前用户名称</span></span><br><span class="line">user=`whoami`</span><br><span class="line"><span class="meta">#</span><span class="bash">5 循环</span></span><br><span class="line">for i in hadoop103 hadoop104</span><br><span class="line">do</span><br><span class="line">echo ------------------- $i --------------</span><br><span class="line">rsync -rvl $pdir/$fname $user@$i:$pdir</span><br><span class="line">done</span><br></pre></td></tr></table></figure>

<p>2）修改脚本 xsync 具有执行权限</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[atguigu@hadoop102 bin]$ chmod 777 xsync</span><br></pre></td></tr></table></figure>

<p>3）修改脚本 xsync 具有所有者和所有者所属组</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@hadoop102 bin]# chown atguigu:atguigu xsync</span><br></pre></td></tr></table></figure>

<p>4）调用脚本形式：xsync 文件名称</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@hadoop102 bin]# su atguigu</span><br><span class="line">[atguigu@hadoop102 ~]$ vim test.txt</span><br><span class="line">输入任意内容</span><br><span class="line">Hello</span><br><span class="line"></span><br><span class="line">[atguigu@hadoop102 bin]$ xsync test.txt</span><br></pre></td></tr></table></figure>

<h4 id="4-2-5-配置-SSH-无密登录"><a href="#4-2-5-配置-SSH-无密登录" class="headerlink" title="4.2.5 配置 SSH 无密登录"></a>4.2.5 配置 SSH 无密登录</h4><p>1）无密钥配置<br>（1）免密登录原理，如下图所示</p>
<p><img src="https://leinuo-blog.oss-cn-beijing.aliyuncs.com/images/typora/1594021569076.png" alt="1594021569076"></p>
<p>（2）生成公钥和私钥</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[atguigu@hadoop102 .ssh]$ ssh-keygen -t rsa</span><br></pre></td></tr></table></figure>

<p>然后敲（三个回车），就会生成两个文件 id_rsa（私钥）、id_rsa.pub（公钥）</p>
<p>（3）将公钥拷贝到要免密登录的目标机器上</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[atguigu@hadoop102 .ssh]$ ssh-copy-id hadoop102</span><br><span class="line">[atguigu@hadoop102 .ssh]$ ssh-copy-id hadoop103</span><br><span class="line">[atguigu@hadoop102 .ssh]$ ssh-copy-id hadoop104</span><br></pre></td></tr></table></figure>

<p>2）.ssh 文件夹下（~/.ssh）的文件功能解释</p>
<p><img src="https://leinuo-blog.oss-cn-beijing.aliyuncs.com/images/typora/1594021610044.png" alt="1594021610044"></p>
<h4 id="4-2-6-集群整体操作-脚本-（可选）"><a href="#4-2-6-集群整体操作-脚本-（可选）" class="headerlink" title="4.2.6 集群整体操作 脚本 （可选）"></a>4.2.6 集群整体操作 脚本 （可选）</h4><p>1）在/usr/local/bin 目录下创建脚本 xcall</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@hadoop102 bin]$ vim xcall</span><br></pre></td></tr></table></figure>

<p>2）在脚本中编写如下内容</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">! /bin/bash</span></span><br><span class="line">for i in hadoop102 hadoop103 hadoop104</span><br><span class="line">do</span><br><span class="line">echo --------- $i ----------</span><br><span class="line">ssh $i "source /etc/profile ; $*"</span><br><span class="line">done</span><br></pre></td></tr></table></figure>

<p>3）修改脚本执行权限</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@hadoop102 bin]$ chmod +x xcall</span><br></pre></td></tr></table></figure>

<p>4）修改脚本 xcall 具有所有者和所有者所属组</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@hadoop102 bin]# chown atguigu:atguigu xcall</span><br></pre></td></tr></table></figure>

<p>5）测试</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@hadoop102 bin]# xcall ls /home/atguigu</span><br></pre></td></tr></table></figure>

<h3 id="4-3-JDK-安装"><a href="#4-3-JDK-安装" class="headerlink" title="4.3 JDK 安装"></a>4.3 JDK 安装</h3><p>0）集群规划</p>
<p><img src="https://leinuo-blog.oss-cn-beijing.aliyuncs.com/images/typora/1594021710342.png" alt="1594021710342"></p>
<p>1）在 hadoop102 的/opt 目录下创建 module 和 software 两个文件夹。software 主要用于存储<br>上传到服务器的 JAR 包，module 主要用来安装项目相关软件。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@hadoop102 opt]# mkdir software</span><br><span class="line">[root@hadoop102 opt]# mkdir module</span><br><span class="line">[root@hadoop102 opt]# ll</span><br><span class="line">total 8</span><br><span class="line">drwxr-xr-x 2 root root 4096 Sep 29 10:59 module</span><br><span class="line">drwxr-xr-x 2 root root 4096 Sep 29 10:59 software</span><br></pre></td></tr></table></figure>

<p>2）修改 module 和 software 两个文件夹的所有者和所有者的组</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@hadoop102 opt]# chown atguigu:atguigu module/ software/</span><br></pre></td></tr></table></figure>

<p>3）切换到 atguigu 用户</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[root@hadoop102 ~]# su atguigu</span><br></pre></td></tr></table></figure>

<p>4）用 SecureCRT 工具将 JDK 导入到/opt/software 文件夹下面</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[atguigu@hadoop102 opt]$ cd software/</span><br><span class="line">[atguigu@hadoop102 software]$ ls</span><br><span class="line">jdk-8u144-linux-x64.tar.gz</span><br></pre></td></tr></table></figure>

<p>5）解压 JDK 到/opt/module 目录下</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[atguigu@hadoop102 software]$ tar -zxvf jdk-8u144-linux-</span><br><span class="line">x64.tar.gz -C /opt/module/</span><br></pre></td></tr></table></figure>

<p>6）配置 JDK 环境变量</p>
<p>（1）先获取 JDK 路径</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[atguigu@hadoop102 jdk1.8.0_144]$ pwd</span><br><span class="line">/opt/module/jdk1.8.0_144</span><br></pre></td></tr></table></figure>

<p>（2）打开/etc/profile 文件</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[atguigu@hadoop102 software]$ sudo vi /etc/profile</span><br></pre></td></tr></table></figure>

<p>在 profile 文件末尾添加 JDK 路径</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">JAVA_HOME</span></span><br><span class="line">export JAVA_HOME=/opt/module/jdk1.8.0_144</span><br><span class="line">export PATH=$PATH:$JAVA_HOME/bin</span><br></pre></td></tr></table></figure>

<p>（4）让修改后的文件生效</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[atguigu@hadoop102 jdk1.8.0_144]$ source /etc/profile</span><br></pre></td></tr></table></figure>

<p>7）测试 JDK 是否安装成功</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[atguigu@hadoop102 jdk1.8.0_144]# java -version</span><br><span class="line">java version "1.8.0_144"</span><br></pre></td></tr></table></figure>

<p>注意：重启（如果 java -version 可以用就不用重启）</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[atguigu@hadoop102 jdk1.8.0_144]$ sync</span><br><span class="line">[atguigu@hadoop102 jdk1.8.0_144]$ sudo reboot</span><br></pre></td></tr></table></figure>

<p>8）分别在 hadoop103 和 hadoop104 上创建 module 文件夹，并修改文件夹的所有者和所有者<br>组为 atguigu。参考步骤 1-2<br>9）同步 jdk1.8.0_144 到 hadoop103 和 hadoop104</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[atguigu@hadoop102 module]$ xsync jdk1.8.0_144/</span><br></pre></td></tr></table></figure>

<p>10）分别在 hadoop103 和 hadoop104 上配置 JDK 的环境变量，并执行 source 命令</p>
<h4 id="4-4-日志生成"><a href="#4-4-日志生成" class="headerlink" title="4.4 日志生成"></a>4.4 日志生成</h4><p>1）集群规划</p>
<p><img src="https://leinuo-blog.oss-cn-beijing.aliyuncs.com/images/typora/1594021915786.png" alt="1594021915786"></p>
<p>2）拷贝 3.4.16 节，打包好的 log-collector-1.0-SNAPSHOT-jar-with-dependencies.jar 程序到<br>hadoop102 的/opt/module/目录下，并执行如下命令</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[atguigu@hadoop102 module]$ java -jar log-collector-1.0-</span><br><span class="line">SNAPSHOT-jar-with-dependencies.jar &gt; /dev/null 2&gt;&amp;1 &amp;</span><br></pre></td></tr></table></figure>

<p>3）然后进入到/opt/module/logs 目录，观察日志是否写入成功</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[atguigu@hadoop102 logs]$ ls</span><br><span class="line">app-2019-10-07.log</span><br></pre></td></tr></table></figure>

<h4 id="4-5-Flume-安装-及使用"><a href="#4-5-Flume-安装-及使用" class="headerlink" title="4.5 Flume 安装 及使用"></a>4.5 Flume 安装 及使用</h4><p>1）Flume 官网地址：<a href="http://flume.apache.org/" target="_blank" rel="noopener">http://flume.apache.org/</a><br>2）文档查看地址：<a href="http://flume.apache.org/FlumeUserGuide.html" target="_blank" rel="noopener">http://flume.apache.org/FlumeUserGuide.html</a><br>3）下载地址：<a href="http://archive.apache.org/dist/flume/" target="_blank" rel="noopener">http://archive.apache.org/dist/flume/</a></p>
<h3 id="4-5-2-Flume-安装"><a href="#4-5-2-Flume-安装" class="headerlink" title="4.5.2 Flume 安装"></a>4.5.2 Flume 安装</h3><p>0）集群规划</p>
<p><img src="https://leinuo-blog.oss-cn-beijing.aliyuncs.com/images/typora/1594021984920.png" alt="1594021984920"></p>
<p>1）将 apache-flume-1.7.0-bin.tar.gz 上传到 hadoop102 的/opt/software 目录下<br>2）解压 apache-flume-1.7.0-bin.tar.gz 到/opt/module/目录下</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[atguigu@hadoop102 software]$ tar -zxf apache-flume-1.7.0-</span><br><span class="line">bin.tar.gz -C /opt/module/</span><br></pre></td></tr></table></figure>

<p>3）修改 apache-flume-1.7.0-bin 的名称为 flume</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[atguigu@hadoop102 module]$ mv apache-flume-1.7.0-bin flume</span><br></pre></td></tr></table></figure>

<p>4）将 flume/conf 下的 flume-env.sh.template 文件修改为 flume-env.sh，并配置 flume-env.sh 文件</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[atguigu@hadoop102 conf]$ mv flume-env.sh.template flume-env.sh</span><br><span class="line">[atguigu@hadoop102 conf]$ vi flume-env.sh</span><br><span class="line">export JAVA_HOME=/opt/module/jdk1.8.0_144</span><br></pre></td></tr></table></figure>

<h3 id="4-5-3-Flume-配置"><a href="#4-5-3-Flume-配置" class="headerlink" title="4.5.3 Flume 配置"></a>4.5.3 Flume 配置</h3><p>1）在/opt/module/flume/conf 中添加文件 file-flume-log.conf，该文件是一个 Flume 作业的核<br>心文件，咱们上述的 Source、Channel、Sink 都是通过这个配置文件来实现的。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[atguigu@hadoop102 conf]$ vim file-flume-log.conf</span><br><span class="line">添加如下内容</span><br><span class="line"><span class="meta">#</span><span class="bash"> 定义组件名称</span></span><br><span class="line">a1.sources = r1</span><br><span class="line">a1.sinks = k1</span><br><span class="line">a1.channels = c1</span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">##source 部分</span></span></span><br><span class="line">a1.sources.r1.type = TAILDIR</span><br><span class="line"><span class="meta">#</span><span class="bash">记录偏移量实现断点续传</span></span><br><span class="line">a1.sources.r1.positionFile =/opt/module/flume/test</span><br><span class="line">/taildir_position.json</span><br><span class="line">a1.sources.r1.channels = c1</span><br><span class="line">a1.sources.r1.filegroups=f1</span><br><span class="line">a1.sources.r1.filegroups.f1=/opt/module/logs/app.+</span><br><span class="line">a1.sources.r1.fileHeader = true</span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">##channel 部分</span></span></span><br><span class="line">a1.channels.c1.type = memory</span><br><span class="line">a1.channels.c1.capacity = 1000</span><br><span class="line">a1.channels.c1.transactionCapacity = 1000</span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">##sink 部分 先输出到控制台中</span></span></span><br><span class="line">a1.sinks.k1.type = logger</span><br><span class="line"><span class="meta">#</span><span class="bash"> 把 <span class="built_in">source</span> 和 sink 绑定到 channel 中</span></span><br><span class="line">a1.sources.r1.channels = c1</span><br><span class="line">a1.sinks.k1.channel = c1</span><br></pre></td></tr></table></figure>

<p>2）启动 Flume 进程</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[atguigu@hadoop102 flume]$ /opt/module/flume/bin/flume-ng agent -n a1 -c /opt/module/flume/conf/ -f /opt/module/flume/conf/file-flume-log.conf -Dflume.root.logger=info,console</span><br></pre></td></tr></table></figure>

<p>说明</p>
<p><img src="https://leinuo-blog.oss-cn-beijing.aliyuncs.com/images/typora/1594022116513.png" alt="1594022116513"></p>
<p>验证，启动 Flume 的同时，运行日志生成程序，观察 Flume 控制台是否滚动打印日志。</p>
<h3 id="4-6-DataHub-安装-及使用"><a href="#4-6-DataHub-安装-及使用" class="headerlink" title="4.6 DataHub 安装 及使用"></a>4.6 DataHub 安装 及使用</h3><h3 id="4-6-1-DataHub-简介"><a href="#4-6-1-DataHub-简介" class="headerlink" title="4.6.1 DataHub 简介"></a>4.6.1 DataHub 简介</h3><p>​    Flume 部分已经可以输出后，咱们开始搭建真正需要输出的目的地—DataHub，即阿里云数据总线服务。<br>​    通俗来说这个 DataHub 类似于传统大数据解决方案中 Kafka 的角色，提供了一个数据队列功能。</p>
<p>​    对于 离线计算，DataHub 除了供了一个 缓冲的队列作用。<br>​    同时由于 DataHub 提供了各种与其他阿里云上下游产品的对接功能，所以 DataHub 又扮演了一个 数据的分发枢纽工作。</p>
<p><img src="https://leinuo-blog.oss-cn-beijing.aliyuncs.com/images/typora/1594022208858.png" alt="1594022208858"></p>
<p>1）DataHub 输入组件包括<br>Flume：主流的开源日志采集框架<br>DTS：类似 Canal，日志实时监控采集框架<br>Logstash：也是日志采集框架，通常和 Elasticsearch、 Kibana 集合使用<br>Fluentd：Fluentd 是一个实时开源的数据收集器<br>OGG：实时监控 Oracle 中数据变化<br>Java sdk：支持 JavaAPI 方式访问</p>
<p>2）DataHub 输出组件包括<br>RDS：类似与传统 MySQL 数据库<br>AnalyticDB：面向分析型的分布式数据库<br>MaxCompute：离线分析框架<br>Elasticsearch：数据分析，倒排索引<br>StreamCompute：实时分析框架<br>TableSotre：类似于 Redis，KV 形式存储数据</p>
<p>OSS：类似于 HDFS，存储图片、视频</p>
<h3 id="4-6-2-创建-DataHub-与-与-Topic"><a href="#4-6-2-创建-DataHub-与-与-Topic" class="headerlink" title="4.6.2 创建 DataHub 与 与 Topic"></a>4.6.2 创建 DataHub 与 与 Topic</h3><p>阿里云 DataHub 控制台入口：<a href="https://datahub.console.aliyun.com/datahub" target="_blank" rel="noopener">https://datahub.console.aliyun.com/datahub</a></p>
<p>1）进入到 DataHub 控制台</p>
<p><img src="https://leinuo-blog.oss-cn-beijing.aliyuncs.com/images/typora/1594022358005.png" alt="1594022358005"></p>
<p>2）点击创建 Project</p>
<p><img src="https://leinuo-blog.oss-cn-beijing.aliyuncs.com/images/typora/1594022484565.png" alt="1594022484565"></p>
<p>3）点击查看，准备创建主题</p>
<p><img src="https://leinuo-blog.oss-cn-beijing.aliyuncs.com/images/typora/1594022540580.png" alt="1594022540580"></p>
<p>4）点击创建 Topic</p>
<p><img src="https://leinuo-blog.oss-cn-beijing.aliyuncs.com/images/typora/1594022550545.png" alt="1594022550545"></p>
<p>5）配置 Topic 详情</p>
<p><img src="https://leinuo-blog.oss-cn-beijing.aliyuncs.com/images/typora/1594022563425.png" alt="1594022563425"></p>
<p>说明：<img src="https://leinuo-blog.oss-cn-beijing.aliyuncs.com/images/typora/1594022580170.png" alt="1594022580170"></p>
<h3 id="4-7-Flume-到-推送数据到-DataHub"><a href="#4-7-Flume-到-推送数据到-DataHub" class="headerlink" title="4.7 Flume 到 推送数据到 DataHub"></a>4.7 Flume 到 推送数据到 DataHub</h3><h4 id="4-7-1-Flume-DataHub-插件安装"><a href="#4-7-1-Flume-DataHub-插件安装" class="headerlink" title="4.7.1 Flume-DataHub 插件安装"></a>4.7.1 Flume-DataHub 插件安装</h4><p>Flume 默认是不支持 DataHub 的，所以要给 Flume 安装 DataHub 的 Sink 插件<br>1）首先在 Flume 安装目录建立插件文件夹</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[atguigu@hadoop102 flume]$ mkdir plugins.d</span><br></pre></td></tr></table></figure>

<p>2 ） 利 用 SecureCRT 工 具 把 aliyun-flume-datahub-sink-2.0.2.tar.gz 拷贝到该/opt/module/flume/plugins.d 目录下，并原地解压缩</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[atguigu@hadoop102 plugins.d]$ ls</span><br><span class="line">aliyun-flume-datahub-sink-2.0.2.tar.gz</span><br><span class="line">[atguigu@hadoop102 plugins.d]$ tar -zxvf aliyun-flume-datahub-sink-2.0.2.tar.gz</span><br></pre></td></tr></table></figure>

<h3 id="4-7-2-Flume-配置文件修改"><a href="#4-7-2-Flume-配置文件修改" class="headerlink" title="4.7.2 Flume 配置文件修改"></a>4.7.2 Flume 配置文件修改</h3><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[atguigu@hadoop102 conf]$ vim /opt/module/flume/conf/file-</span><br><span class="line">flume-datahub.conf</span><br><span class="line"><span class="meta">#</span><span class="bash"> 定义组件名称</span></span><br><span class="line">a1.sources = r1</span><br><span class="line">a1.sinks = k1</span><br><span class="line">a1.channels = c1</span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">## source 部分</span></span></span><br><span class="line">a1.sources.r1.type = TAILDIR</span><br><span class="line">a1.sources.r1.positionFile =</span><br><span class="line">/opt/module/flume/test/taildir_position.json</span><br><span class="line">a1.sources.r1.channels = c1</span><br><span class="line">a1.sources.r1.filegroups = f1</span><br><span class="line">a1.sources.r1.filegroups.f1 = /opt/module/logs/app.+</span><br><span class="line">a1.sources.r1.fileHeader = true</span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">## channel 部分</span></span></span><br><span class="line">a1.channels.c1.type = memory</span><br><span class="line">a1.channels.c1.capacity = 1000</span><br><span class="line">a1.channels.c1.transactionCapacity = 1000</span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">## sink 部分</span></span></span><br><span class="line">a1.sinks.k1.type = com.aliyun.datahub.flume.sink.DatahubSink</span><br><span class="line">a1.sinks.k1.datahub.accessID = LTAI4FiU71dZAL17SdLBa6Nt</span><br><span class="line">a1.sinks.k1.datahub.accessKey = 63YzSmqMOSjDR5A2ZXEzFLM2tREY6m</span><br><span class="line">a1.sinks.k1.datahub.endPoint = http://dh-cn-hangzhou.aliyun-</span><br><span class="line">inc.com</span><br><span class="line">a1.sinks.k1.datahub.project = gmall_datahub</span><br><span class="line">a1.sinks.k1.datahub.topic = base_log</span><br><span class="line">a1.sinks.k1.batchSize = 100</span><br><span class="line">a1.sinks.k1.serializer = DELIMITED</span><br><span class="line">a1.sinks.k1.serializer.delimiter = "\\u007C"</span><br><span class="line">a1.sinks.k1.serializer.fieldnames = event_time,log_string</span><br><span class="line">a1.sinks.k1.serializer.charset = UTF-8</span><br><span class="line">a1.sinks.k1.shard.number = 1</span><br><span class="line">a1.sinks.k1.shard.maxTimeOut = 60</span><br><span class="line"><span class="meta">#</span><span class="bash"> 把 <span class="built_in">source</span> 和 sink 绑定到 channel 中</span></span><br><span class="line">a1.sources.r1.channels = c1</span><br><span class="line">a1.sinks.k1.channel = c1</span><br></pre></td></tr></table></figure>

<p>说明：</p>
<p><img src="https://leinuo-blog.oss-cn-beijing.aliyuncs.com/images/typora/1594022708310.png" alt="1594022708310"></p>
<p><img src="https://leinuo-blog.oss-cn-beijing.aliyuncs.com/images/typora/1594022717013.png" alt="1594022717013"></p>
<h3 id="4-7-3-获取-AccessID-和-AccessKey"><a href="#4-7-3-获取-AccessID-和-AccessKey" class="headerlink" title="4.7.3 获取 AccessID 和 AccessKey"></a>4.7.3 获取 AccessID 和 AccessKey</h3><p>DataHub 服务并不是靠 IP 来定位的，而是靠阿里云账号，每个阿里云账号只能有一个DataHub，每个阿里云账号也会有唯一的 AccessID 和 AccessKey。所以通过 AccessId 和AccessKey 就可以直接锁定某个阿里云账号的 DataHub。</p>
<p>1）悬浮鼠标到阿里云账号头像上-&gt;点击 accesskeys</p>
<p><img src="https://leinuo-blog.oss-cn-beijing.aliyuncs.com/images/typora/1594022781189.png" alt="1594022781189"></p>
<p>2）点击继续使用 AccessKey</p>
<p><img src="https://leinuo-blog.oss-cn-beijing.aliyuncs.com/images/typora/1594022791332.png" alt="1594022791332"></p>
<p>3）新用户需要点击创建 AccessKey</p>
<p><img src="https://leinuo-blog.oss-cn-beijing.aliyuncs.com/images/typora/1594022799132.png" alt="1594022799132"></p>
<p>4）获取到 AccessKeyID 和 AccessKeySecret 值</p>
<p><img src="https://leinuo-blog.oss-cn-beijing.aliyuncs.com/images/typora/1594022810459.png" alt="1594022810459"></p>
<h3 id="4-7-4-查看接收"><a href="#4-7-4-查看接收" class="headerlink" title="4.7.4 查看接收"></a>4.7.4 查看接收</h3><p>1）启动 Flume 进程</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[atguigu@hadoop102 flume]$ /opt/module/flume/bin/flume-ng agent -n a1 -c /opt/module/flume/conf/ -f /opt/module/flume/conf/file-flume-datahub.conf -Dflume.root.logger=info,console</span><br></pre></td></tr></table></figure>

<p>2）启动日志生成程序</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[atguigu@hadoop102 module]$ java -jar log-collector-1.0-SNAPSHOT-jar-with-dependencies.jar &gt; /dev/null 2&gt;&amp;1 &amp;</span><br></pre></td></tr></table></figure>

<p>3）观察 DataHub 中数据量</p>
<p><img src="https://leinuo-blog.oss-cn-beijing.aliyuncs.com/images/typora/1594022869175.png" alt="1594022869175"></p>
<p>应该可以去 DataHub 后台看到对应的数据已经进入主题。</p>
<h3 id="4-8-DataWorks-和-和-MaxCompute"><a href="#4-8-DataWorks-和-和-MaxCompute" class="headerlink" title="4.8 DataWorks 和 和 MaxCompute"></a>4.8 DataWorks 和 和 MaxCompute</h3><h3 id="4-8-1-简介"><a href="#4-8-1-简介" class="headerlink" title="4.8.1 简介"></a>4.8.1 简介</h3><p>MaxCompute（大数据计算服务）是阿里巴巴自主研发的海量数据处理平台，主要提供数据上传和下载通道，提供 SQL 及 MapReduce 等多种计算分析服务，同时还提供完善的安全解决方案。DataWorks（数据工场，原大数据开发套件）是基于 MaxCompute 计算引擎的一站式大数据工场，它能帮助您快速完成数据集成、开发、治理、服务、质量、安全等全套数据研发工作。</p>
<p><img src="https://leinuo-blog.oss-cn-beijing.aliyuncs.com/images/typora/1594022929089.png" alt="1594022929089"></p>
<p>盘古：相当于 Hadoop 中的 HDFS<br>伏羲：相当于 Hadoop 中的 YARN<br>MaxCompute Engine：相当于 MR、Tez 等计算引擎<br>MaxCompute 和 DataWorks 一起向用户提供完善的 ETL 和数仓管理能力，以及 SQL、<br>MR、Graph 等多种经典的分布式计算模型，能够更快速地解决用户海量数据计算问题，有效降低企业成本，保障数据安全。</p>
<h3 id="4-8-2-创建工作空间"><a href="#4-8-2-创建工作空间" class="headerlink" title="4.8.2 创建工作空间"></a>4.8.2 创建工作空间</h3><p>控制台入口：<a href="https://workbench.data.aliyun.com/consolenew" target="_blank" rel="noopener">https://workbench.data.aliyun.com/consolenew</a></p>
<p>1）购买 DataWorks</p>
<p><img src="https://leinuo-blog.oss-cn-beijing.aliyuncs.com/images/typora/1594023227719.png" alt="1594023227719"></p>
<p><img src="https://leinuo-blog.oss-cn-beijing.aliyuncs.com/images/typora/1594023233782.png" alt="1594023233782"></p>
<p>3）配置工作空间</p>
<p><img src="https://leinuo-blog.oss-cn-beijing.aliyuncs.com/images/typora/1594023243497.png" alt="1594023243497"></p>
<p>4）在选择引擎环节，购买 MaxCompute<br>如果没有购买 MaxCompute，可以点击按量付费，右侧的去购买按钮</p>
<p><img src="https://leinuo-blog.oss-cn-beijing.aliyuncs.com/images/typora/1594023259511.png" alt="1594023259511"></p>
<p>（1）按量购买 MaxCompute</p>
<p><img src="https://leinuo-blog.oss-cn-beijing.aliyuncs.com/images/typora/1594023272554.png" alt="1594023272554"></p>
<p>（2）确认订单</p>
<p><img src="https://leinuo-blog.oss-cn-beijing.aliyuncs.com/images/typora/1594023281234.png" alt="1594023281234"></p>
<p>5）引擎详情</p>
<p><img src="https://leinuo-blog.oss-cn-beijing.aliyuncs.com/images/typora/1594023289149.png" alt="1594023289149"></p>
<p>6）查看工作空间列表</p>
<p><img src="https://leinuo-blog.oss-cn-beijing.aliyuncs.com/images/typora/1594023306035.png" alt="1594023306035">7）进入数据开发</p>
<p><img src="https://leinuo-blog.oss-cn-beijing.aliyuncs.com/images/typora/1594023323493.png" alt="1594023323493"></p>
<p><img src="https://leinuo-blog.oss-cn-beijing.aliyuncs.com/images/typora/1594023329824.png" alt="1594023329824"></p>
]]></content>
      <categories>
        <category>大数据</category>
        <category>大数据实战项目</category>
        <category>阿里云离线数仓</category>
        <category>1.阿里云服务器搭建部署</category>
      </categories>
      <tags>
        <tag>大数据</tag>
      </tags>
  </entry>
  <entry>
    <title>3.业务数仓理论</title>
    <url>/2020/04/07/3.%E4%B8%9A%E5%8A%A1%E6%95%B0%E4%BB%93%E7%90%86%E8%AE%BA/</url>
    <content><![CDATA[<h2 id="第-6-章-业务数仓理论"><a href="#第-6-章-业务数仓理论" class="headerlink" title="第 6 章 业务数仓理论"></a>第 6 章 业务数仓理论</h2><h3 id="6-1-表的-分类"><a href="#6-1-表的-分类" class="headerlink" title="6.1 表的 分类"></a>6.1 表的 分类</h3><h4 id="6-1-1-实体表"><a href="#6-1-1-实体表" class="headerlink" title="6.1.1 实体表"></a>6.1.1 实体表</h4><p>实体表，一般是指一个现实存在的业务对象，比如用户，商品，商家，销售员等等。<br>用户表：</p>
<p><img src="https://leinuo-blog.oss-cn-beijing.aliyuncs.com/images/typora/1594026026671.png" alt="1594026026671"></p>
<h4 id="6-1-2-维度表"><a href="#6-1-2-维度表" class="headerlink" title="6.1.2 维度表"></a>6.1.2 维度表</h4><p>维度表，一般是指对应一些业务状态，编号的解释表。也可以称之为码表。<br>比如地区表，订单状态，支付方式，审批状态，商品分类等等。</p>
<p>订单状态表：</p>
<p><img src="https://leinuo-blog.oss-cn-beijing.aliyuncs.com/images/typora/1594026051566.png" alt="1594026051566"></p>
<p>商品分类表：</p>
<p><img src="https://leinuo-blog.oss-cn-beijing.aliyuncs.com/images/typora/1594026060902.png" alt="1594026060902"></p>
<h3 id="6-1-3-事务型事实表"><a href="#6-1-3-事务型事实表" class="headerlink" title="6.1.3 事务型事实表"></a>6.1.3 事务型事实表</h3><p>事务型事实表，一般指随着业务发生不断产生的数据。特点是一旦发生不会再变化。一般比如，交易流水，操作日志，出库入库记录等等。</p>
<p>交易流水表：</p>
<p><img src="https://leinuo-blog.oss-cn-beijing.aliyuncs.com/images/typora/1594026087576.png" alt="1594026087576"></p>
<h3 id="6-1-4-周期型事实表"><a href="#6-1-4-周期型事实表" class="headerlink" title="6.1.4 周期型事实表"></a>6.1.4 周期型事实表</h3><p>周期型事实表，一般指随着业务发生不断产生的数据。<br>与事务型不同的是，数据会随着业务周期性的推进而变化。<br>比如订单，其中订单状态会周期性变化。再比如，请假、贷款申请，随着批复状态在周期性变化。</p>
<p>订单表：</p>
<p><img src="https://leinuo-blog.oss-cn-beijing.aliyuncs.com/images/typora/1594026116684.png" alt="1594026116684"></p>
<h3 id="6-2-同步策略"><a href="#6-2-同步策略" class="headerlink" title="6.2 同步策略"></a>6.2 同步策略</h3><p>数据同步策略的类型包括：全量表、增量表、新增及变化表<br> 全量表：存储完整的数据。<br> 增量表：存储新增加的数据。<br> 新增及变化表：存储新增加的数据和变化的数据。</p>
<h4 id="6-2-1-实体表同步-策略"><a href="#6-2-1-实体表同步-策略" class="headerlink" title="6.2.1 实体表同步 策略"></a>6.2.1 实体表同步 策略</h4><p>实体表：比如用户，商品，商家，销售员等<br>实体表数据量比较小：通常可以做每日全量，就是每天存一份完整数据。即每日全量。</p>
<h4 id="6-2-2-维度表同步-策略"><a href="#6-2-2-维度表同步-策略" class="headerlink" title="6.2.2 维度表同步 策略"></a>6.2.2 维度表同步 策略</h4><p>维度表：比如订单状态，审批状态，商品分类<br>维度表数据量比较小：通常可以做每日全量，就是每天存一份完整数据。即每日全量。<br>说明：<br>1）针对可能会有变化的状态数据可以存储每日全量。<br>2）没变化的客观世界的维度（比如性别，地区，民族，政治成分，鞋子尺码）可以只存一份固定值。</p>
<h4 id="6-2-3-事务型事实表同步-策略"><a href="#6-2-3-事务型事实表同步-策略" class="headerlink" title="6.2.3 事务型事实表同步 策略"></a>6.2.3 事务型事实表同步 策略</h4><p>事务型事实表：比如，交易流水，操作日志，出库入库记录等。<br>因为数据不会变化，而且数据量巨大，所以每天只同步新增数据即可，所以可以做成每日增量表，即每日创建一个分区存储。</p>
<h4 id="6-2-4-周期型事实表同步-策略"><a href="#6-2-4-周期型事实表同步-策略" class="headerlink" title="6.2.4 周期型事实表同步 策略"></a>6.2.4 周期型事实表同步 策略</h4><p>周期型事实表：比如，订单、请假、贷款申请等<br>这类表从数据量的角度，存每日全量的话，数据量太大，冗余也太大。如果用每日增量的话无法反应数据变化。</p>
<p>每日新增及变化量，包括了当日的新增和修改。</p>
]]></content>
      <categories>
        <category>大数据</category>
        <category>大数据实战项目</category>
        <category>阿里云离线数仓</category>
        <category>3.业务数仓理论</category>
      </categories>
      <tags>
        <tag>大数据</tag>
      </tags>
  </entry>
  <entry>
    <title>2.用户行为数仓搭建</title>
    <url>/2020/04/07/2.%E7%94%A8%E6%88%B7%E8%A1%8C%E4%B8%BA%E6%95%B0%E4%BB%93%E6%90%AD%E5%BB%BA/</url>
    <content><![CDATA[<h2 id="第-5-章-用户行为数仓搭建"><a href="#第-5-章-用户行为数仓搭建" class="headerlink" title="第 5 章 用户行为数仓搭建"></a>第 5 章 用户行为数仓搭建</h2><h3 id="5-1-数仓分层概念"><a href="#5-1-数仓分层概念" class="headerlink" title="5.1 数仓分层概念"></a>5.1 数仓分层概念</h3><h4 id="5-1-1-数仓分层"><a href="#5-1-1-数仓分层" class="headerlink" title="5.1.1 数仓分层"></a>5.1.1 数仓分层</h4><p><img src="https://leinuo-blog.oss-cn-beijing.aliyuncs.com/images/typora/1594023504588.png" alt="1594023504588"></p>
<p>1）ODS 层<br>原始数据层，存放原始数据，直接加载原始日志、数据，数据保持原貌不做处理。<br>2）DWD 层<br>对 ODS 层数据进行清洗（去除空值，脏数据，超过极限范围的数据）<br>3）DWS 层<br>以 DWD 为基础，进行轻度汇总。<br>4）ADS 层<br>为各种统计报表提供数据</p>
<h4 id="5-1-2-数仓分层优点"><a href="#5-1-2-数仓分层优点" class="headerlink" title="5.1.2 数仓分层优点"></a>5.1.2 数仓分层优点</h4><p>1）把复杂问题简单化<br>将一个复杂的任务分解成多个步骤来完成，每一层只处理单一的步骤，比较简单、并且<br>方便定位问题。<br>2）减少重复开发<br>规范数据分层，通过的中间层数据，能够减少极大的重复计算，增加一次计算结果的复<br>用性。<br>3）隔离原始数据<br>不论是数据的异常还是数据的敏感性，使真实数据与统计数据解耦开。</p>
<h4 id="5-1-3-数仓命名"><a href="#5-1-3-数仓命名" class="headerlink" title="5.1.3 数仓命名"></a>5.1.3 数仓命名</h4><p> ODS层命名为ods前缀<br> DWD层命名为dwd前缀</p>
<p> DWS层命名为dws前缀<br> ADS层命名为ads前缀<br> 维度表命名为dim前缀<br> 每日全量导入命名为df（day full）后缀<br> 每日增量导入命名为di（day increase）后缀</p>
<h4 id="5-2-数仓分层配置"><a href="#5-2-数仓分层配置" class="headerlink" title="5.2 数仓分层配置"></a>5.2 数仓分层配置</h4><h4 id="5-2-1-建立业务流程"><a href="#5-2-1-建立业务流程" class="headerlink" title="5.2.1 建立业务流程"></a>5.2.1 建立业务流程</h4><p>1）点击数据开发-&gt;业务流程-&gt;新建业务流程-&gt;输入业务名称</p>
<p><img src="https://leinuo-blog.oss-cn-beijing.aliyuncs.com/images/typora/1594023571084.png" alt="1594023571084"></p>
<p>2）再业务流程下面就可以看到业务 1</p>
<p><img src="https://leinuo-blog.oss-cn-beijing.aliyuncs.com/images/typora/1594023583660.png" alt="1594023583660"></p>
<h4 id="5-2-2-配置表主题"><a href="#5-2-2-配置表主题" class="headerlink" title="5.2.2 配置表主题"></a>5.2.2 配置表主题</h4><p>1）进入配置中心<br>点击最下方的齿轮，进入配置中心</p>
<p><img src="https://leinuo-blog.oss-cn-beijing.aliyuncs.com/images/typora/1594023600022.png" alt="1594023600022"></p>
<p>2）配置主题管理，这里主要是划分表的主题<br>主题一般是表的对应的业务主题，比如：基础表、用户、商品、广告等对应的业务线。</p>
<p><img src="https://leinuo-blog.oss-cn-beijing.aliyuncs.com/images/typora/1594023611879.png" alt="1594023611879"></p>
<h4 id="5-2-3-配置表层级"><a href="#5-2-3-配置表层级" class="headerlink" title="5.2.3 配置表层级"></a>5.2.3 配置表层级</h4><p><img src="https://leinuo-blog.oss-cn-beijing.aliyuncs.com/images/typora/1594023635420.png" alt="1594023635420">在同一页面中，DataWorks 还提供了一个物理分类的划分维度，用户可以根据情况自行<br>决定划分方式。<br>本案例中，对数据表按照数据来源划分为，日志、数据库和综合。</p>
<h3 id="5-3-原始数据层-（ODS-层）"><a href="#5-3-原始数据层-（ODS-层）" class="headerlink" title="5.3 原始数据层 （ODS 层）"></a>5.3 原始数据层 （ODS 层）</h3><p><img src="https://leinuo-blog.oss-cn-beijing.aliyuncs.com/images/typora/1594023695464.png" alt="1594023695464"></p>
<h3 id="5-3-1-建表语句"><a href="#5-3-1-建表语句" class="headerlink" title="5.3.1 建表语句"></a>5.3.1 建表语句</h3><p>1）回到数据开发的界面，按下图开始创建数据表</p>
<p><img src="https://leinuo-blog.oss-cn-beijing.aliyuncs.com/images/typora/1594023709003.png" alt="1594023709003"></p>
<p>2）输入表名-&gt;提交</p>
<p><img src="https://leinuo-blog.oss-cn-beijing.aliyuncs.com/images/typora/1594023717362.png" alt="1594023717362"></p>
<p>开始建立的表要和原始数据最基本的结构一致。</p>
<h3 id="5-3-2-配置基本属性"><a href="#5-3-2-配置基本属性" class="headerlink" title="5.3.2 配置基本属性"></a>5.3.2 配置基本属性</h3><p>配置新创建的表的主题。</p>
<p><img src="https://leinuo-blog.oss-cn-beijing.aliyuncs.com/images/typora/1594023740749.png" alt="1594023740749"></p>
<h3 id="5-3-3-配置物理模型"><a href="#5-3-3-配置物理模型" class="headerlink" title="5.3.3 配置物理模型"></a>5.3.3 配置物理模型</h3><p>注意选择层级和物理分类，由于建立的原始日志表，所以此处选择 ODS 层。分类是日志。</p>
<p><img src="https://leinuo-blog.oss-cn-beijing.aliyuncs.com/images/typora/1594023761360.png" alt="1594023761360"></p>
<p>​    从 DataHub 过来的数据必须选择分区表。如果手工文件导入的表可以选择非分区。<br>​    为了减少再购买存储服务器，所以选择内部表。真实企业开发时，大部分情况都是创建外部表，需要在 OSS 服务中申请存储空间。</p>
<h3 id="5-3-4-配置字段"><a href="#5-3-4-配置字段" class="headerlink" title="5.3.4 配置字段"></a>5.3.4 配置字段</h3><p>发送过来的日志包含两部分：服务器时间和日志详情。这里面设计了两个字段对应接收。</p>
<p><img src="https://leinuo-blog.oss-cn-beijing.aliyuncs.com/images/typora/1594023854289.png" alt="1594023854289"></p>
<p>注意：这里的主键，是指一个标志而已，本身不提供索引和唯一性约束。</p>
<h3 id="5-3-5-配置分区"><a href="#5-3-5-配置分区" class="headerlink" title="5.3.5 配置分区"></a>5.3.5 配置分区</h3><p>表示数据按如下字段进行分区</p>
<p><img src="https://leinuo-blog.oss-cn-beijing.aliyuncs.com/images/typora/1594023878123.png" alt="1594023878123"></p>
<p>​     由于目前 DataHub 至 MaxCompute 的接口，只支持按年月日+小时+分钟方式分区，所以这三个字段是必须有的，且字段类型必须是 String。</p>
<h3 id="5-3-6-查看建表语句"><a href="#5-3-6-查看建表语句" class="headerlink" title="5.3.6 查看建表语句"></a>5.3.6 查看建表语句</h3><p>点击 DDL 模式，可以查看自动生成的建表语句。和普通的创建表语句一样。</p>
<p><img src="https://leinuo-blog.oss-cn-beijing.aliyuncs.com/images/typora/1594023947054.png" alt="1594023947054"></p>
<h3 id="5-3-7-提交到生产环境"><a href="#5-3-7-提交到生产环境" class="headerlink" title="5.3.7 提交到生产环境"></a>5.3.7 提交到生产环境</h3><p>点击提交到生产环境，表就创建完成了。</p>
<p><img src="https://leinuo-blog.oss-cn-beijing.aliyuncs.com/images/typora/1594023967700.png" alt="1594023967700"></p>
<h3 id="5-4-表的基本操作"><a href="#5-4-表的基本操作" class="headerlink" title="5.4 表的基本操作"></a>5.4 表的基本操作</h3><h4 id="5-4-1-查看表结构"><a href="#5-4-1-查看表结构" class="headerlink" title="5.4.1 查看表结构"></a>5.4.1 查看表结构</h4><p>在最左侧菜单中选择【表管理】，可以在右侧查看表的结构信息。但是不可以修改。</p>
<p><img src="https://leinuo-blog.oss-cn-beijing.aliyuncs.com/images/typora/1594023991126.png" alt="1594023991126"></p>
<h4 id="5-4-2-在业务流程中导入表"><a href="#5-4-2-在业务流程中导入表" class="headerlink" title="5.4.2 在业务流程中导入表"></a>5.4.2 在业务流程中导入表</h4><p>1）在业务 1 中建 ods、dwd、dws、ads，4 个文件夹</p>
<p><img src="https://leinuo-blog.oss-cn-beijing.aliyuncs.com/images/typora/1594024055393.png" alt="1594024055393"></p>
<p><img src="https://leinuo-blog.oss-cn-beijing.aliyuncs.com/images/typora/1594024059753.png" alt="1594024059753"></p>
<p><img src="https://leinuo-blog.oss-cn-beijing.aliyuncs.com/images/typora/1594024063816.png" alt="1594024063816"></p>
<p>2）向 ods 文件夹中导入表</p>
<p><img src="https://leinuo-blog.oss-cn-beijing.aliyuncs.com/images/typora/1594024073580.png" alt="1594024073580"></p>
<p><img src="https://leinuo-blog.oss-cn-beijing.aliyuncs.com/images/typora/1594024080431.png" alt="1594024080431"></p>
<h3 id="5-4-3-临时查询"><a href="#5-4-3-临时查询" class="headerlink" title="5.4.3 临时查询"></a>5.4.3 临时查询</h3><p>1）点击临时查询-&gt;新建-&gt;ODPS SQL</p>
<p><img src="https://leinuo-blog.oss-cn-beijing.aliyuncs.com/images/typora/1594024097393.png" alt="1594024097393"></p>
<p>2）创建临时查询节点</p>
<p><img src="https://leinuo-blog.oss-cn-beijing.aliyuncs.com/images/typora/1594024104537.png" alt="1594024104537"></p>
<p>3）可以执行 SQL 命令</p>
<p><img src="https://leinuo-blog.oss-cn-beijing.aliyuncs.com/images/typora/1594024126755.png" alt="1594024126755"></p>
<h3 id="5-5-DataHub-到-推送数据到-MaxCompute"><a href="#5-5-DataHub-到-推送数据到-MaxCompute" class="headerlink" title="5.5 DataHub 到 推送数据到 MaxCompute"></a>5.5 DataHub 到 推送数据到 MaxCompute</h3><p>​    如下图，之前 Flume 中的数据利用 DataHub Sink 把数据写入到了 DataHub 中，DataHub中提供了很多的其他第三方的 DataConnector 可以连接各种例如：MaxCompute，ElasticSearch，ADB，RDS 等数据库。</p>
<p>所以下面就要建立 DataConnector 把数据推送到 MaxCompute 中。</p>
<h4 id="5-5-1-建-创建-DataConnector"><a href="#5-5-1-建-创建-DataConnector" class="headerlink" title="5.5.1 建 创建 DataConnector"></a>5.5.1 建 创建 DataConnector</h4><p>1）在 DataHub 中找到 Topic，在某个 Topic 下，点击右上角的 DataConnector</p>
<p><img src="https://leinuo-blog.oss-cn-beijing.aliyuncs.com/images/typora/1594024190452.png" alt="1594024190452"></p>
<p>2）点击同步到 MaxCompute 离线表</p>
<p><img src="https://leinuo-blog.oss-cn-beijing.aliyuncs.com/images/typora/1594024231024.png" alt="1594024231024"></p>
<p>3） 创建 DataConnector</p>
<p><img src="https://leinuo-blog.oss-cn-beijing.aliyuncs.com/images/typora/1594024263855.png" alt="1594024263855"></p>
<p>（1）MaxCompute Project：名称要和 MaxCompute 创建的工作空间名称一致<br>（2）MaxCompute Table：数据导入到 MaxCompute 中的表名<br>（3）AccessKey ID：LTAI4FiU71dZAL17SdLBa6Nt<br>（4）AccessKey Secret：63YzSmqMOSjDR5A2ZXEzFLM2tREY6m<br>注意：AccessKey ID 和 AccessKey Secret 要和自己的阿里云账号一一对应<br>（5）分区选项：system、event_time<br>通常采用 system 时间分区。<br>如果是 event_time 方式分区，就要在 topic 中包含一个 event_time 的字段。不过这个字段与以往的timestamp 不同的是，必须精确到微秒级。而且这个字段一旦用于分区，则不会再写入到实体表中。<br>（6）分区范围：15 分钟起</p>
<h3 id="5-5-2-发送数据"><a href="#5-5-2-发送数据" class="headerlink" title="5.5.2 发送数据"></a>5.5.2 发送数据</h3><p>建立好表后 DataConnector 就可以尝试发送数据了。<br>注意：如果已经启动了 Flume，就不需要再次启动了。<br>1）启动 Flume 程序</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[atguigu@hadoop102 flume]$ /opt/module/flume/bin/flume-ng agent -n a1 -c /opt/module/flume/conf/ -f /opt/module/flume/conf/file-flume-datahub.conf -Dflume.root.logger=info,console</span><br></pre></td></tr></table></figure>

<p>2）在服务器 hadoop102 上执行命令</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[atguigu@hadoop102 module]$ java -jar log-collector-1.0-SNAPSHOT-jar-with-dependencies.jar &gt; /dev/null 2&gt;&amp;1 &amp;</span><br></pre></td></tr></table></figure>

<h4 id="5-5-3-接收数据"><a href="#5-5-3-接收数据" class="headerlink" title="5.5.3 接收数据"></a>5.5.3 接收数据</h4><p>1）观察 DataHub 中接收到数据（速度很快</p>
<p><img src="https://leinuo-blog.oss-cn-beijing.aliyuncs.com/images/typora/1594024388519.png" alt="1594024388519"></p>
<p>2）查看 MaxCompute 中接收到数据（1-5 分钟的延迟）</p>
<p><img src="https://leinuo-blog.oss-cn-beijing.aliyuncs.com/images/typora/1594024396848.png" alt="1594024396848"></p>
<h3 id="5-6-明细数据层（DWD-层）"><a href="#5-6-明细数据层（DWD-层）" class="headerlink" title="5.6 明细数据层（DWD 层）"></a>5.6 明细数据层（DWD 层）</h3><p>​     DWD 层主要是对 ODS 层数据进行清洗（去除空值，脏数据，超过极限范围的数据）。DWD 层处理后的表，能够成为非常明确可用的基础明细数据。<br>​     本次项目中需要将用户行为过来基础日志，根据表类型，一张一张的解析出来 11 张不同类型的表数据，方便后续的处理。</p>
<h3 id="5-6-1-日志格式分析"><a href="#5-6-1-日志格式分析" class="headerlink" title="5.6.1 日志格式分析"></a>5.6.1 日志格式分析</h3><p>1）日志格式：服务器时间 | json<br>2）其中 json 包括：<br>cm：公共字段的 key；<br>ap：app 的名称；<br>et：具体事件</p>
<p><img src="https://leinuo-blog.oss-cn-beijing.aliyuncs.com/images/typora/1594024489786.png" alt="1594024489786"></p>
<h3 id="5-6-2-自定义-UDTF-（解析-具体事件字段）"><a href="#5-6-2-自定义-UDTF-（解析-具体事件字段）" class="headerlink" title="5.6.2 自定义 UDTF （解析 具体事件字段）"></a>5.6.2 自定义 UDTF （解析 具体事件字段）</h3><p>开发 UDTF 有两个方法：</p>
<ul>
<li><p>方法 1：在本地 IDEA 中创建工程，开发代码，打包，把 JAR 上传到 DataStudio 成为资源 JAR 包。然后基于资源 JAR 包，声明函数。</p>
</li>
<li><p>方法 2：直接在 FunctionStudio 中开发，然后在线打包发布程序，声明函数。</p>
<p> 相比而言，从发布流程上来说利用 FunctionStudio 更快捷方便。但是从 IDEA 开发角度来说，网页版本的 FunctionStudio，肯定不如客户端的功能强大、反应速度流畅。不过也可以两者配合起来使用。<br>本次案例主要介绍通过 FunctionStudio 来编写 UDTF 函数。<br>1）按下图所示，打开 FunctionStudio 的界面</p>
</li>
</ul>
<p><img src="https://leinuo-blog.oss-cn-beijing.aliyuncs.com/images/typora/1594024631792.png" alt="1594024631792"></p>
<p>2）创建工程</p>
<p><img src="https://leinuo-blog.oss-cn-beijing.aliyuncs.com/images/typora/1594024649248.png" alt="1594024649248"></p>
<p>3）选择命名工程名，选择 udfjava</p>
<p><img src="https://leinuo-blog.oss-cn-beijing.aliyuncs.com/images/typora/1594024661647.png" alt="1594024661647"></p>
<p>4）添加代码文件<br>可以看到工程创建好后，默认有很多参考的模板。</p>
<p><img src="https://leinuo-blog.oss-cn-beijing.aliyuncs.com/images/typora/1594024680996.png" alt="1594024680996"></p>
<p>5）在 udtf 目录下创建一个 FlatEventUDTF 类</p>
<p><img src="https://leinuo-blog.oss-cn-beijing.aliyuncs.com/images/typora/1594024725424.png" alt="1594024725424"></p>
<p>6）编写代码<br>首先分析日志结构</p>
<p><img src="https://leinuo-blog.oss-cn-beijing.aliyuncs.com/images/typora/1594024738853.png" alt="1594024738853"></p>
<p>（1）在 pom.xml 中要加入 fastJson 依赖</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">groupId</span>&gt;</span>com.alibaba<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>fastjson<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">version</span>&gt;</span>1.2.28.odps<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p>（2）编写自定义 UDTF 代码</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> com.aliyun.odps.udf.ExecutionContext;</span><br><span class="line"><span class="keyword">import</span> com.aliyun.odps.udf.UDFException;</span><br><span class="line"><span class="keyword">import</span> com.aliyun.odps.udf.UDTF;</span><br><span class="line"><span class="keyword">import</span> com.aliyun.odps.udf.annotation.Resolve;</span><br><span class="line"><span class="keyword">import</span> com.alibaba.fastjson.*;</span><br><span class="line"><span class="comment">// TODO define input and output types, e.g."string,string-&gt;string,bigint".</span></span><br><span class="line"><span class="meta">@Resolve</span>(&#123;<span class="string">"string-&gt;bigint,string,string"</span>&#125;)</span><br><span class="line">	<span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">FlatEventUDTF</span> <span class="keyword">extends</span> <span class="title">UDTF</span> </span>&#123;</span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setup</span><span class="params">(ExecutionContext ctx)</span> <span class="keyword">throws</span> UDFException</span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">&#125;</span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">process</span><span class="params">(Object[] args)</span> <span class="keyword">throws</span> UDFException </span>&#123;</span><br><span class="line">	</span><br><span class="line">    String event =(String) args[<span class="number">0</span>];</span><br><span class="line">	JSONArray jsonArray = JSON.parseArray(event);</span><br><span class="line">	</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; jsonArray.size(); i++) &#123;</span><br><span class="line">	JSONObject jsonObject = jsonArray.getJSONObject(i);</span><br><span class="line">	String ett =(String) jsonObject.getString(<span class="string">"ett"</span>);</span><br><span class="line">	String eventName =(String) jsonObject.getString(<span class="string">"en"</span>);</span><br><span class="line">	String eventJson =(String) jsonObject.getString(<span class="string">"kv"</span>);</span><br><span class="line">	forward(Long.parseLong(ett),eventName,eventJson);</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">close</span><span class="params">()</span> <span class="keyword">throws</span> UDFException </span>&#123;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>其中，@Resolve({“string-&gt;bigint,string,string”} 表示该函数，传入参数是 string，返回参数是三个，分别为长整型，和两个字符串，对应的返回值就是事件的时间、事件名称和事件内容（json 格式）。</p>
<p>7）打包部署<br>（1）提交到 Data Studio 开发环境</p>
<p><img src="https://leinuo-blog.oss-cn-beijing.aliyuncs.com/images/typora/1594025039992.png" alt="1594025039992"></p>
<p>（2）提交函数详情</p>
<p><img src="https://leinuo-blog.oss-cn-beijing.aliyuncs.com/images/typora/1594025047520.png" alt="1594025047520"></p>
<p>（3）提交函数过程，控制台打印</p>
<p><img src="https://leinuo-blog.oss-cn-beijing.aliyuncs.com/images/typora/1594025060194.png" alt="1594025060194"></p>
<p>提交成功后 ，回到 DataWorks 中的资源菜单中可以看到增加了 1 个 JAR 包，函数菜单中可以看到定义的函数。</p>
<p><img src="https://leinuo-blog.oss-cn-beijing.aliyuncs.com/images/typora/1594025070805.png" alt="1594025070805"></p>
<p>8）测试函数<br>在 DataStudio 中临时查询，执行如下语句</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">select</span><br><span class="line">FlatEventUDTF(GET_JSON_OBJECT(log_string,&#39;$.et&#39;)) as</span><br><span class="line">(event_time, event_name, event_json)</span><br><span class="line">from ods_base_log</span><br><span class="line">where ds&#x3D;&#39;20191008&#39;;</span><br></pre></td></tr></table></figure>

<p>能看到如下结果表示运行正确</p>
<p><img src="https://leinuo-blog.oss-cn-beijing.aliyuncs.com/images/typora/1594025170633.png" alt="1594025170633"></p>
<h3 id="5-6-3-DWD-层建表（启动-日志表）"><a href="#5-6-3-DWD-层建表（启动-日志表）" class="headerlink" title="5.6.3 DWD 层建表（启动 日志表）"></a>5.6.3 DWD 层建表（启动 日志表）</h3><p>在流程中建立表，在 DWD 文件夹中增加一个 dwd_start_log 的表，可以直接用 ddl 模式建立，再进行微调。<br>1）在业务 1 的表上，右键新建表</p>
<p><img src="https://leinuo-blog.oss-cn-beijing.aliyuncs.com/images/typora/1594025206201.png" alt="1594025206201"></p>
<p>2）点击 DDL 模式</p>
<p><img src="https://leinuo-blog.oss-cn-beijing.aliyuncs.com/images/typora/1594025212690.png" alt="1594025212690"></p>
<p>3）在 DDL 模式中添加建表语句</p>
<p><img src="https://leinuo-blog.oss-cn-beijing.aliyuncs.com/images/typora/1594025238499.png" alt="1594025238499"></p>
<p>详细建表语句如下</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">CREATE TABLE &#96;dwd_start_log&#96; (</span><br><span class="line">&#96;mid&#96; string,</span><br><span class="line">&#96;user_id&#96; string,</span><br><span class="line">&#96;version_code&#96; string,</span><br><span class="line">&#96;version_name&#96; string,</span><br><span class="line">&#96;lang&#96; string,</span><br><span class="line">&#96;source&#96; string,</span><br><span class="line">&#96;os&#96; string,</span><br><span class="line">&#96;area&#96; string,</span><br><span class="line">&#96;model&#96; string,</span><br><span class="line">&#96;brand&#96; string,</span><br><span class="line">&#96;sdk_version&#96; string,</span><br><span class="line">&#96;email&#96; string,</span><br><span class="line">&#96;height_width&#96; string,</span><br><span class="line">&#96;network&#96; string,</span><br><span class="line">&#96;lng&#96; string,</span><br><span class="line">&#96;lat&#96; string,</span><br><span class="line">&#96;entry&#96; string,</span><br><span class="line">&#96;open_ad_type&#96; string,</span><br><span class="line">&#96;action&#96; string,</span><br><span class="line">&#96;loading_time&#96; string,</span><br><span class="line">&#96;detail&#96; string,</span><br><span class="line">&#96;event_time&#96; string COMMENT &#39;事件时间&#39;</span><br><span class="line">)</span><br><span class="line">PARTITIONED BY (ds string, hh string, mm string);</span><br></pre></td></tr></table></figure>

<p>4）补充一些相关字段。</p>
<p><img src="https://leinuo-blog.oss-cn-beijing.aliyuncs.com/images/typora/1594025273829.png" alt="1594025273829"></p>
<p>5）补充分区格式</p>
<p><img src="https://leinuo-blog.oss-cn-beijing.aliyuncs.com/images/typora/1594025287100.png" alt="1594025287100"></p>
<p>6）提交到生产环境</p>
<p><img src="https://leinuo-blog.oss-cn-beijing.aliyuncs.com/images/typora/1594025298188.png" alt="1594025298188"></p>
<h3 id="5-6-4-手动将-将-ODS-层数据导入-DWD-层"><a href="#5-6-4-手动将-将-ODS-层数据导入-DWD-层" class="headerlink" title="5.6.4 手动将 将 ODS 层数据导入 DWD 层"></a>5.6.4 手动将 将 ODS 层数据导入 DWD 层</h3><p>1）在临时查询页面，把 ods 层 ods_base_log 里面的数据导入到 dwd_start_log</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">INSERT OVERWRITE TABLE dwd_start_log PARTITION (ds,hh,mm)</span><br><span class="line">select</span><br><span class="line">GET_JSON_OBJECT(log_string,&#39;$.cm.mid&#39;) mid,</span><br><span class="line">GET_JSON_OBJECT(log_string,&#39;$.cm.uid&#39;) user_id,</span><br><span class="line">GET_JSON_OBJECT(log_string,&#39;$.cm.vc&#39;) version_code,</span><br><span class="line">GET_JSON_OBJECT(log_string,&#39;$.cm.vn&#39;) version_name,</span><br><span class="line">GET_JSON_OBJECT(log_string,&#39;$.cm.l&#39;) lang,</span><br><span class="line">GET_JSON_OBJECT(log_string,&#39;$.cm.sr&#39;) source,</span><br><span class="line">GET_JSON_OBJECT(log_string,&#39;$.cm.os&#39;) os,</span><br><span class="line">GET_JSON_OBJECT(log_string,&#39;$.cm.ar&#39;) area,</span><br><span class="line">GET_JSON_OBJECT(log_string,&#39;$.cm.md&#39;) model,</span><br><span class="line">GET_JSON_OBJECT(log_string,&#39;$.cm.ba&#39;) brand,</span><br><span class="line">GET_JSON_OBJECT(log_string,&#39;$.cm.sv&#39;) sdk_version,</span><br><span class="line">GET_JSON_OBJECT(log_string,&#39;$.cm.hw&#39;) height_width,</span><br><span class="line">GET_JSON_OBJECT(log_string,&#39;$.cm.g&#39;) email,</span><br><span class="line">GET_JSON_OBJECT(log_string,&#39;$.cm.hw&#39;) sv,</span><br><span class="line">GET_JSON_OBJECT(log_string,&#39;$.cm.ln&#39;) ln,</span><br><span class="line">GET_JSON_OBJECT(log_string,&#39;$.cm.la&#39;) la,</span><br><span class="line">GET_JSON_OBJECT( event_view.event_json,&#39;$.entry&#39;) entry,</span><br><span class="line">GET_JSON_OBJECT( event_view.event_json,&#39;$.loading_time&#39;)</span><br><span class="line">loading_time,</span><br><span class="line">GET_JSON_OBJECT( event_view.event_json,&#39;$.action&#39;) action,</span><br><span class="line">GET_JSON_OBJECT( event_view.event_json,&#39;$.open_ad_type&#39;)</span><br><span class="line">open_ad_type,</span><br><span class="line">GET_JSON_OBJECT( event_view.event_json,&#39;$.detail&#39;) detail ,</span><br><span class="line">event_view.event_time,</span><br><span class="line">ds,</span><br><span class="line">hh,</span><br><span class="line">mm</span><br><span class="line">from ods_base_log</span><br><span class="line">LATERAL VIEW FlatEventUDTF(GET_JSON_OBJECT(log_string,&#39;$.et&#39; ))</span><br><span class="line">event_view as event_time,event_name,event_json</span><br><span class="line">where ds&#x3D;&#39;20191008&#39; and event_view.event_name &#x3D; &#39;start&#39;;</span><br></pre></td></tr></table></figure>

<p>2）查看导入结果</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> * <span class="keyword">from</span> dwd_start_log <span class="keyword">WHERE</span> ds=<span class="string">'20191008'</span>;</span><br></pre></td></tr></table></figure>

<h3 id="5-6-5-数据导入脚本-本"><a href="#5-6-5-数据导入脚本-本" class="headerlink" title="5.6.5 数据导入脚本 本"></a>5.6.5 数据导入脚本 本</h3><p>1）在流程中加入一个数据开发脚本</p>
<p><img src="https://leinuo-blog.oss-cn-beijing.aliyuncs.com/images/typora/1594025377362.png" alt="1594025377362"></p>
<p><img src="https://leinuo-blog.oss-cn-beijing.aliyuncs.com/images/typora/1594025393721.png" alt="1594025393721"></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">INSERT OVERWRITE TABLE dwd_start_log PARTITION (ds,hh,mm)</span><br><span class="line">select</span><br><span class="line">GET_JSON_OBJECT(log_string,&#39;$.cm.mid&#39;) mid,</span><br><span class="line">GET_JSON_OBJECT(log_string,&#39;$.cm.uid&#39;) user_id,</span><br><span class="line">GET_JSON_OBJECT(log_string,&#39;$.cm.vc&#39;) version_code,</span><br><span class="line">GET_JSON_OBJECT(log_string,&#39;$.cm.vn&#39;) version_name,</span><br><span class="line">GET_JSON_OBJECT(log_string,&#39;$.cm.l&#39;) lang,</span><br><span class="line">GET_JSON_OBJECT(log_string,&#39;$.cm.sr&#39;) source,</span><br><span class="line">GET_JSON_OBJECT(log_string,&#39;$.cm.os&#39;) os,</span><br><span class="line">GET_JSON_OBJECT(log_string,&#39;$.cm.ar&#39;) area,</span><br><span class="line">GET_JSON_OBJECT(log_string,&#39;$.cm.md&#39;) model,</span><br><span class="line">GET_JSON_OBJECT(log_string,&#39;$.cm.ba&#39;) brand,</span><br><span class="line">GET_JSON_OBJECT(log_string,&#39;$.cm.sv&#39;) sdk_version,</span><br><span class="line">GET_JSON_OBJECT(log_string,&#39;$.cm.hw&#39;) height_width,</span><br><span class="line">GET_JSON_OBJECT(log_string,&#39;$.cm.g&#39;) email,</span><br><span class="line">GET_JSON_OBJECT(log_string,&#39;$.cm.hw&#39;) sv,</span><br><span class="line">GET_JSON_OBJECT(log_string,&#39;$.cm.ln&#39;) ln,</span><br><span class="line">GET_JSON_OBJECT(log_string,&#39;$.cm.la&#39;) la,</span><br><span class="line">GET_JSON_OBJECT( event_view.event_json,&#39;$.entry&#39;) entry,</span><br><span class="line">GET_JSON_OBJECT( event_view.event_json,&#39;$.loading_time&#39;)</span><br><span class="line">loading_time,</span><br><span class="line">GET_JSON_OBJECT( event_view.event_json,&#39;$.action&#39;) action,</span><br><span class="line">GET_JSON_OBJECT( event_view.event_json,&#39;$.open_ad_type&#39;)</span><br><span class="line">open_ad_type,</span><br><span class="line">GET_JSON_OBJECT( event_view.event_json,&#39;$.detail&#39;) detail,</span><br><span class="line">event_view.event_time,</span><br><span class="line">ds,</span><br><span class="line">hh,</span><br><span class="line">mm</span><br><span class="line">from ods_base_log</span><br><span class="line">LATERAL VIEW FlatEventUdtf (GET_JSON_OBJECT(log_string,&#39;$.et&#39; ))</span><br><span class="line">event_view as event_time,event_name,event_json</span><br><span class="line">where ds&#x3D;&#39;$&#123;bizdate&#125;&#39; and event_view.event_name &#x3D; &#39;start&#39; ;</span><br></pre></td></tr></table></figure>

<p>注意：在上面的 SQL 中我们使用了一个${bizedate}作为外部传入的日期参数。</p>
<p>2）配置参数<br>在执行或者调度该脚本的时候传入相应的参数。</p>
<p><img src="https://leinuo-blog.oss-cn-beijing.aliyuncs.com/images/typora/1594025427699.png" alt="1594025427699"></p>
<p>（1）在右侧有一个调度配置，打开可以对参数进行设置。</p>
<p><img src="https://leinuo-blog.oss-cn-beijing.aliyuncs.com/images/typora/1594025434520.png" alt="1594025434520"></p>
<p>注意：<br>这里参数设置用的是 花括号，bizdate=${yyyymmdd}，表示取前一日的日期；<br>如果采用方括号，如，bizdate= $[yyyymmdd]，表示取当前日期。</p>
<p>（2）配置脚本执行时间</p>
<p><img src="https://leinuo-blog.oss-cn-beijing.aliyuncs.com/images/typora/1594025447403.png" alt="1594025447403"></p>
<h3 id="5-7-服务数据层（DWS-层）"><a href="#5-7-服务数据层（DWS-层）" class="headerlink" title="5.7 服务数据层（DWS 层）"></a>5.7 服务数据层（DWS 层）</h3><p>需求：日活统计</p>
<h3 id="5-7-1-建表语句"><a href="#5-7-1-建表语句" class="headerlink" title="5.7.1 建表语句"></a>5.7.1 建表语句</h3><p>1）创建表 dws_uv_detail_d</p>
<p><img src="https://leinuo-blog.oss-cn-beijing.aliyuncs.com/images/typora/1594025476715.png" alt="1594025476715"></p>
<p>2）点击 DDL 模式</p>
<p><img src="https://leinuo-blog.oss-cn-beijing.aliyuncs.com/images/typora/1594025483083.png" alt="1594025483083"></p>
<p>3）在 DDL 模式中添加建表语句</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">CREATE TABLE &#96;dws_uv_detail_d&#96; (</span><br><span class="line">&#96;mid&#96; string COMMENT &#39;设备唯一标识&#39;,</span><br><span class="line">&#96;user_id&#96; string COMMENT &#39;用户标识&#39;,</span><br><span class="line">&#96;version_code&#96; string COMMENT &#39;程序版本号&#39;,</span><br><span class="line">&#96;version_name&#96; string COMMENT &#39;程序版本名&#39;,</span><br><span class="line">&#96;lang&#96; string COMMENT &#39;系统语言&#39;,</span><br><span class="line">&#96;source&#96; string COMMENT &#39;渠道号&#39;,</span><br><span class="line">&#96;os&#96; string COMMENT &#39;系统版本&#39;,</span><br><span class="line">&#96;area&#96; string COMMENT &#39;区域&#39;,</span><br><span class="line">&#96;model&#96; string COMMENT &#39;手机型号&#39;,</span><br><span class="line">&#96;brand&#96; string COMMENT &#39;手机品牌&#39;,</span><br><span class="line">&#96;sdk_version&#96; string COMMENT &#39;sdkversion&#39;,</span><br><span class="line">&#96;email&#96; string COMMENT &#39;email&#39;,</span><br><span class="line">&#96;height_width&#96; string COMMENT &#39;屏幕宽高&#39;,</span><br><span class="line">&#96;network&#96; string COMMENT &#39;网络模式&#39;,</span><br><span class="line">&#96;lng&#96; string COMMENT &#39;经度&#39;,</span><br><span class="line">&#96;lat&#96; string COMMENT &#39;纬度&#39;,</span><br><span class="line">&#96;event_time&#96; bigint</span><br><span class="line">)</span><br><span class="line">COMMENT &#39;活跃用户按天明细&#39;</span><br><span class="line">PARTITIONED BY (ds string,hh string,mm string);</span><br></pre></td></tr></table></figure>

<p>4）补全建表信息描述</p>
<p><img src="https://leinuo-blog.oss-cn-beijing.aliyuncs.com/images/typora/1594025515492.png" alt="1594025515492"></p>
<p><img src="https://leinuo-blog.oss-cn-beijing.aliyuncs.com/images/typora/1594025518679.png" alt="1594025518679"></p>
<p>5）提交到生产环境</p>
<p><img src="https://leinuo-blog.oss-cn-beijing.aliyuncs.com/images/typora/1594025524520.png" alt="1594025524520"></p>
<h3 id="5-7-2-手动将-将-DWD-层数据导入-DWS-层"><a href="#5-7-2-手动将-将-DWD-层数据导入-DWS-层" class="headerlink" title="5.7.2 手动将 将 DWD 层数据导入 DWS 层"></a>5.7.2 手动将 将 DWD 层数据导入 DWS 层</h3><p>1）在临时查询页面，把 DWD 层 dwd_start_log 里面的数据导入到 dws_uv_detail_d</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">table</span> dws_uv_detail_d <span class="keyword">partition</span>(ds,hh,mm)</span><br><span class="line"><span class="keyword">select</span></span><br><span class="line"><span class="keyword">mid</span>,</span><br><span class="line">user_id,</span><br><span class="line">version_code,</span><br><span class="line">version_name,</span><br><span class="line">lang,</span><br><span class="line"><span class="keyword">source</span>,</span><br><span class="line">os,</span><br><span class="line">area,</span><br><span class="line"><span class="keyword">model</span>,</span><br><span class="line">brand,</span><br><span class="line">sdk_version,</span><br><span class="line">email,</span><br><span class="line">height_width,</span><br><span class="line">network,</span><br><span class="line">lng,</span><br><span class="line">lat,</span><br><span class="line">event_time,</span><br><span class="line">ds,</span><br><span class="line">hh,</span><br><span class="line">mm</span><br><span class="line"><span class="keyword">from</span></span><br><span class="line">(</span><br><span class="line"><span class="keyword">select</span></span><br><span class="line">*,</span><br><span class="line">ROW_NUMBER() <span class="keyword">OVER</span>(<span class="keyword">PARTITION</span> <span class="keyword">BY</span> <span class="keyword">mid</span> <span class="keyword">ORDER</span> <span class="keyword">BY</span> event_time</span><br><span class="line"><span class="keyword">asc</span>) rn</span><br><span class="line"><span class="keyword">from</span> dwd_start_log</span><br><span class="line"><span class="keyword">where</span> ds=<span class="string">'20191008'</span></span><br><span class="line">) st <span class="keyword">where</span> rn = <span class="number">1</span>;</span><br></pre></td></tr></table></figure>

<p>2）查看导入结果</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">SELECT * from dws_uv_detail_d WHERE ds&#x3D;&#39;20191008&#39;;</span><br></pre></td></tr></table></figure>

<h3 id="5-7-3-数据导入脚本"><a href="#5-7-3-数据导入脚本" class="headerlink" title="5.7.3 数据导入脚本"></a>5.7.3 数据导入脚本</h3><p>DWS 层一般围绕某个主题进行聚合、拼接处理。<br>针对统计日活的需求，DWS 主要的工作就进行以日为单位的去重操作。</p>
<p>1）在流程中加入一个数据开发脚本</p>
<p><img src="https://leinuo-blog.oss-cn-beijing.aliyuncs.com/images/typora/1594025783457.png" alt="1594025783457"></p>
<p><img src="https://leinuo-blog.oss-cn-beijing.aliyuncs.com/images/typora/1594025787737.png" alt="1594025787737"></p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">table</span> dws_uv_detail_d <span class="keyword">partition</span>(ds,hh,mm)</span><br><span class="line"><span class="keyword">select</span></span><br><span class="line"><span class="keyword">mid</span>,</span><br><span class="line">user_id,</span><br><span class="line">version_code,</span><br><span class="line">version_name,</span><br><span class="line">lang,</span><br><span class="line"><span class="keyword">source</span>,</span><br><span class="line">os,</span><br><span class="line">area,</span><br><span class="line"><span class="keyword">model</span>,</span><br><span class="line">brand,</span><br><span class="line">sdk_version,</span><br><span class="line">email,</span><br><span class="line">height_width,</span><br><span class="line">network,</span><br><span class="line">lng,</span><br><span class="line">lat,</span><br><span class="line">event_time,</span><br><span class="line">ds,</span><br><span class="line">hh,</span><br><span class="line">mm</span><br><span class="line"><span class="keyword">from</span></span><br><span class="line">(</span><br><span class="line"><span class="keyword">select</span></span><br><span class="line">*,</span><br><span class="line">ROW_NUMBER() <span class="keyword">OVER</span>(<span class="keyword">PARTITION</span> <span class="keyword">BY</span> <span class="keyword">mid</span> <span class="keyword">ORDER</span> <span class="keyword">BY</span> event_time</span><br><span class="line"><span class="keyword">asc</span>) rn</span><br><span class="line"><span class="keyword">from</span> dwd_start_log</span><br><span class="line"><span class="keyword">where</span> ds = <span class="string">'$&#123;bizdate&#125;'</span></span><br><span class="line">) st <span class="keyword">where</span> rn = <span class="number">1</span>;</span><br></pre></td></tr></table></figure>

<p>2）配置参数<br>点击调度配置-&gt; bizdate=${yyyymmdd}</p>
<h3 id="5-8-应用数据层（ADS-层）"><a href="#5-8-应用数据层（ADS-层）" class="headerlink" title="5.8 应用数据层（ADS 层）"></a>5.8 应用数据层（ADS 层）</h3><p>统计各个渠道的 uv 个数</p>
<h4 id="5-8-1-建表语句"><a href="#5-8-1-建表语句" class="headerlink" title="5.8.1 建表语句"></a>5.8.1 建表语句</h4><p>1）创建表 ads_uv_source_d</p>
<p><img src="https://leinuo-blog.oss-cn-beijing.aliyuncs.com/images/typora/1594025826738.png" alt="1594025826738"></p>
<p>2）点击 DDL 模式</p>
<p><img src="https://leinuo-blog.oss-cn-beijing.aliyuncs.com/images/typora/1594025833213.png" alt="1594025833213"></p>
<p>3）在 DDL 模式中添加建表语句</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> <span class="string">`ads_uv_source_d`</span> (</span><br><span class="line"><span class="string">`source`</span> <span class="keyword">string</span> <span class="keyword">COMMENT</span> <span class="string">'渠道'</span>,</span><br><span class="line"><span class="string">`ct`</span> <span class="built_in">bigint</span> <span class="keyword">COMMENT</span> <span class="string">'个数'</span></span><br><span class="line">)</span><br><span class="line"><span class="keyword">COMMENT</span> <span class="string">'日活渠道统计'</span></span><br><span class="line">PARTITIONED <span class="keyword">BY</span> (ds <span class="keyword">string</span>);</span><br></pre></td></tr></table></figure>

<p>4）补全建表信息描述</p>
<p><img src="https://leinuo-blog.oss-cn-beijing.aliyuncs.com/images/typora/1594025854812.png" alt="1594025854812"></p>
<p><img src="https://leinuo-blog.oss-cn-beijing.aliyuncs.com/images/typora/1594025861415.png" alt="1594025861415"></p>
<p>5）提交到生产环境</p>
<p><img src="https://leinuo-blog.oss-cn-beijing.aliyuncs.com/images/typora/1594025867648.png" alt="1594025867648"></p>
<h3 id="5-8-2-数据导入脚本"><a href="#5-8-2-数据导入脚本" class="headerlink" title="5.8.2 数据导入脚本"></a>5.8.2 数据导入脚本</h3><p>1）在流程中加入一个数据开发脚本</p>
<p><img src="https://leinuo-blog.oss-cn-beijing.aliyuncs.com/images/typora/1594025879754.png" alt="1594025879754"></p>
<p><img src="https://leinuo-blog.oss-cn-beijing.aliyuncs.com/images/typora/1594025886943.png" alt="1594025886943"></p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">insert</span> OVERWRITE <span class="keyword">table</span> ads_uv_source_d <span class="keyword">PARTITION</span></span><br><span class="line">(ds=<span class="string">'$&#123;bizdate&#125;'</span>)</span><br><span class="line"><span class="keyword">SELECT</span></span><br><span class="line"><span class="keyword">source</span>,</span><br><span class="line"><span class="keyword">COUNT</span>(*) ct</span><br><span class="line"><span class="keyword">from</span> dws_uv_detail_d</span><br><span class="line"><span class="keyword">where</span> ds=<span class="string">'$&#123;bizdate&#125;'</span></span><br><span class="line"><span class="keyword">group</span> <span class="keyword">by</span> <span class="keyword">source</span>;</span><br></pre></td></tr></table></figure>

<p>2）配置参数<br>点击调度配置-&gt; bizdate=${yyyymmdd}</p>
<h3 id="5-9-日活-需求：-全流程-业务调度"><a href="#5-9-日活-需求：-全流程-业务调度" class="headerlink" title="5.9 日活 需求： 全流程 业务调度"></a>5.9 日活 需求： 全流程 业务调度</h3><p>1）点击业务 1，右侧就会出现之前写的脚本。默认三个脚本之间没有关系，可以根据业务需求，手动连线。</p>
<p><img src="https://leinuo-blog.oss-cn-beijing.aliyuncs.com/images/typora/1594025927419.png" alt="1594025927419"></p>
<p>2）点击执行</p>
<p><img src="https://leinuo-blog.oss-cn-beijing.aliyuncs.com/images/typora/1594025933834.png" alt="1594025933834"></p>
<p>3）查询运行日志</p>
<p><img src="https://leinuo-blog.oss-cn-beijing.aliyuncs.com/images/typora/1594025940991.png" alt="1594025940991"></p>
<p>4）临时查询，检查结果</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> * <span class="keyword">from</span> ads_uv_source_d <span class="keyword">WHERE</span> ds=<span class="string">'20191008'</span>;</span><br></pre></td></tr></table></figure>

<p><img src="https://leinuo-blog.oss-cn-beijing.aliyuncs.com/images/typora/1594025962371.png" alt="1594025962371"></p>
]]></content>
      <categories>
        <category>大数据</category>
        <category>大数据实战项目</category>
        <category>阿里云离线数仓</category>
        <category>2.用户行为数仓搭建</category>
      </categories>
      <tags>
        <tag>大数据</tag>
      </tags>
  </entry>
  <entry>
    <title>4.业务数仓搭建</title>
    <url>/2020/04/07/4.%E4%B8%9A%E5%8A%A1%E6%95%B0%E4%BB%93%E6%90%AD%E5%BB%BA/</url>
    <content><![CDATA[<h2 id="第-7-章-业务数仓搭建"><a href="#第-7-章-业务数仓搭建" class="headerlink" title="第 7 章 业务数仓搭建"></a>第 7 章 业务数仓搭建</h2><h3 id="7-1-业务-数仓-架构图"><a href="#7-1-业务-数仓-架构图" class="headerlink" title="7.1 业务 数仓 架构图"></a>7.1 业务 数仓 架构图</h3><h4 id="7-1-1-业务数仓系统流程设计"><a href="#7-1-1-业务数仓系统流程设计" class="headerlink" title="7.1.1 业务数仓系统流程设计"></a>7.1.1 业务数仓系统流程设计</h4><h4 id="7-1-2-业务表结构"><a href="#7-1-2-业务表结构" class="headerlink" title="7.1.2 业务表结构"></a>7.1.2 业务表结构</h4><p><img src="https://leinuo-blog.oss-cn-beijing.aliyuncs.com/images/typora/1594026298145.png" alt="1594026298145"></p>
<h4 id="7-1-3-业务数仓分层"><a href="#7-1-3-业务数仓分层" class="headerlink" title="7.1.3 业务数仓分层"></a>7.1.3 业务数仓分层</h4><p><img src="https://leinuo-blog.oss-cn-beijing.aliyuncs.com/images/typora/1594026326050.png" alt="1594026326050"></p>
<h3 id="7-2-RDS-服务器准备"><a href="#7-2-RDS-服务器准备" class="headerlink" title="7.2 RDS 服务器准备"></a>7.2 RDS 服务器准备</h3><h4 id="7-2-1-RDS-服务器购买"><a href="#7-2-1-RDS-服务器购买" class="headerlink" title="7.2.1 RDS 服务器购买"></a>7.2.1 RDS 服务器购买</h4><p>阿里云关系型数据库（Relational Database Service，简称 RDS）是一种稳定可靠、可弹性伸缩的在线数据库服务。</p>
<p>购买 RDS for MySQL 服务器：<a href="https://www.aliyun.com/product/rds/mysql" target="_blank" rel="noopener">https://www.aliyun.com/product/rds/mysql</a></p>
<p>1）点击立即购买</p>
<p><img src="https://leinuo-blog.oss-cn-beijing.aliyuncs.com/images/typora/1594026360901.png" alt="1594026360901"></p>
<p>2）购买 RDS 服务器配置</p>
<p><img src="https://leinuo-blog.oss-cn-beijing.aliyuncs.com/images/typora/1594026371006.png" alt="1594026371006"></p>
<p><img src="https://leinuo-blog.oss-cn-beijing.aliyuncs.com/images/typora/1594026376061.png" alt="1594026376061"></p>
<p>3）点击立即购买</p>
<p><img src="https://leinuo-blog.oss-cn-beijing.aliyuncs.com/images/typora/1594026383946.png" alt="1594026383946"></p>
<p><img src="https://leinuo-blog.oss-cn-beijing.aliyuncs.com/images/typora/1594026390486.png" alt="1594026390486"></p>
<p><img src="https://leinuo-blog.oss-cn-beijing.aliyuncs.com/images/typora/1594026393372.png" alt="1594026393372"></p>
<p>4）控制台列表</p>
<p><img src="https://leinuo-blog.oss-cn-beijing.aliyuncs.com/images/typora/1594026399967.png" alt="1594026399967"></p>
<p>注意：实例一旦创建，服务开始计费，使用过程中不能停机，只能释放实例。</p>
<h4 id="7-2-2-RDS-服务器配置"><a href="#7-2-2-RDS-服务器配置" class="headerlink" title="7.2.2 RDS 服务器配置"></a>7.2.2 RDS 服务器配置</h4><p>服务建立好以后，首先要建立连接通道，可以让用户远程操控 RDS 服务器。</p>
<p>1）从列表中点入实例</p>
<p><img src="https://leinuo-blog.oss-cn-beijing.aliyuncs.com/images/typora/1594026427418.png" alt="1594026427418"></p>
<p>2）配置白名单</p>
<p>白名单：白名单上的 IP 地址，是可以访问该 RDS 服务器，其他 IP 地址都拒绝。<br>白名单的 IP 地址都是本机对外显示的外网地址。获取外面 IP 地址的方法，就是在百度上搜索 IP 查询。</p>
<p><img src="https://leinuo-blog.oss-cn-beijing.aliyuncs.com/images/typora/1594026451487.png" alt="1594026451487"></p>
<p>（1）点击添加白名单分组</p>
<p><img src="https://leinuo-blog.oss-cn-beijing.aliyuncs.com/images/typora/1594026461958.png" alt="1594026461958"></p>
<p>（2）配置白名单详情</p>
<p><img src="https://leinuo-blog.oss-cn-beijing.aliyuncs.com/images/typora/1594026475008.png" alt="1594026475008"></p>
<p>3）基本信息-&gt;申请外网地址</p>
<p><img src="https://leinuo-blog.oss-cn-beijing.aliyuncs.com/images/typora/1594026482709.png" alt="1594026482709"></p>
<p><img src="https://leinuo-blog.oss-cn-beijing.aliyuncs.com/images/typora/1594026487908.png" alt="1594026487908"></p>
<p><img src="https://leinuo-blog.oss-cn-beijing.aliyuncs.com/images/typora/1594026492788.png" alt="1594026492788"></p>
<p>框住的部分就是，用户可以通过客户端工具直接访问的地址。</p>
<p>4）账号管理-&gt;创建账号</p>
<p><img src="https://leinuo-blog.oss-cn-beijing.aliyuncs.com/images/typora/1594026506200.png" alt="1594026506200"></p>
<p><img src="https://leinuo-blog.oss-cn-beijing.aliyuncs.com/images/typora/1594026510629.png" alt="1594026510629"></p>
<h4 id="7-2-3-RDS-服务器购买"><a href="#7-2-3-RDS-服务器购买" class="headerlink" title="7.2.3 RDS 服务器购买"></a>7.2.3 RDS 服务器购买</h4><p>1）利用客户端工具进行登录<br>注意：SQL 主机地址，一定是 RDS 服务器暴露的外网 IP 地址。</p>
<p><img src="https://leinuo-blog.oss-cn-beijing.aliyuncs.com/images/typora/1594026529858.png" alt="1594026529858"></p>
<p>一旦登录成功，那么 RDS 服务就和一台本地安装的服务器操作起来基本差不多。</p>
<h3 id="7-3-创建业务数据库及表"><a href="#7-3-创建业务数据库及表" class="headerlink" title="7.3 创建业务数据库及表"></a>7.3 创建业务数据库及表</h3><p>1）建立业务数据库</p>
<p><img src="https://leinuo-blog.oss-cn-beijing.aliyuncs.com/images/typora/1594026561546.png" alt="1594026561546"></p>
<p>2）导入数据库脚本</p>
<p><img src="https://leinuo-blog.oss-cn-beijing.aliyuncs.com/images/typora/1594026568213.png" alt="1594026568213"></p>
<p><img src="https://leinuo-blog.oss-cn-beijing.aliyuncs.com/images/typora/1594026571437.png" alt="1594026571437"></p>
<p>3）数据导入后可以看到有表、存储过程和函数（用于生成随机数据）。</p>
<p><img src="https://leinuo-blog.oss-cn-beijing.aliyuncs.com/images/typora/1594026579771.png" alt="1594026579771"></p>
<p>4）生成数据</p>
<p>在 MySQL 中执行以下语句<br>作用：调用存储过程生成某天的业务数据</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">CALL init_data(&#39;20191008&#39;,100,30,TRUE)</span><br></pre></td></tr></table></figure>

<p>参数 1：数据生成的日期，比如订单日期。<br>参数 2：生成的订单数。<br>参数 3：生成的用户数。<br>参数 4 ：是否清楚之前的订单及用户信息。</p>
<p><img src="https://leinuo-blog.oss-cn-beijing.aliyuncs.com/images/typora/1594026618565.png" alt="1594026618565"></p>
<h3 id="7-4-ODS-层数据表创建"><a href="#7-4-ODS-层数据表创建" class="headerlink" title="7.4 ODS 层数据表创建"></a>7.4 ODS 层数据表创建</h3><p>两种方式建表：<br>方式一：一张一张建表，和用户行为数仓中的方式一样；<br>方式二：可以在临时查询中统一执行建表语句。</p>
<p>本次我们采用第二种方式。</p>
<p>1）建表脚本</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">CREATE TABLE &#96;ods_order_info_di&#96; (</span><br><span class="line">&#96;id&#96; string COMMENT &#39;订单编号&#39;,</span><br><span class="line">&#96;total_amount&#96; double COMMENT &#39;订单金额&#39;,</span><br><span class="line">&#96;order_status&#96; string COMMENT &#39;订单状态&#39;,</span><br><span class="line">&#96;user_id&#96; string COMMENT &#39;用户 id&#39;,</span><br><span class="line">&#96;payment_way&#96; string COMMENT &#39;支付方式&#39;,</span><br><span class="line">&#96;out_trade_no&#96; string COMMENT &#39;支付流水号&#39;,</span><br><span class="line">&#96;create_time&#96; string COMMENT &#39;创建时间&#39;,</span><br><span class="line">&#96;operate_time&#96; string COMMENT &#39;操作时间&#39;,</span><br><span class="line">&#96;province_id&#96; string COMMENT &#39;省份&#39;</span><br><span class="line">)</span><br><span class="line">COMMENT &#39;订单表&#39;</span><br><span class="line">PARTITIONED BY (ds string);</span><br><span class="line">CREATE TABLE &#96;ods_order_detail_di&#96; (</span><br><span class="line">&#96;id&#96; string COMMENT &#39;明细 id&#39;,</span><br><span class="line">&#96;order_id&#96; string COMMENT &#39;订单 id&#39;,</span><br><span class="line">&#96;sku_id&#96; string COMMENT &#39;商品 id&#39;,</span><br><span class="line">&#96;sku_name&#96; string COMMENT &#39;商品名称&#39;,</span><br><span class="line">&#96;order_price&#96; double COMMENT &#39;购买价格&#39;,</span><br><span class="line">&#96;sku_num&#96; bigint COMMENT &#39;购物数量&#39;,</span><br><span class="line">&#96;create_time&#96; string COMMENT &#39;创建时间&#39;</span><br><span class="line">)</span><br><span class="line">COMMENT &#39;订单明细&#39;</span><br><span class="line">PARTITIONED BY (ds string);</span><br><span class="line">CREATE TABLE &#96;ods_sku_info_df&#96; (</span><br><span class="line">&#96;id&#96; string COMMENT &#39;skuid&#39;,</span><br><span class="line">&#96;spu_id&#96; string COMMENT &#39;spuid&#39;,</span><br><span class="line">&#96;price&#96; double COMMENT &#39;价格&#39;,</span><br><span class="line">&#96;sku_name&#96; string COMMENT &#39;商品名称&#39;,</span><br><span class="line">&#96;sku_desc&#96; string COMMENT &#39;商品描述&#39;,</span><br><span class="line">&#96;weight&#96; double COMMENT &#39;重量(千克)&#39;,</span><br><span class="line">&#96;tm_id&#96; string COMMENT &#39;品牌 id&#39;,</span><br><span class="line">&#96;category3_id&#96; string COMMENT &#39;品类 id&#39;,</span><br><span class="line">&#96;create_time&#96; string COMMENT &#39;创建时间&#39;</span><br><span class="line">)</span><br><span class="line">COMMENT &#39;商品信息&#39;</span><br><span class="line">PARTITIONED BY (ds string);</span><br><span class="line">CREATE TABLE &#96;ods_user_info_df&#96; (</span><br><span class="line">&#96;id&#96; string COMMENT &#39;用户 id&#39;,</span><br><span class="line">&#96;name&#96; string COMMENT &#39;姓名&#39;,</span><br><span class="line">&#96;birthday&#96; string COMMENT &#39;生日&#39;,</span><br><span class="line">&#96;gender&#96; string COMMENT &#39;性别&#39;,</span><br><span class="line">&#96;email&#96; string COMMENT &#39;邮箱&#39;,</span><br><span class="line">&#96;user_level&#96; string COMMENT &#39;用户等级&#39;,</span><br><span class="line">&#96;create_time&#96; string COMMENT &#39;创建时间&#39;</span><br><span class="line">)</span><br><span class="line">COMMENT &#39;用户信息&#39;</span><br><span class="line">PARTITIONED BY (ds string);</span><br><span class="line">CREATE TABLE &#96;ods_base_category3_df&#96; (</span><br><span class="line">&#96;id&#96; string COMMENT &#39;三级品类 id&#39;,</span><br><span class="line">&#96;name&#96; string COMMENT &#39;名称&#39;,</span><br><span class="line">&#96;category2_id&#96; string COMMENT &#39;二级品类 id&#39;</span><br><span class="line">)</span><br><span class="line">COMMENT &#39;三级品类信息&#39;</span><br><span class="line">PARTITIONED BY (ds string);</span><br><span class="line">CREATE TABLE &#96;ods_base_trademark_df&#96; (</span><br><span class="line">&#96;tm_id&#96; string COMMENT &#39;品牌 id&#39;,</span><br><span class="line">&#96;tm_name&#96; string COMMENT &#39;名称&#39;</span><br><span class="line">)</span><br><span class="line">COMMENT &#39;品牌信息&#39;</span><br><span class="line">PARTITIONED BY (ds string);</span><br><span class="line">CREATE TABLE &#96;ods_base_category1_df&#96; (</span><br><span class="line">&#96;id&#96; string COMMENT &#39;一级品类 id&#39;,</span><br><span class="line">&#96;name&#96; string COMMENT &#39;名称&#39;</span><br><span class="line">)</span><br><span class="line">COMMENT &#39;一级品类信息&#39;</span><br><span class="line">PARTITIONED BY (ds string);</span><br><span class="line">CREATE TABLE &#96;ods_base_category2_df&#96; (</span><br><span class="line">&#96;id&#96; string COMMENT &#39;二级品类 id&#39;,</span><br><span class="line">&#96;name&#96; string COMMENT &#39;名称&#39;,</span><br><span class="line">&#96;category1_id&#96; string COMMENT &#39;一级品类 id&#39;</span><br><span class="line">)</span><br><span class="line">COMMENT &#39;二级品类信息&#39;</span><br><span class="line">PARTITIONED BY (ds string);</span><br><span class="line">CREATE TABLE &#96;ods_payment_info_di&#96; (</span><br><span class="line">&#96;id&#96; bigint COMMENT &#39;编号&#39;,</span><br><span class="line">&#96;out_trade_no&#96; string COMMENT &#39;对外业务编号&#39;,</span><br><span class="line">&#96;order_id&#96; string COMMENT &#39;订单编号&#39;,</span><br><span class="line">&#96;user_id&#96; string COMMENT &#39;用户编号&#39;,</span><br><span class="line">&#96;alipay_trade_no&#96; string COMMENT &#39;支付宝交易流水编号&#39;,</span><br><span class="line">&#96;total_amount&#96; double COMMENT &#39;支付金额&#39;,</span><br><span class="line">&#96;subject&#96; string COMMENT &#39;交易内容&#39;,</span><br><span class="line">&#96;payment_type&#96; string COMMENT &#39;支付类型&#39;,</span><br><span class="line">&#96;payment_time&#96; string COMMENT &#39;支付时间&#39;</span><br><span class="line">)</span><br><span class="line">COMMENT &#39;支付流水表&#39;</span><br><span class="line">PARTITIONED BY (ds string);</span><br><span class="line">CREATE TABLE &#96;ods_base_region_df&#96; (</span><br><span class="line">&#96;id&#96; bigint COMMENT &#39;地区 id&#39;,</span><br><span class="line">&#96;region_name&#96; string COMMENT &#39;地区名称&#39;</span><br><span class="line">)</span><br><span class="line">COMMENT &#39;地区&#39;</span><br><span class="line">PARTITIONED BY (ds string);</span><br><span class="line">CREATE TABLE &#96;ods_base_province_df&#96; (</span><br><span class="line">&#96;id&#96; bigint COMMENT &#39;品牌 id&#39;,</span><br><span class="line">&#96;name&#96; string COMMENT &#39;名称&#39;,</span><br><span class="line">&#96;region_id&#96; bigint COMMENT &#39;地区 id&#39;</span><br><span class="line">)</span><br><span class="line">COMMENT &#39;省份&#39;</span><br><span class="line">PARTITIONED BY (ds string);</span><br></pre></td></tr></table></figure>

<p>2）在表管理里面查看创建的表</p>
<p><img src="https://leinuo-blog.oss-cn-beijing.aliyuncs.com/images/typora/1594026728423.png" alt="1594026728423"></p>
<p>3）数据开发-&gt;ods-&gt;导入表</p>
<p><img src="https://leinuo-blog.oss-cn-beijing.aliyuncs.com/images/typora/1594026737315.png" alt="1594026737315"></p>
<p>4）选择刚创建的所有表</p>
<p><img src="https://leinuo-blog.oss-cn-beijing.aliyuncs.com/images/typora/1594026746734.png" alt="1594026746734"></p>
<h3 id="7-5-数据同步"><a href="#7-5-数据同步" class="headerlink" title="7.5 数据同步"></a>7.5 数据同步</h3><p>目前 MySQL 里面的数据已经有了，ODS 层表也已经建好，现在需要创建一个脚本，将MySQL 中数据同步到 ODS 层对应的表。</p>
<h4 id="7-5-1-建立数据同步节点"><a href="#7-5-1-建立数据同步节点" class="headerlink" title="7.5.1 建立数据同步节点"></a>7.5.1 建立数据同步节点</h4><p>1）数据集成-&gt;新建数据集成节点-&gt;数据同步</p>
<p><img src="https://leinuo-blog.oss-cn-beijing.aliyuncs.com/images/typora/1594026777737.png" alt="1594026777737"></p>
<p>2）填写节点名称（表名+后缀，见名知意就好，例如：ods_user_info_sync）</p>
<p><img src="https://leinuo-blog.oss-cn-beijing.aliyuncs.com/images/typora/1594026786032.png" alt="1594026786032"></p>
<p>3）鼠标悬浮在选择数据源右侧的问号上-&gt;数据源进行新建操作</p>
<p><img src="https://leinuo-blog.oss-cn-beijing.aliyuncs.com/images/typora/1594026794452.png" alt="1594026794452"></p>
<p>4）点击新增数据源-&gt;点击 MySQL</p>
<p><img src="https://leinuo-blog.oss-cn-beijing.aliyuncs.com/images/typora/1594026830355.png" alt="1594026830355"></p>
<p>5）配置新增 MySQL 数据源</p>
<p><img src="https://leinuo-blog.oss-cn-beijing.aliyuncs.com/images/typora/1594026838611.png" alt="1594026838611"></p>
<p>（1）数据源名称：可以任意取，这里取的 gmall_rds</p>
<p>（2）RDS 实例 ID：根据问号的提示获取，每个人的不一样。rm-bp1t1li837a1v9gb8</p>
<p><img src="https://leinuo-blog.oss-cn-beijing.aliyuncs.com/images/typora/1594026856395.png" alt="1594026856395"></p>
<p>（3）RDS 实例主账号 ID：根据问号提示获取，每个人的不一样。1902761552218725</p>
<p><img src="https://leinuo-blog.oss-cn-beijing.aliyuncs.com/images/typora/1594026867775.png" alt="1594026867775"></p>
<p>（4）数据库名：要连接的数据库名称，我这里是 gmall<br>（5）用户名、密码：要连接的数据库的用户名和密码<br>（6）白名单配置：因为要用 MaxCompute 访问 RDS 所以要给 RDS 增加对应的白名单IP 地址。</p>
<p>a）点击点我查看如何添加白名单按钮</p>
<p><img src="https://leinuo-blog.oss-cn-beijing.aliyuncs.com/images/typora/1594026886825.png" alt="1594026886825"></p>
<p>b）根据购买的 RDS 服务器地址，选择对应 IP 地址</p>
<table>
<thead>
<tr>
<th>区域</th>
<th>白名单</th>
</tr>
</thead>
<tbody><tr>
<td>华东 1（杭州）</td>
<td>100.64.0.0/10,11.193.102.0/24,11.193.215.0/24,11.194.110.0/24,11.194.73.0/24,118.31.157.0/24,47.97.53.0/24,11.196.23.0/24,47.99.12.0/24,47.99.13.0/24,114.55.197.0/24,11.197.246.0/24,11.197.247.0/24</td>
</tr>
</tbody></table>
<p>c）在 RDS 中添加白名单</p>
<p><img src="https://leinuo-blog.oss-cn-beijing.aliyuncs.com/images/typora/1594026911785.png" alt="1594026911785"></p>
<p><img src="https://leinuo-blog.oss-cn-beijing.aliyuncs.com/images/typora/1594026919614.png" alt="1594026919614"></p>
<p>5）配置完新增 MySQL 数据源，就会在数据集成窗口发现增加了一个 gmall_rds 数据源</p>
<p><img src="https://leinuo-blog.oss-cn-beijing.aliyuncs.com/images/typora/1594027080728.png" alt="1594027080728"></p>
<p>6）再次回到 DataStudio 工作空间，发现就可以选择数据源了</p>
<p><img src="https://leinuo-blog.oss-cn-beijing.aliyuncs.com/images/typora/1594027089159.png" alt="1594027089159"></p>
<p>7）配置数据去向数据源-&gt;鼠标悬浮问号上面-&gt;点击数据源进行新建操作-&gt;新增数据源MaxCompute（ODPS）</p>
<p><img src="https://leinuo-blog.oss-cn-beijing.aliyuncs.com/images/typora/1594027100397.png" alt="1594027100397"></p>
<p><img src="https://leinuo-blog.oss-cn-beijing.aliyuncs.com/images/typora/1594027103932.png" alt="1594027103932"></p>
<p>8）配置 MaxCompute 数据源详情</p>
<p><img src="https://leinuo-blog.oss-cn-beijing.aliyuncs.com/images/typora/1594027123138.png" alt="1594027123138"></p>
<p>AccessKey ID：LTAI4FiU71dZAL17SdLBa6Nt<br>Access Key Secret：63YzSmqMOSjDR5A2ZXEzFLM2tREY6m</p>
<p><img src="https://leinuo-blog.oss-cn-beijing.aliyuncs.com/images/typora/1594027132095.png" alt="1594027132095"></p>
<h4 id="7-5-2-每日全量表同步"><a href="#7-5-2-每日全量表同步" class="headerlink" title="7.5.2 每日全量表同步"></a>7.5.2 每日全量表同步</h4><p>用户表同步策略：每日全量<br>每 日 全 量 导 入 的 表 包 括 ： ods_user_info 、 ods_base_category1 、 ods_base_category2 、<br>ods_base_category3、ods_base_province、ods_base_region、ods_base_trademark、ods_sku_info<br>1）配置完数据源后-&gt;点击运行（点击运行前，检查字段映射关系</p>
<p><img src="https://leinuo-blog.oss-cn-beijing.aliyuncs.com/images/typora/1594027147527.png" alt="1594027147527"></p>
<p><img src="https://leinuo-blog.oss-cn-beijing.aliyuncs.com/images/typora/1594027150410.png" alt="1594027150410"></p>
<p><img src="https://leinuo-blog.oss-cn-beijing.aliyuncs.com/images/typora/1594027181753.png" alt="1594027181753"></p>
<p>2）临时查询，验证结果</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> ods_user_info_df <span class="keyword">where</span> ds=<span class="string">'20191008'</span>;</span><br></pre></td></tr></table></figure>

<p><img src="https://leinuo-blog.oss-cn-beijing.aliyuncs.com/images/typora/1594027200557.png" alt="1594027200557"></p>
<p>3）重复执行步骤 1-2，依次导入：ods_base_category1、ods_base_category2、ods_base_category3、<br>ods_base_province、ods_base_region、ods_base_trademark、ods_sku_info</p>
<h3 id="7-5-3-每日增量表同步"><a href="#7-5-3-每日增量表同步" class="headerlink" title="7.5.3 每日增量表同步"></a>7.5.3 每日增量表同步</h3><p>1）同步策略：每日增量<br>每日新增的表包括：ods_order_detail</p>
<p><img src="https://leinuo-blog.oss-cn-beijing.aliyuncs.com/images/typora/1594027215666.png" alt="1594027215666"></p>
<p>每日增量的区别就是要按照日期进行过滤，只筛选出今天新产生的数据<br>条件：</p>
<figure class="highlight"><table><tr><td class="code"><pre><span class="line">DATE_FORMAT(create_time,'%Y%m%d')='$&#123;bizdate&#125;'</span><br></pre></td></tr></table></figure>

<p>注意：此处必须是 MySql 的函数语法，不是 Hive 的。</p>
<p>${bizdate}是系统自带参数用于取前一天的日期。</p>
<p>2）临时查询，验证结果</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> ods_order_detail_di <span class="keyword">WHERE</span> ds=<span class="string">'20191008'</span>;</span><br></pre></td></tr></table></figure>

<h3 id="7-5-4-每日新增及-变化-表同步"><a href="#7-5-4-每日新增及-变化-表同步" class="headerlink" title="7.5.4 每日新增及 变化 表同步"></a>7.5.4 每日新增及 变化 表同步</h3><p>1）同步策略：每日新增及变化<br>每日新增及变化的表包括：ods_order_info</p>
<p><img src="https://leinuo-blog.oss-cn-beijing.aliyuncs.com/images/typora/1594027281383.png" alt="1594027281383"></p>
<p>条件</p>
<figure class="highlight"><table><tr><td class="code"><pre><span class="line">DATE_FORMAT(create_time,'%Y%m%d')='$&#123;bizdate&#125;' or</span><br><span class="line">DATE_FORMAT(operate_time,'%Y%m%d')='$&#123;bizdate&#125;'</span><br></pre></td></tr></table></figure>

<p>2）临时查询，验证结果</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> ods_order_info_di <span class="keyword">WHERE</span> ds=<span class="string">'20191008'</span>;</span><br></pre></td></tr></table></figure>

<p><img src="https://leinuo-blog.oss-cn-beijing.aliyuncs.com/images/typora/1594027317656.png" alt="1594027317656"></p>
<h3 id="7-5-5-ODS-层"><a href="#7-5-5-ODS-层" class="headerlink" title="7.5.5 ODS 层"></a>7.5.5 ODS 层</h3><p>把节点从左侧拖放至右侧，按照上下游的关系用线连接好。</p>
<p><img src="https://leinuo-blog.oss-cn-beijing.aliyuncs.com/images/typora/1594027331298.png" alt="1594027331298"></p>
<h3 id="7-6-DWD-层"><a href="#7-6-DWD-层" class="headerlink" title="7.6 DWD 层"></a>7.6 DWD 层</h3><p>DWD 层，一般是对 ODS 层数据进行一定的清洗加工，如果是面对关系导入过来的数<br>据表，还要把原本的关系型表结构，进行一定程度的维度退化。作为更易处理的明细数据。<br>比如：</p>
<p>ODS 地区 + ODS 省份=&gt; DWD 省份地区<br>ODS 商品信息 + ODS 品牌 + ODS 商品一级分类 + ODS 商品二级分类 + ODS 商品三级分类=&gt;DWD 商品信息<br>DWD 层表结构如下图所示</p>
<p><img src="https://leinuo-blog.oss-cn-beijing.aliyuncs.com/images/typora/1594027399374.png" alt="1594027399374"></p>
<h3 id="7-6-1-建表语句"><a href="#7-6-1-建表语句" class="headerlink" title="7.6.1 建表语句"></a>7.6.1 建表语句</h3><p>1）临时查询中，执行建表语句</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> <span class="string">`dwd_order_info_di`</span> (</span><br><span class="line"><span class="string">`id`</span> <span class="keyword">string</span> <span class="keyword">COMMENT</span> <span class="string">'订单 id'</span>,</span><br><span class="line"><span class="string">`total_amount`</span> <span class="keyword">double</span> <span class="keyword">COMMENT</span> <span class="string">'订单总额'</span>,</span><br><span class="line"><span class="string">`order_status`</span> <span class="keyword">string</span> <span class="keyword">COMMENT</span> <span class="string">' 1 未支付 2 已支付 3 已发货 4 已</span></span><br><span class="line"><span class="string">收货 5 完成'</span>,</span><br><span class="line"><span class="string">`user_id`</span> <span class="keyword">string</span> <span class="keyword">COMMENT</span> <span class="string">'用户 id'</span>,</span><br><span class="line"><span class="string">`payment_way`</span> <span class="keyword">string</span> <span class="keyword">COMMENT</span> <span class="string">'付款方式'</span>,</span><br><span class="line"><span class="string">`out_trade_no`</span> <span class="keyword">string</span> <span class="keyword">COMMENT</span> <span class="string">'订单流失号'</span>,</span><br><span class="line"><span class="string">`province_id`</span> <span class="keyword">string</span> <span class="keyword">COMMENT</span> <span class="string">'省市 id'</span>,</span><br><span class="line"><span class="string">`create_time`</span> <span class="keyword">string</span> <span class="keyword">COMMENT</span> <span class="string">'创建时间'</span>,</span><br><span class="line"><span class="string">`operate_time`</span> <span class="keyword">string</span> <span class="keyword">COMMENT</span> <span class="string">'修改时间'</span></span><br><span class="line">)</span><br><span class="line"><span class="keyword">COMMENT</span> <span class="string">'订单表'</span></span><br><span class="line">PARTITIONED <span class="keyword">BY</span> (ds <span class="keyword">string</span>);</span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> <span class="string">`dwd_order_detail_di`</span> (</span><br><span class="line"><span class="string">`id`</span> <span class="keyword">string</span> <span class="keyword">COMMENT</span> <span class="string">'明细 id'</span>,</span><br><span class="line"><span class="string">`order_id`</span> <span class="keyword">string</span> <span class="keyword">COMMENT</span> <span class="string">'订单 id'</span>,</span><br><span class="line"><span class="string">`user_id`</span> <span class="keyword">string</span> <span class="keyword">COMMENT</span> <span class="string">'用户 id'</span>,</span><br><span class="line"><span class="string">`sku_id`</span> <span class="keyword">string</span> <span class="keyword">COMMENT</span> <span class="string">'商品 id'</span>,</span><br><span class="line"><span class="string">`sku_name`</span> <span class="keyword">string</span> <span class="keyword">COMMENT</span> <span class="string">'商品名称'</span>,</span><br><span class="line"><span class="string">`order_price`</span> <span class="keyword">string</span> <span class="keyword">COMMENT</span> <span class="string">'购买价格'</span>,</span><br><span class="line"><span class="string">`sku_num`</span> <span class="keyword">string</span> <span class="keyword">COMMENT</span> <span class="string">'购物数量'</span>,</span><br><span class="line"><span class="string">`province_id`</span> <span class="keyword">string</span> <span class="keyword">COMMENT</span> <span class="string">'省市 id'</span>,</span><br><span class="line"><span class="string">`create_time`</span> <span class="keyword">string</span> <span class="keyword">COMMENT</span> <span class="string">'创建时间'</span></span><br><span class="line">)</span><br><span class="line"><span class="keyword">COMMENT</span> <span class="string">'订单明细'</span></span><br><span class="line">PARTITIONED <span class="keyword">BY</span> (ds <span class="keyword">string</span>);</span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> <span class="string">`dim_sku_info_df`</span> (</span><br><span class="line"><span class="string">`id`</span> <span class="keyword">string</span> <span class="keyword">COMMENT</span> <span class="string">'商品 id'</span>,</span><br><span class="line"><span class="string">`spu_id`</span> <span class="keyword">string</span> <span class="keyword">COMMENT</span> <span class="string">'spuid'</span>,</span><br><span class="line"><span class="string">`price`</span> <span class="keyword">double</span> <span class="keyword">COMMENT</span> <span class="string">'商品价格'</span>,</span><br><span class="line"><span class="string">`sku_name`</span> <span class="keyword">string</span> <span class="keyword">COMMENT</span> <span class="string">'商品名称'</span>,</span><br><span class="line"><span class="string">`sku_desc`</span> <span class="keyword">string</span> <span class="keyword">COMMENT</span> <span class="string">'商品描述'</span>,</span><br><span class="line"><span class="string">`weight`</span> <span class="keyword">double</span> <span class="keyword">COMMENT</span> <span class="string">'重量'</span>,</span><br><span class="line"><span class="string">`tm_id`</span> <span class="keyword">string</span> <span class="keyword">COMMENT</span> <span class="string">'品牌 id'</span>,</span><br><span class="line"><span class="string">`tm_name`</span> <span class="keyword">string</span> <span class="keyword">COMMENT</span> <span class="string">'品牌名称'</span>,</span><br><span class="line"><span class="string">`category3_id`</span> <span class="keyword">string</span> <span class="keyword">COMMENT</span> <span class="string">'三级分类 id'</span>,</span><br><span class="line"><span class="string">`category2_id`</span> <span class="keyword">string</span> <span class="keyword">COMMENT</span> <span class="string">'二级分类 id'</span>,</span><br><span class="line"><span class="string">`category1_id`</span> <span class="keyword">string</span> <span class="keyword">COMMENT</span> <span class="string">'一级分类 id'</span>,</span><br><span class="line"><span class="string">`category3_name`</span> <span class="keyword">string</span> <span class="keyword">COMMENT</span> <span class="string">'三级分类名称'</span>,</span><br><span class="line"><span class="string">`category2_name`</span> <span class="keyword">string</span> <span class="keyword">COMMENT</span> <span class="string">'二级分类名称'</span>,</span><br><span class="line"><span class="string">`category1_name`</span> <span class="keyword">string</span> <span class="keyword">COMMENT</span> <span class="string">'一级分类名称'</span>,</span><br><span class="line"><span class="string">`create_time`</span> <span class="keyword">string</span> <span class="keyword">COMMENT</span> <span class="string">'创建时间'</span></span><br><span class="line">)</span><br><span class="line"><span class="keyword">COMMENT</span> <span class="string">'商品表信息'</span></span><br><span class="line">PARTITIONED <span class="keyword">BY</span> (ds <span class="keyword">string</span>);</span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> <span class="string">`dim_user_info_df`</span> (</span><br><span class="line"><span class="string">`id`</span> <span class="keyword">string</span> <span class="keyword">COMMENT</span> <span class="string">'id'</span>,</span><br><span class="line"><span class="string">`name`</span> <span class="keyword">string</span> <span class="keyword">COMMENT</span> <span class="string">'用户名称'</span>,</span><br><span class="line"><span class="string">`birthday`</span> <span class="keyword">string</span> <span class="keyword">COMMENT</span> <span class="string">'生日'</span>,</span><br><span class="line"><span class="string">`gender`</span> <span class="keyword">string</span> <span class="keyword">COMMENT</span> <span class="string">'性别'</span>,</span><br><span class="line"><span class="string">`email`</span> <span class="keyword">string</span> <span class="keyword">COMMENT</span> <span class="string">'邮箱'</span>,</span><br><span class="line"><span class="string">`user_level`</span> <span class="keyword">string</span> <span class="keyword">COMMENT</span> <span class="string">'等级'</span>,</span><br><span class="line"><span class="string">`create_time`</span> <span class="keyword">string</span> <span class="keyword">COMMENT</span> <span class="string">'注册时间'</span></span><br><span class="line">)</span><br><span class="line"><span class="keyword">COMMENT</span> <span class="string">'用户信息表'</span></span><br><span class="line">PARTITIONED <span class="keyword">BY</span> (ds <span class="keyword">string</span>);</span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> <span class="string">`dim_base_province_df`</span> (</span><br><span class="line"><span class="string">`id`</span> <span class="keyword">string</span> <span class="keyword">COMMENT</span> <span class="string">'id'</span>,</span><br><span class="line"><span class="string">`province_name`</span> <span class="keyword">string</span> <span class="keyword">COMMENT</span> <span class="string">'省市名称'</span>,</span><br><span class="line"><span class="string">`region_id`</span> <span class="keyword">string</span> <span class="keyword">COMMENT</span> <span class="string">'地区 id'</span>,</span><br><span class="line"><span class="string">`region_name`</span> <span class="keyword">string</span> <span class="keyword">COMMENT</span> <span class="string">'地区名称'</span></span><br><span class="line">)</span><br><span class="line"><span class="keyword">COMMENT</span> <span class="string">'地区省市表'</span></span><br><span class="line">PARTITIONED <span class="keyword">BY</span> (ds <span class="keyword">string</span>);</span><br></pre></td></tr></table></figure>

<p>2）在表管理里面查看创建的表</p>
<p><img src="https://leinuo-blog.oss-cn-beijing.aliyuncs.com/images/typora/1594027438207.png" alt="1594027438207"></p>
<p>3）数据开发-&gt;dwd-&gt;导入表</p>
<p><img src="https://leinuo-blog.oss-cn-beijing.aliyuncs.com/images/typora/1594027445833.png" alt="1594027445833"></p>
<p>4）选择刚创建的所有表</p>
<p><img src="https://leinuo-blog.oss-cn-beijing.aliyuncs.com/images/typora/1594027460259.png" alt="1594027460259"></p>
<h3 id="7-6-2-手动将数据导入-DWD-层"><a href="#7-6-2-手动将数据导入-DWD-层" class="headerlink" title="7.6.2 手动将数据导入 DWD 层"></a>7.6.2 手动将数据导入 DWD 层</h3><p>1）在临时查询中执行</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">Insert</span> overwrite <span class="keyword">table</span> dwd_order_info_di <span class="keyword">partition</span>(ds)</span><br><span class="line"><span class="keyword">select</span> <span class="keyword">id</span>,</span><br><span class="line">total_amount,</span><br><span class="line">order_status,</span><br><span class="line">user_id,</span><br><span class="line">payment_way,</span><br><span class="line">out_trade_no,</span><br><span class="line">province_id,</span><br><span class="line">create_time,</span><br><span class="line">operate_time,</span><br><span class="line">ds</span><br><span class="line"><span class="keyword">from</span> ods_order_info_di</span><br><span class="line"><span class="keyword">where</span> ds=<span class="string">'$&#123;bizdate&#125;'</span> <span class="keyword">and</span> <span class="keyword">id</span> <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">null</span>;</span><br><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">table</span> dwd_order_detail_di <span class="keyword">partition</span>(ds)</span><br><span class="line"><span class="keyword">select</span> od.id,</span><br><span class="line">order_id,</span><br><span class="line">oi.user_id,</span><br><span class="line">sku_id,</span><br><span class="line">sku_name,</span><br><span class="line">order_price,</span><br><span class="line">sku_num,</span><br><span class="line">oi.province_id,</span><br><span class="line">od.create_time,</span><br><span class="line">od.ds</span><br><span class="line"><span class="keyword">from</span> ods_order_detail_di od <span class="keyword">join</span> ods_order_info_di oi</span><br><span class="line"><span class="keyword">on</span> od.order_id = oi.id <span class="keyword">and</span> oi.ds = <span class="string">'$&#123;bizdate&#125;'</span></span><br><span class="line"><span class="keyword">and</span> od.ds = <span class="string">'$&#123;bizdate&#125;'</span> <span class="keyword">and</span> od.id <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">null</span>;</span><br><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">table</span> dim_sku_info_df <span class="keyword">partition</span>(ds)</span><br><span class="line"><span class="keyword">select</span></span><br><span class="line">sku.id,</span><br><span class="line">sku.spu_id,</span><br><span class="line">sku.price,</span><br><span class="line">sku.sku_name,</span><br><span class="line">sku.sku_desc,</span><br><span class="line">sku.weight,</span><br><span class="line">sku.tm_id,</span><br><span class="line">tm.tm_name,</span><br><span class="line">sku.category3_id,</span><br><span class="line">c2.id category2_id ,</span><br><span class="line">c1.id category1_id,</span><br><span class="line">c3.name category3_name,</span><br><span class="line">c2.name category2_name,</span><br><span class="line">c1.name category1_name,</span><br><span class="line">sku.create_time,</span><br><span class="line">sku.ds</span><br><span class="line"><span class="keyword">from</span></span><br><span class="line">(</span><br><span class="line"><span class="keyword">select</span> *</span><br><span class="line"><span class="keyword">from</span> ods_sku_info_df</span><br><span class="line"><span class="keyword">where</span> ds=<span class="string">'$&#123;bizdate&#125;'</span> <span class="keyword">and</span> <span class="keyword">id</span> <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">null</span></span><br><span class="line">) sku</span><br><span class="line"><span class="keyword">join</span> ods_base_category3_df c3 <span class="keyword">on</span> sku.category3_id = c3.id <span class="keyword">and</span></span><br><span class="line">c3.ds = <span class="string">'$&#123;bizdate&#125;'</span></span><br><span class="line"><span class="keyword">join</span> ods_base_category2_df c2 <span class="keyword">on</span> c3.category2_id = c2.id <span class="keyword">and</span></span><br><span class="line">c2.ds = <span class="string">'$&#123;bizdate&#125;'</span></span><br><span class="line"><span class="keyword">join</span> ods_base_category1_df c1 <span class="keyword">on</span> c2.category1_id = c1.id <span class="keyword">and</span></span><br><span class="line">c1.ds = <span class="string">'$&#123;bizdate&#125;'</span></span><br><span class="line"><span class="keyword">join</span> ods_base_trademark_df tm <span class="keyword">on</span> tm.tm_id = sku.tm_id <span class="keyword">and</span> tm.ds</span><br><span class="line">= <span class="string">'$&#123;bizdate&#125;'</span>;</span><br><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">table</span> dim_user_info_df <span class="keyword">partition</span>(ds)</span><br><span class="line"><span class="keyword">select</span> <span class="keyword">id</span>,</span><br><span class="line"><span class="keyword">name</span> ,</span><br><span class="line">birthday,</span><br><span class="line">gender,</span><br><span class="line">email,</span><br><span class="line">user_level,</span><br><span class="line">create_time,</span><br><span class="line">ds <span class="keyword">from</span> ods_user_info_df</span><br><span class="line"><span class="keyword">where</span> ds=<span class="string">'$&#123;bizdate&#125;'</span> <span class="keyword">and</span> <span class="keyword">id</span> <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">null</span>;</span><br><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">table</span> dim_base_province_df <span class="keyword">PARTITION</span> (ds)</span><br><span class="line"><span class="keyword">select</span></span><br><span class="line">p.id,</span><br><span class="line">p.name,</span><br><span class="line">p.region_id,</span><br><span class="line">r.region_name,</span><br><span class="line">p.ds</span><br><span class="line"><span class="keyword">from</span> ods_base_province_df p <span class="keyword">join</span> ods_base_region_df r</span><br><span class="line"><span class="keyword">on</span> p.region_id = r.id <span class="keyword">and</span> p.ds=<span class="string">'$&#123;bizdate&#125;'</span> <span class="keyword">and</span> r.ds =</span><br><span class="line"><span class="string">'$&#123;bizdate&#125;'</span>;</span><br></pre></td></tr></table></figure>

<p>2）在临时查询中查询结果</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> dwd_order_info_di <span class="keyword">where</span> ds=<span class="string">'20191008'</span>;</span><br><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> dwd_order_detail_di <span class="keyword">where</span> ds=<span class="string">'20191008'</span>;</span><br><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> dim_sku_info_df <span class="keyword">where</span> ds=<span class="string">'20191008'</span>;</span><br><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> dim_user_info_df <span class="keyword">where</span> ds=<span class="string">'20191008'</span>;</span><br><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> dim_base_province_df <span class="keyword">where</span> ds=<span class="string">'20191008'</span>;</span><br></pre></td></tr></table></figure>

<h3 id="7-6-3-数据导入脚本"><a href="#7-6-3-数据导入脚本" class="headerlink" title="7.6.3 数据导入脚本"></a>7.6.3 数据导入脚本</h3><p>1）编写 dwd_order_info_di 表脚本<br>（1）在流程中加入一个数据开发脚本</p>
<p><img src="https://leinuo-blog.oss-cn-beijing.aliyuncs.com/images/typora/1594027519293.png" alt="1594027519293"></p>
<p><img src="https://leinuo-blog.oss-cn-beijing.aliyuncs.com/images/typora/1594027522413.png" alt="1594027522413"></p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">Insert</span> overwrite <span class="keyword">table</span> dwd_order_info_di <span class="keyword">partition</span>(ds)</span><br><span class="line"><span class="keyword">Select</span> <span class="keyword">id</span>,</span><br><span class="line">total_amount,</span><br><span class="line">order_status,</span><br><span class="line">user_id,</span><br><span class="line">payment_way,</span><br><span class="line">out_trade_no,</span><br><span class="line">province_id,</span><br><span class="line">create_time,</span><br><span class="line">operate_time,</span><br><span class="line">ds</span><br><span class="line"><span class="keyword">from</span> ods_order_info_di</span><br><span class="line"><span class="keyword">where</span> ds=<span class="string">'$&#123;bizdate&#125;'</span> <span class="keyword">and</span> <span class="keyword">id</span> <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">null</span>;</span><br></pre></td></tr></table></figure>

<p>（2）配置参数</p>
<p>点击调度配置-&gt; bizdate=${yyyymmdd}</p>
<p>2）编写 dwd_order_detail_di 表脚本<br>（1）新建节点 dwd_order_detail_di_sql</p>
<p><img src="https://leinuo-blog.oss-cn-beijing.aliyuncs.com/images/typora/1594027549710.png" alt="1594027549710"></p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">table</span> dwd_order_detail_di <span class="keyword">partition</span>(ds)</span><br><span class="line"><span class="keyword">select</span> od.id,</span><br><span class="line">order_id,</span><br><span class="line">oi.user_id,</span><br><span class="line">sku_id,</span><br><span class="line">sku_name,</span><br><span class="line">order_price,</span><br><span class="line">sku_num,</span><br><span class="line">oi.province_id,</span><br><span class="line">od.create_time,</span><br><span class="line">od.ds</span><br><span class="line"><span class="keyword">from</span> ods_order_detail_di od <span class="keyword">join</span> ods_order_info_di oi</span><br><span class="line"><span class="keyword">on</span> od.order_id = oi.id <span class="keyword">and</span> oi.ds = <span class="string">'$&#123;bizdate&#125;'</span></span><br><span class="line"><span class="keyword">and</span> od.ds = <span class="string">'$&#123;bizdate&#125;'</span> <span class="keyword">and</span> od.id <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">null</span>;</span><br></pre></td></tr></table></figure>

<p>（2）配置参数<br>点击调度配置-&gt; bizdate=${yyyymmdd}<br>3）编写 dim_sku_info_df 表脚本<br>（1）新建节点 dim_sku_info_df_sql</p>
<p><img src="https://leinuo-blog.oss-cn-beijing.aliyuncs.com/images/typora/1594027571919.png" alt="1594027571919"></p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">table</span> dim_sku_info_df <span class="keyword">partition</span>(ds)</span><br><span class="line"><span class="keyword">select</span></span><br><span class="line">sku.id,</span><br><span class="line">sku.spu_id,</span><br><span class="line">sku.price,</span><br><span class="line">sku.sku_name,</span><br><span class="line">sku.sku_desc,</span><br><span class="line">sku.weight,</span><br><span class="line">sku.tm_id,</span><br><span class="line">tm.tm_name,</span><br><span class="line">sku.category3_id,</span><br><span class="line">c2.id category2_id ,</span><br><span class="line">c1.id category1_id,</span><br><span class="line">c3.name category3_name,</span><br><span class="line">c2.name category2_name,</span><br><span class="line">c1.name category1_name,</span><br><span class="line">sku.create_time,</span><br><span class="line">sku.ds</span><br><span class="line"><span class="keyword">from</span></span><br><span class="line">(</span><br><span class="line"><span class="keyword">select</span> *</span><br><span class="line"><span class="keyword">from</span> ods_sku_info_df</span><br><span class="line"><span class="keyword">where</span> ds=<span class="string">'$&#123;bizdate&#125;'</span> <span class="keyword">and</span> <span class="keyword">id</span> <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">null</span></span><br><span class="line">) sku</span><br><span class="line"><span class="keyword">join</span> ods_base_category3_df c3 <span class="keyword">on</span> sku.category3_id = c3.id <span class="keyword">and</span></span><br><span class="line">c3.ds = <span class="string">'$&#123;bizdate&#125;'</span></span><br><span class="line"><span class="keyword">join</span> ods_base_category2_df c2 <span class="keyword">on</span> c3.category2_id = c2.id <span class="keyword">and</span></span><br><span class="line">c2.ds = <span class="string">'$&#123;bizdate&#125;'</span></span><br><span class="line"><span class="keyword">join</span> ods_base_category1_df c1 <span class="keyword">on</span> c2.category1_id = c1.id <span class="keyword">and</span></span><br><span class="line">c1.ds = <span class="string">'$&#123;bizdate&#125;'</span></span><br><span class="line"><span class="keyword">join</span> ods_base_trademark_df tm <span class="keyword">on</span> tm.tm_id = sku.tm_id <span class="keyword">and</span> tm.ds</span><br><span class="line">= <span class="string">'$&#123;bizdate&#125;'</span>;</span><br></pre></td></tr></table></figure>

<p>（2）配置参数<br>点击调度配置-&gt; bizdate=${yyyymmdd}</p>
<p>4）编写 dim_user_info_df 表脚本<br>（1）新建节点 dim_user_info_df_sql</p>
<p><img src="https://leinuo-blog.oss-cn-beijing.aliyuncs.com/images/typora/1594027604097.png" alt="1594027604097"></p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">table</span> dim_user_info_df <span class="keyword">partition</span>(ds)</span><br><span class="line"><span class="keyword">select</span> <span class="keyword">id</span>,</span><br><span class="line"><span class="keyword">name</span> ,</span><br><span class="line">birthday,</span><br><span class="line">gender,</span><br><span class="line">email,</span><br><span class="line">user_level,</span><br><span class="line">create_time,</span><br><span class="line">ds <span class="keyword">from</span> ods_user_info_df</span><br><span class="line"><span class="keyword">where</span> ds=<span class="string">'$&#123;bizdate&#125;'</span> <span class="keyword">and</span> <span class="keyword">id</span> <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">null</span>;</span><br></pre></td></tr></table></figure>

<p>（2）配置参数<br>点击调度配置-&gt; bizdate=${yyyymmdd}<br>5）编写 dim_base_province_df 表脚本<br>（1）新建节点 dim_base_province_df_sql</p>
<p><img src="https://leinuo-blog.oss-cn-beijing.aliyuncs.com/images/typora/1594027664281.png" alt="1594027664281"></p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">table</span> dim_base_province_df <span class="keyword">PARTITION</span> (ds)</span><br><span class="line"><span class="keyword">select</span></span><br><span class="line">p.id,</span><br><span class="line">p.name,</span><br><span class="line">p.region_id,</span><br><span class="line">r.region_name,</span><br><span class="line">p.ds</span><br><span class="line"><span class="keyword">from</span> ods_base_province_df p <span class="keyword">join</span> ods_base_region_df r</span><br><span class="line"><span class="keyword">on</span> p.region_id = r.id <span class="keyword">and</span> p.ds=<span class="string">'$&#123;bizdate&#125;'</span> <span class="keyword">and</span> r.ds =</span><br><span class="line"><span class="string">'$&#123;bizdate&#125;'</span>;</span><br></pre></td></tr></table></figure>

<p>（2）配置参数<br>点击调度配置-&gt; bizdate=${yyyymmdd}</p>
<h3 id="7-7-DWS-层"><a href="#7-7-DWS-层" class="headerlink" title="7.7 DWS 层"></a>7.7 DWS 层</h3><p>​    DWS 层主要指针对明细粒度的数据进行短周期的汇总。DWS 公共汇总层是面向分析对<br>象的主题聚集建模。<br>​    在本教程中，最终的分析目标为：最近一天某个类目、某个地区、某类人群购买商品的<br>销售总额、购买力分布。因此，我们可以以最终交易成功的商品、买家、地区等角度对最近<br>一天的数据进行组合，组合成为涵盖多个维度的事实宽表。</p>
<h3 id="7-7-1-建表语句"><a href="#7-7-1-建表语句" class="headerlink" title="7.7.1 建表语句"></a>7.7.1 建表语句</h3><p>1）临时查询中，执行建表语句</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> <span class="string">`dws_trade_detail_di`</span> (</span><br><span class="line"><span class="string">`user_id`</span> <span class="keyword">string</span> <span class="keyword">COMMENT</span> <span class="string">'用户 id'</span>,</span><br><span class="line"><span class="string">`sku_id`</span> <span class="keyword">string</span> <span class="keyword">COMMENT</span> <span class="string">'商品 Id'</span>,</span><br><span class="line"><span class="string">`user_gender`</span> <span class="keyword">string</span> <span class="keyword">COMMENT</span> <span class="string">'用户性别'</span>,</span><br><span class="line"><span class="string">`user_age`</span> <span class="keyword">string</span> <span class="keyword">COMMENT</span> <span class="string">'用户年龄'</span>,</span><br><span class="line"><span class="string">`user_level`</span> <span class="keyword">string</span> <span class="keyword">COMMENT</span> <span class="string">'用户等级'</span>,</span><br><span class="line"><span class="string">`sku_price`</span> <span class="keyword">double</span> <span class="keyword">COMMENT</span> <span class="string">'商品当日价格'</span>,</span><br><span class="line"><span class="string">`sku_name`</span> <span class="keyword">string</span> <span class="keyword">COMMENT</span> <span class="string">'商品名称'</span>,</span><br><span class="line"><span class="string">`sku_category3_id`</span> <span class="keyword">string</span> <span class="keyword">COMMENT</span> <span class="string">'商品三级品类 id'</span>,</span><br><span class="line"><span class="string">`sku_category2_id`</span> <span class="keyword">string</span> <span class="keyword">COMMENT</span> <span class="string">'商品二级品类 id'</span>,</span><br><span class="line"><span class="string">`sku_category1_id`</span> <span class="keyword">string</span> <span class="keyword">COMMENT</span> <span class="string">'商品一级品类 id'</span>,</span><br><span class="line"><span class="string">`sku_category3_name`</span> <span class="keyword">string</span> <span class="keyword">COMMENT</span> <span class="string">'商品三级品类名称'</span>,</span><br><span class="line"><span class="string">`sku_category2_name`</span> <span class="keyword">string</span> <span class="keyword">COMMENT</span> <span class="string">'商品二级品类名称'</span>,</span><br><span class="line"><span class="string">`sku_category1_name`</span> <span class="keyword">string</span> <span class="keyword">COMMENT</span> <span class="string">'商品一级品类名称'</span>,</span><br><span class="line"><span class="string">`spu_id`</span> <span class="keyword">string</span> <span class="keyword">COMMENT</span> <span class="string">'商品 spu'</span>,</span><br><span class="line"><span class="string">`tm_id`</span> <span class="keyword">string</span> <span class="keyword">COMMENT</span> <span class="string">'品牌 id'</span>,</span><br><span class="line"><span class="string">`tm_name`</span> <span class="keyword">string</span> <span class="keyword">COMMENT</span> <span class="string">'品牌名称'</span>,</span><br><span class="line"><span class="string">`province_id`</span> <span class="keyword">string</span> <span class="keyword">COMMENT</span> <span class="string">'省市 id'</span>,</span><br><span class="line"><span class="string">`province_name`</span> <span class="keyword">string</span> <span class="keyword">COMMENT</span> <span class="string">'省市名称'</span>,</span><br><span class="line"><span class="string">`region_id`</span> <span class="keyword">string</span> <span class="keyword">COMMENT</span> <span class="string">'地区 id'</span>,</span><br><span class="line"><span class="string">`region_name`</span> <span class="keyword">string</span> <span class="keyword">COMMENT</span> <span class="string">'地区名称'</span>,</span><br><span class="line"><span class="string">`sku_num`</span> <span class="built_in">bigint</span> <span class="keyword">COMMENT</span> <span class="string">'购买个数'</span>,</span><br><span class="line"><span class="string">`order_count`</span> <span class="built_in">bigint</span> <span class="keyword">COMMENT</span> <span class="string">'当日下单单数'</span>,</span><br><span class="line"><span class="string">`order_amount`</span> <span class="keyword">double</span> <span class="keyword">COMMENT</span> <span class="string">'当日下单金额'</span></span><br><span class="line">)</span><br><span class="line"><span class="keyword">COMMENT</span> <span class="string">'用户单日交易行为宽表'</span></span><br><span class="line">PARTITIONED <span class="keyword">BY</span> (ds <span class="keyword">string</span>);</span><br></pre></td></tr></table></figure>

<p>2）在表管理里面查看创建的表</p>
<p><img src="https://leinuo-blog.oss-cn-beijing.aliyuncs.com/images/typora/1594027742459.png" alt="1594027742459"></p>
<p>3）数据开发-&gt;dws-&gt;导入表</p>
<p><img src="https://leinuo-blog.oss-cn-beijing.aliyuncs.com/images/typora/1594027749912.png" alt="1594027749912"></p>
<p>4）选择刚创建的所有表</p>
<p><img src="https://leinuo-blog.oss-cn-beijing.aliyuncs.com/images/typora/1594027758281.png" alt="1594027758281"></p>
<h3 id="7-7-2-手动将数据导入-DWS-层"><a href="#7-7-2-手动将数据导入-DWS-层" class="headerlink" title="7.7.2 手动将数据导入 DWS 层"></a>7.7.2 手动将数据导入 DWS 层</h3><p>1）在临时查询中执行</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">with</span> tmp_trade <span class="keyword">AS</span></span><br><span class="line">(</span><br><span class="line"><span class="keyword">select</span></span><br><span class="line">od.user_id,od.sku_id,od.province_id,</span><br><span class="line"><span class="keyword">sum</span>(sku_num) sku_num,</span><br><span class="line"><span class="keyword">count</span>(*) order_count,</span><br><span class="line"><span class="keyword">sum</span>(od.order_price*sku_num) order_amount</span><br><span class="line"><span class="keyword">from</span> dwd_order_detail_di od</span><br><span class="line"><span class="keyword">where</span> od.ds=<span class="string">'$&#123;bizdate&#125;'</span></span><br><span class="line"><span class="keyword">group</span> <span class="keyword">by</span> od.user_id, od.sku_id, od.province_id</span><br><span class="line">)</span><br><span class="line"><span class="keyword">insert</span> OVERWRITE <span class="keyword">TABLE</span> dws_trade_detail_di <span class="keyword">PARTITION</span> (ds=<span class="string">'$&#123;bizdate&#125;'</span>)</span><br><span class="line"><span class="keyword">select</span></span><br><span class="line">tmp_trade.user_id,</span><br><span class="line">tmp_trade.sku_id,</span><br><span class="line">u.gender,</span><br><span class="line">months_between(to_char(<span class="keyword">to_date</span>(<span class="string">'$&#123;bizdate&#125;'</span>,<span class="string">'yyyymmdd'</span>),<span class="string">'yyyy-mm-dd'</span>), u.birthday)/<span class="number">12</span> age,</span><br><span class="line">u.user_level,</span><br><span class="line">price,</span><br><span class="line">sku_name,</span><br><span class="line">category3_id,</span><br><span class="line">category2_id,</span><br><span class="line">category1_id,</span><br><span class="line">category3_name,</span><br><span class="line">category2_name,</span><br><span class="line">category1_name,</span><br><span class="line">spu_id,</span><br><span class="line">tm_id,</span><br><span class="line">tm_name,</span><br><span class="line">p.id,</span><br><span class="line">p.province_name,</span><br><span class="line">p.region_id,</span><br><span class="line">p.region_name,</span><br><span class="line">tmp_trade.sku_num,</span><br><span class="line">tmp_trade.order_count,</span><br><span class="line">tmp_trade.order_amount</span><br><span class="line"><span class="keyword">from</span> tmp_trade</span><br><span class="line"><span class="keyword">left</span> <span class="keyword">join</span> dim_user_info_df u <span class="keyword">on</span> u.id = tmp_trade.user_id <span class="keyword">and</span> u.ds = <span class="string">'$&#123;bizdate&#125;'</span></span><br><span class="line"><span class="keyword">left</span> <span class="keyword">join</span> dim_sku_info_df s <span class="keyword">on</span> tmp_trade.sku_id = s.id <span class="keyword">and</span> s.ds = <span class="string">'$&#123;bizdate&#125;'</span></span><br><span class="line"><span class="keyword">left</span> <span class="keyword">join</span> dim_base_province_df p <span class="keyword">on</span> tmp_trade.province_id = p.id <span class="keyword">and</span> p.ds=<span class="string">'$&#123;bizdate&#125;'</span>;</span><br></pre></td></tr></table></figure>

<p>2）查看结果</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> * <span class="keyword">from</span> dws_trade_detail_di <span class="keyword">WHERE</span> ds=<span class="string">'20191008'</span></span><br></pre></td></tr></table></figure>

<h4 id="7-7-3-数据导入脚本"><a href="#7-7-3-数据导入脚本" class="headerlink" title="7.7.3 数据导入脚本"></a>7.7.3 数据导入脚本</h4><p>1）编写 dws_trade_detail_di 表脚本<br>（1）在流程中加入一个数据开发脚本</p>
<p><img src="https://leinuo-blog.oss-cn-beijing.aliyuncs.com/images/typora/1594027867497.png" alt="1594027867497"></p>
<p>（2）新建节点 dws_trade_detail_di_sql</p>
<p><img src="https://leinuo-blog.oss-cn-beijing.aliyuncs.com/images/typora/1594027876411.png" alt="1594027876411"></p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">with</span> tmp_trade <span class="keyword">AS</span></span><br><span class="line">(</span><br><span class="line"><span class="keyword">select</span></span><br><span class="line">od.user_id,od.sku_id,od.province_id,</span><br><span class="line"><span class="keyword">sum</span>(sku_num) sku_num,</span><br><span class="line"><span class="keyword">count</span>(*) order_count,</span><br><span class="line"><span class="keyword">sum</span>(od.order_price*sku_num) order_amount</span><br><span class="line"><span class="keyword">from</span> dwd_order_detail_di od</span><br><span class="line"><span class="keyword">where</span> od.ds=<span class="string">'$&#123;bizdate&#125;'</span></span><br><span class="line"><span class="keyword">group</span> <span class="keyword">by</span> od.user_id, od.sku_id, od.province_id</span><br><span class="line">)</span><br><span class="line"><span class="keyword">insert</span> OVERWRITE <span class="keyword">TABLE</span> dws_trade_detail_di <span class="keyword">PARTITION</span> (ds=<span class="string">'$&#123;bizdate&#125;'</span>)</span><br><span class="line"><span class="keyword">select</span></span><br><span class="line">tmp_trade.user_id,</span><br><span class="line">tmp_trade.sku_id,</span><br><span class="line">u.gender,</span><br><span class="line">months_between(<span class="string">'$&#123;bizdate&#125;'</span>, u.birthday)/<span class="number">12</span> age,</span><br><span class="line">u.user_level,</span><br><span class="line">price,</span><br><span class="line">sku_name,</span><br><span class="line">category3_id,</span><br><span class="line">category2_id,</span><br><span class="line">category1_id,</span><br><span class="line">category3_name,</span><br><span class="line">category2_name,</span><br><span class="line">category1_name,</span><br><span class="line">spu_id,</span><br><span class="line">tm_id,</span><br><span class="line">tm_name,</span><br><span class="line">p.id,</span><br><span class="line">p.province_name,</span><br><span class="line">p.region_id,</span><br><span class="line">p.region_name,</span><br><span class="line">tmp_trade.sku_num,</span><br><span class="line">tmp_trade.order_count,</span><br><span class="line">tmp_trade.order_amount</span><br><span class="line"><span class="keyword">from</span> tmp_trade</span><br><span class="line"><span class="keyword">left</span> <span class="keyword">join</span> dim_user_info_df u <span class="keyword">on</span> u.id = tmp_trade.user_id <span class="keyword">and</span> u.ds = <span class="string">'$&#123;bizdate&#125;'</span></span><br><span class="line"><span class="keyword">left</span> <span class="keyword">join</span> dim_sku_info_df s <span class="keyword">on</span> tmp_trade.sku_id = s.id <span class="keyword">and</span> s.ds = <span class="string">'$&#123;bizdate&#125;'</span></span><br><span class="line"><span class="keyword">left</span> <span class="keyword">join</span> dim_base_province_df p <span class="keyword">on</span> tmp_trade.province_id = p.id <span class="keyword">and</span> p.ds=<span class="string">'$&#123;bizdate&#125;'</span>;</span><br></pre></td></tr></table></figure>

<p>（2）配置参数<br>点击调度配置-&gt; bizdate=${yyyymmdd}</p>
<h3 id="7-8-ADS-层"><a href="#7-8-ADS-层" class="headerlink" title="7.8 ADS 层"></a>7.8 ADS 层</h3><p>ADS 层主要指针对某一个特定的维度进行的汇总。<br>在本课程中，主要分析三个需求：用户各个年龄段统计、地区销售统计、热门商品排行<br>所以主要是针对年龄、地区、商品进行汇总统计，统计四个指标下单数、购买商品个数、<br>销售额、平均客单价。</p>
<h4 id="7-8-1-建表语句"><a href="#7-8-1-建表语句" class="headerlink" title="7.8.1 建表语句"></a>7.8.1 建表语句</h4><p>1）临时查询中，执行建表语句</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> <span class="string">`ads_trade_age_d`</span> (</span><br><span class="line"><span class="string">`age`</span> <span class="built_in">bigint</span> <span class="keyword">COMMENT</span> <span class="string">'年龄'</span>,</span><br><span class="line"><span class="string">`sku_num`</span> <span class="built_in">bigint</span> <span class="keyword">COMMENT</span> <span class="string">'购买商品个数'</span>,</span><br><span class="line"><span class="string">`order_count`</span> <span class="built_in">bigint</span> <span class="keyword">COMMENT</span> <span class="string">'订单个数'</span>,</span><br><span class="line"><span class="string">`order_amount`</span> <span class="keyword">double</span> <span class="keyword">COMMENT</span> <span class="string">'销售额'</span>,</span><br><span class="line"><span class="string">`avg_amount`</span> <span class="keyword">double</span> <span class="keyword">COMMENT</span> <span class="string">'平均客单价'</span></span><br><span class="line">)</span><br><span class="line"><span class="keyword">COMMENT</span> <span class="string">'年龄销售统计'</span></span><br><span class="line">PARTITIONED <span class="keyword">BY</span> (ds <span class="keyword">string</span>);</span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> <span class="string">`ads_trade_province_d`</span> (</span><br><span class="line"><span class="string">`province`</span> <span class="keyword">string</span> <span class="keyword">COMMENT</span> <span class="string">'省份 id'</span>,</span><br><span class="line"><span class="string">`province_name`</span> <span class="keyword">string</span> <span class="keyword">COMMENT</span> <span class="string">'省市名称'</span>,</span><br><span class="line"><span class="string">`region_id`</span> <span class="keyword">string</span> <span class="keyword">COMMENT</span> <span class="string">'地区 ID'</span>,</span><br><span class="line"><span class="string">`region_name`</span> <span class="keyword">string</span> <span class="keyword">COMMENT</span> <span class="string">'地区名称'</span>,</span><br><span class="line"><span class="string">`sku_num`</span> <span class="built_in">bigint</span> <span class="keyword">COMMENT</span> <span class="string">'购买商品个数'</span>,</span><br><span class="line"><span class="string">`order_count`</span> <span class="built_in">bigint</span> <span class="keyword">COMMENT</span> <span class="string">'订单个数'</span>,</span><br><span class="line"><span class="string">`order_amount`</span> <span class="keyword">double</span> <span class="keyword">COMMENT</span> <span class="string">'销售额'</span>,</span><br><span class="line"><span class="string">`avg_amount`</span> <span class="keyword">double</span> <span class="keyword">COMMENT</span> <span class="string">'平均客单价'</span></span><br><span class="line">)</span><br><span class="line"><span class="keyword">COMMENT</span> <span class="string">'地区销售统计'</span></span><br><span class="line">PARTITIONED <span class="keyword">BY</span> (ds <span class="keyword">string</span>);</span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> <span class="string">`ads_trade_sku_d`</span> (</span><br><span class="line"><span class="string">`sku_id`</span> <span class="keyword">string</span> <span class="keyword">COMMENT</span> <span class="string">'商品 id'</span>,</span><br><span class="line"><span class="string">`sku_name`</span> <span class="keyword">string</span> <span class="keyword">COMMENT</span> <span class="string">'商品名称'</span>,</span><br><span class="line"><span class="string">`sku_num`</span> <span class="built_in">bigint</span> <span class="keyword">COMMENT</span> <span class="string">'购买商品个数'</span>,</span><br><span class="line"><span class="string">`category3_id`</span> <span class="keyword">string</span> <span class="keyword">COMMENT</span> <span class="string">'三级分类 id'</span>,</span><br><span class="line"><span class="string">`category2_id`</span> <span class="keyword">string</span> <span class="keyword">COMMENT</span> <span class="string">'二级分类 id'</span>,</span><br><span class="line"><span class="string">`category1_id`</span> <span class="keyword">string</span> <span class="keyword">COMMENT</span> <span class="string">'一级分类 id'</span>,</span><br><span class="line"><span class="string">`category3_name`</span> <span class="keyword">string</span> <span class="keyword">COMMENT</span> <span class="string">'三级分类名称'</span>,</span><br><span class="line"><span class="string">`category2_name`</span> <span class="keyword">string</span> <span class="keyword">COMMENT</span> <span class="string">'二级分类名称'</span>,</span><br><span class="line"><span class="string">`category1_name`</span> <span class="keyword">string</span> <span class="keyword">COMMENT</span> <span class="string">'一级分类名称'</span>,</span><br><span class="line"><span class="string">`order_count`</span> <span class="built_in">bigint</span> <span class="keyword">COMMENT</span> <span class="string">'订单个数'</span>,</span><br><span class="line"><span class="string">`order_amount`</span> <span class="keyword">double</span> <span class="keyword">COMMENT</span> <span class="string">'销售额'</span>,</span><br><span class="line"><span class="string">`avg_amount`</span> <span class="keyword">double</span> <span class="keyword">COMMENT</span> <span class="string">'平均客单价'</span></span><br><span class="line">)</span><br><span class="line"><span class="keyword">COMMENT</span> <span class="string">'商品销售统计'</span></span><br><span class="line">PARTITIONED <span class="keyword">BY</span> (ds <span class="keyword">string</span>);</span><br></pre></td></tr></table></figure>

<p>2）在表管理里面查看创建的表</p>
<p><img src="https://leinuo-blog.oss-cn-beijing.aliyuncs.com/images/typora/1594027997199.png" alt="1594027997199"></p>
<p>3）数据开发-&gt;ads-&gt;导入表</p>
<p><img src="https://leinuo-blog.oss-cn-beijing.aliyuncs.com/images/typora/1594028004174.png" alt="1594028004174"></p>
<p>4）选择刚创建的所有表</p>
<p><img src="https://leinuo-blog.oss-cn-beijing.aliyuncs.com/images/typora/1594028011747.png" alt="1594028011747"></p>
<h3 id="7-8-2-手动将数据导入-ADS-层"><a href="#7-8-2-手动将数据导入-ADS-层" class="headerlink" title="7.8.2 手动将数据导入 ADS 层"></a>7.8.2 手动将数据导入 ADS 层</h3><p>1）在临时查询中执行</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">insert</span> OVERWRITE <span class="keyword">table</span> ads_trade_age_d <span class="keyword">PARTITION</span> (ds = <span class="string">'$&#123;bizdate&#125;'</span>)</span><br><span class="line"><span class="keyword">select</span></span><br><span class="line"><span class="keyword">round</span>(td.user_age) age,</span><br><span class="line"><span class="keyword">sum</span>(sku_num) sku_num,</span><br><span class="line"><span class="keyword">sum</span>(order_count) order_count,</span><br><span class="line"><span class="keyword">sum</span>(order_amount) order_amount,</span><br><span class="line"><span class="keyword">round</span>(<span class="keyword">avg</span>(order_amount),<span class="number">2</span>) avg_amount</span><br><span class="line"><span class="keyword">from</span> dws_trade_detail_di td</span><br><span class="line"><span class="keyword">where</span> ds = <span class="string">'$&#123;bizdate&#125;'</span></span><br><span class="line"><span class="keyword">group</span> <span class="keyword">by</span> <span class="keyword">round</span>(td.user_age);</span><br><span class="line"><span class="keyword">insert</span> OVERWRITE <span class="keyword">table</span> ads_trade_province_d <span class="keyword">PARTITION</span> (ds =<span class="string">'$&#123;bizdate&#125;'</span>)</span><br><span class="line"><span class="keyword">select</span></span><br><span class="line">td.province_id,td.province_name,td.region_id,td.region_name,</span><br><span class="line"><span class="keyword">sum</span>(sku_num) sku_num,</span><br><span class="line"><span class="keyword">sum</span>(order_count) order_count,</span><br><span class="line"><span class="keyword">sum</span>(order_amount) order_amount,</span><br><span class="line"><span class="keyword">round</span>(<span class="keyword">avg</span>(order_amount),<span class="number">2</span>) avg_amount</span><br><span class="line"><span class="keyword">from</span> dws_trade_detail_di td</span><br><span class="line"><span class="keyword">where</span> ds=<span class="string">'$&#123;bizdate&#125;'</span></span><br><span class="line"><span class="keyword">group</span> <span class="keyword">by</span></span><br><span class="line">td.province_id,td.province_name,td.region_id,td.region_name;</span><br><span class="line"><span class="keyword">insert</span> OVERWRITE <span class="keyword">table</span> ads_trade_sku_d <span class="keyword">PARTITION</span> (ds = <span class="string">'$&#123;bizdate&#125;'</span>)</span><br><span class="line"><span class="keyword">select</span></span><br><span class="line">td.sku_id,td.sku_name,</span><br><span class="line">td.sku_category3_id,</span><br><span class="line">td.sku_category2_id,</span><br><span class="line">td.sku_category1_id,</span><br><span class="line">td.sku_category3_name,</span><br><span class="line">td.sku_category2_name,</span><br><span class="line">td.sku_category1_name,</span><br><span class="line"><span class="keyword">sum</span>(sku_num) sku_num,</span><br><span class="line"><span class="keyword">sum</span>(order_count) order_count,</span><br><span class="line"><span class="keyword">sum</span>(order_amount) order_amount,</span><br><span class="line"><span class="keyword">round</span>(<span class="keyword">avg</span>(order_amount),<span class="number">2</span>) avg_amount</span><br><span class="line"><span class="keyword">from</span> dws_trade_detail_di td</span><br><span class="line"><span class="keyword">where</span> ds = <span class="string">'$&#123;bizdate&#125;'</span></span><br><span class="line"><span class="keyword">group</span> <span class="keyword">by</span></span><br><span class="line">td.sku_id, td.sku_name, td.sku_category3_id,</span><br><span class="line">td.sku_category2_id, td.sku_category1_id,</span><br><span class="line">td.sku_category3_name, td.sku_category2_name,</span><br><span class="line">td.sku_category1_name;</span><br></pre></td></tr></table></figure>

<p>2）在临时查询中查询结果</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> ads_trade_age_d <span class="keyword">WHERE</span> ds=<span class="string">'20191008'</span>;</span><br><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> ads_trade_province_d <span class="keyword">WHERE</span> ds=<span class="string">'20191008'</span>;</span><br><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> ads_trade_sku_d <span class="keyword">WHERE</span> ds=<span class="string">'20191008'</span>;</span><br></pre></td></tr></table></figure>

<h3 id="7-8-3-数据导入脚本"><a href="#7-8-3-数据导入脚本" class="headerlink" title="7.8.3 数据导入脚本"></a>7.8.3 数据导入脚本</h3><p>1）编写 ads_trade_age_d 表脚本<br>（1）在流程中加入一个数据开发脚本</p>
<p><img src="https://leinuo-blog.oss-cn-beijing.aliyuncs.com/images/typora/1594028092781.png" alt="1594028092781"></p>
<p>（2）新建节点 ads_trade_age_d_sql</p>
<p><img src="https://leinuo-blog.oss-cn-beijing.aliyuncs.com/images/typora/1594028101712.png" alt="1594028101712"></p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">insert</span> OVERWRITE <span class="keyword">table</span> ads_trade_age_d <span class="keyword">PARTITION</span> (ds =</span><br><span class="line"><span class="string">'$&#123;bizdate&#125;'</span>)</span><br><span class="line"><span class="keyword">select</span></span><br><span class="line"><span class="keyword">round</span>(td.user_age) age,</span><br><span class="line"><span class="keyword">sum</span>(sku_num) sku_num,</span><br><span class="line"><span class="keyword">sum</span>(order_count) order_count,</span><br><span class="line"><span class="keyword">sum</span>(order_amount) order_amount,</span><br><span class="line"><span class="keyword">round</span>(<span class="keyword">avg</span>(order_amount),<span class="number">2</span>) avg_amount</span><br><span class="line"><span class="keyword">from</span> dws_trade_detail_di td</span><br><span class="line"><span class="keyword">where</span> ds = <span class="string">'$&#123;bizdate&#125;'</span></span><br><span class="line"><span class="keyword">group</span> <span class="keyword">by</span> <span class="keyword">round</span>(td.user_age)</span><br></pre></td></tr></table></figure>

<p>（2）配置参数<br>点击调度配置-&gt; bizdate=${yyyymmdd}<br>2）编写 ads_trade_province_d 表脚本<br>（1）新建节点 ads_trade_province_d_sql</p>
<p><img src="https://leinuo-blog.oss-cn-beijing.aliyuncs.com/images/typora/1594028128339.png" alt="1594028128339"></p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">insert</span> OVERWRITE <span class="keyword">table</span> ads_trade_province_d <span class="keyword">PARTITION</span> (ds = <span class="string">'$&#123;bizdate&#125;'</span>)</span><br><span class="line"><span class="keyword">select</span></span><br><span class="line">td.province_id,td.province_name,td.region_id,td.region_name,</span><br><span class="line"><span class="keyword">sum</span>(sku_num) sku_num,</span><br><span class="line"><span class="keyword">sum</span>(order_count) order_count,</span><br><span class="line"><span class="keyword">sum</span>(order_amount) order_amount,</span><br><span class="line"><span class="keyword">round</span>(<span class="keyword">avg</span>(order_amount),<span class="number">2</span>) avg_amount</span><br><span class="line"><span class="keyword">from</span> dws_trade_detail_di td</span><br><span class="line"><span class="keyword">where</span> ds=<span class="string">'$&#123;bizdate&#125;'</span></span><br><span class="line"><span class="keyword">group</span> <span class="keyword">by</span></span><br><span class="line">td.province_id,td.province_name,td.region_id,td.region_name;</span><br></pre></td></tr></table></figure>

<p>（2）配置参数<br>点击调度配置-&gt; bizdate=${yyyymmdd}<br>3）编写 ads_trade_sku_d 表脚本<br>（1）新建节点 ads_trade_sku_d_sql</p>
<p><img src="https://leinuo-blog.oss-cn-beijing.aliyuncs.com/images/typora/1594028154652.png" alt="1594028154652"></p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">insert</span> OVERWRITE <span class="keyword">table</span> ads_trade_sku_d <span class="keyword">PARTITION</span> (ds = <span class="string">'$&#123;bizdate&#125;'</span>)</span><br><span class="line"><span class="keyword">select</span></span><br><span class="line">td.sku_id,td.sku_name,</span><br><span class="line">td.sku_category3_id,</span><br><span class="line">td.sku_category2_id,</span><br><span class="line">td.sku_category1_id,</span><br><span class="line">td.sku_category3_name,</span><br><span class="line">td.sku_category2_name,</span><br><span class="line">td.sku_category1_name,</span><br><span class="line"><span class="keyword">sum</span>(sku_num) sku_num,</span><br><span class="line"><span class="keyword">sum</span>(order_count) order_count,</span><br><span class="line"><span class="keyword">sum</span>(order_amount) order_amount,</span><br><span class="line"><span class="keyword">round</span>(<span class="keyword">avg</span>(order_amount),<span class="number">2</span>) avg_amount</span><br><span class="line"><span class="keyword">from</span> dws_trade_detail_di td</span><br><span class="line"><span class="keyword">where</span> ds = <span class="string">'$&#123;bizdate&#125;'</span></span><br><span class="line"><span class="keyword">group</span> <span class="keyword">by</span></span><br><span class="line">td.sku_id, td.sku_name, td.sku_category3_id,</span><br><span class="line">td.sku_category2_id, td.sku_category1_id,</span><br><span class="line">td.sku_category3_name, td.sku_category2_name,</span><br><span class="line">td.sku_category1_name;</span><br></pre></td></tr></table></figure>

<p>（2）配置参数<br>点击调度配置-&gt; bizdate=${yyyymmdd}</p>
<h3 id="7-9-作业调度"><a href="#7-9-作业调度" class="headerlink" title="7.9 作业调度"></a>7.9 作业调度</h3><p>1）整个业务部分的数仓任务应该包括如下图：</p>
<p><img src="https://leinuo-blog.oss-cn-beijing.aliyuncs.com/images/typora/1594028191795.png" alt="1594028191795"></p>
<p><img src="https://leinuo-blog.oss-cn-beijing.aliyuncs.com/images/typora/1594028200199.png" alt="1594028200199"></p>
<p>2）在原理 ODS 层的基础上继续增加 DWD 层、DWS 层业务、ADS 层业务。</p>
<p><img src="https://leinuo-blog.oss-cn-beijing.aliyuncs.com/images/typora/1594028211201.png" alt="1594028211201"></p>
<p>3）在公共表中预览执行结果</p>
<p><img src="https://leinuo-blog.oss-cn-beijing.aliyuncs.com/images/typora/1594028224562.png" alt="1594028224562"></p>
]]></content>
      <categories>
        <category>大数据</category>
        <category>大数据实战项目</category>
        <category>阿里云离线数仓</category>
        <category>4.业务数仓搭建</category>
      </categories>
      <tags>
        <tag>大数据</tag>
      </tags>
  </entry>
  <entry>
    <title>5.数据导出与作业调度</title>
    <url>/2020/04/07/5.%E6%95%B0%E6%8D%AE%E5%AF%BC%E5%87%BA%E4%B8%8E%E4%BD%9C%E4%B8%9A%E8%B0%83%E5%BA%A6/</url>
    <content><![CDATA[<h2 id="第-8-章-数据导出与作业调度"><a href="#第-8-章-数据导出与作业调度" class="headerlink" title="第 8 章 数据导出与作业调度"></a>第 8 章 数据导出与作业调度</h2><p>将 MaxCompute 中的计算完的结果，需要导入到 RDS 数据库中，用于后续的可视化。</p>
<h3 id="8-1-创建结果数据库"><a href="#8-1-创建结果数据库" class="headerlink" title="8.1 创建结果数据库"></a>8.1 创建结果数据库</h3><p>1）在 RDS 服务器中，新建一个 gmall_adb 数据库，用来保存之后从 MaxCompute 中的结果<br>数据。</p>
<p><img src="https://leinuo-blog.oss-cn-beijing.aliyuncs.com/images/typora/1594028344024.png" alt="1594028344024"></p>
<p>2）建 4 张和 ADS 层结果一样的表</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> <span class="string">`uv_source_d`</span> (</span><br><span class="line"><span class="string">`source`</span> <span class="built_in">varchar</span>(<span class="number">20</span>) <span class="keyword">NOT</span> <span class="literal">NULL</span> <span class="keyword">COMMENT</span> <span class="string">'渠道'</span>,</span><br><span class="line"><span class="string">`ct`</span> <span class="built_in">bigint</span>(<span class="number">20</span>) <span class="keyword">DEFAULT</span> <span class="literal">NULL</span> <span class="keyword">COMMENT</span> <span class="string">'个数'</span>,</span><br><span class="line"><span class="string">`ds`</span> <span class="built_in">varchar</span>(<span class="number">8</span>) <span class="keyword">NOT</span> <span class="literal">NULL</span> <span class="keyword">COMMENT</span> <span class="string">'日期'</span>,</span><br><span class="line">PRIMARY <span class="keyword">KEY</span> (<span class="string">`source`</span>,<span class="string">`ds`</span>)</span><br><span class="line">) <span class="keyword">ENGINE</span>=<span class="keyword">InnoDB</span> <span class="keyword">DEFAULT</span> <span class="keyword">CHARSET</span>=utf8 <span class="keyword">COMMENT</span>=<span class="string">'渠道日活'</span>;</span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> <span class="string">`trade_age_d`</span> (</span><br><span class="line"><span class="string">`age`</span> <span class="built_in">BIGINT</span>(<span class="number">20</span>) <span class="keyword">NOT</span> <span class="literal">NULL</span> <span class="keyword">COMMENT</span> <span class="string">'年龄'</span>,</span><br><span class="line"><span class="string">`sku_num`</span> <span class="built_in">BIGINT</span>(<span class="number">20</span>) <span class="keyword">DEFAULT</span> <span class="literal">NULL</span> <span class="keyword">COMMENT</span> <span class="string">'购买商品个数'</span>,</span><br><span class="line"><span class="string">`order_count`</span> <span class="built_in">BIGINT</span>(<span class="number">20</span>) <span class="keyword">DEFAULT</span> <span class="literal">NULL</span> <span class="keyword">COMMENT</span> <span class="string">'订单个数'</span>,</span><br><span class="line"><span class="string">`order_amount`</span> <span class="built_in">DECIMAL</span>(<span class="number">16</span>,<span class="number">2</span>) <span class="keyword">DEFAULT</span> <span class="literal">NULL</span> <span class="keyword">COMMENT</span> <span class="string">'销售额'</span>,</span><br><span class="line"><span class="string">`avg_amount`</span> <span class="built_in">DECIMAL</span>(<span class="number">10</span>,<span class="number">2</span>) <span class="keyword">DEFAULT</span> <span class="literal">NULL</span> <span class="keyword">COMMENT</span> <span class="string">'平均客单价'</span>,</span><br><span class="line"><span class="string">`ds`</span> <span class="built_in">VARCHAR</span>(<span class="number">20</span>) <span class="keyword">NOT</span> <span class="literal">NULL</span> <span class="keyword">COMMENT</span> <span class="string">'日期'</span>,</span><br><span class="line"> PRIMARY <span class="keyword">KEY</span> (<span class="string">`age`</span>,<span class="string">`ds`</span>)</span><br><span class="line">) <span class="keyword">ENGINE</span>=<span class="keyword">INNODB</span> <span class="keyword">DEFAULT</span> <span class="keyword">CHARSET</span>=utf8 <span class="keyword">COMMENT</span>=<span class="string">'年龄销售统计'</span>;</span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> trade_province_d</span><br><span class="line">(</span><br><span class="line">province_id <span class="built_in">VARCHAR</span>(<span class="number">20</span>) <span class="keyword">NOT</span> <span class="literal">NULL</span> <span class="keyword">COMMENT</span> <span class="string">'省市 id'</span>,</span><br><span class="line">province_name <span class="built_in">VARCHAR</span>(<span class="number">20</span>) <span class="keyword">COMMENT</span> <span class="string">'省市名称'</span>,</span><br><span class="line">region_id <span class="built_in">VARCHAR</span>(<span class="number">20</span>) <span class="keyword">COMMENT</span> <span class="string">'地区 ID'</span>,</span><br><span class="line">region_name <span class="built_in">VARCHAR</span>(<span class="number">20</span>) <span class="keyword">COMMENT</span> <span class="string">'地区名称'</span>,</span><br><span class="line">sku_num <span class="built_in">BIGINT</span> <span class="keyword">COMMENT</span> <span class="string">'购买商品个数'</span>,</span><br><span class="line">order_count <span class="built_in">BIGINT</span> <span class="keyword">COMMENT</span> <span class="string">'订单个数'</span>,</span><br><span class="line">order_amount <span class="built_in">DECIMAL</span>(<span class="number">16</span>,<span class="number">2</span>) <span class="keyword">COMMENT</span> <span class="string">'销售额'</span>,</span><br><span class="line">avg_amount <span class="built_in">DECIMAL</span>(<span class="number">10</span>,<span class="number">2</span>) <span class="keyword">COMMENT</span> <span class="string">'平均客单价'</span>,</span><br><span class="line"><span class="string">`ds`</span> <span class="built_in">VARCHAR</span>(<span class="number">20</span>) <span class="keyword">NOT</span> <span class="literal">NULL</span> <span class="keyword">COMMENT</span> <span class="string">'日期'</span> ,</span><br><span class="line">PRIMARY <span class="keyword">KEY</span> (<span class="string">`province_id`</span>,<span class="string">`ds`</span>)</span><br><span class="line">)<span class="keyword">ENGINE</span>=<span class="keyword">INNODB</span> <span class="keyword">DEFAULT</span> <span class="keyword">CHARSET</span>=utf8</span><br><span class="line"><span class="keyword">COMMENT</span> <span class="string">'地区销售统计'</span>;</span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> <span class="string">`trade_sku`</span> (</span><br><span class="line"><span class="string">`sku_id`</span> <span class="built_in">VARCHAR</span>(<span class="number">20</span>) <span class="keyword">NOT</span> <span class="literal">NULL</span> <span class="keyword">COMMENT</span> <span class="string">'商品 id'</span>,</span><br><span class="line"><span class="string">`sku_name`</span> <span class="built_in">VARCHAR</span>(<span class="number">200</span>) <span class="keyword">DEFAULT</span> <span class="literal">NULL</span> <span class="keyword">COMMENT</span> <span class="string">'商品名称'</span>,</span><br><span class="line"><span class="string">`sku_num`</span> <span class="built_in">BIGINT</span>(<span class="number">20</span>) <span class="keyword">DEFAULT</span> <span class="literal">NULL</span> <span class="keyword">COMMENT</span> <span class="string">'购买商品个数'</span>,</span><br><span class="line"><span class="string">`category3_id`</span> <span class="built_in">VARCHAR</span>(<span class="number">20</span>) <span class="keyword">COMMENT</span> <span class="string">'三级分类 id'</span>,</span><br><span class="line"><span class="string">`category2_id`</span> <span class="built_in">VARCHAR</span>(<span class="number">20</span>) <span class="keyword">COMMENT</span> <span class="string">'二级分类 id'</span>,</span><br><span class="line"><span class="string">`category1_id`</span> <span class="built_in">VARCHAR</span>(<span class="number">20</span>) <span class="keyword">COMMENT</span> <span class="string">'一级分类 id'</span>,</span><br><span class="line"><span class="string">`category3_name`</span> <span class="built_in">VARCHAR</span>(<span class="number">20</span>) <span class="keyword">COMMENT</span> <span class="string">'三级分类名称'</span>,</span><br><span class="line"><span class="string">`category2_name`</span> <span class="built_in">VARCHAR</span>(<span class="number">20</span>) <span class="keyword">COMMENT</span> <span class="string">'二级分类名称'</span>,</span><br><span class="line"><span class="string">`category1_name`</span> <span class="built_in">VARCHAR</span>(<span class="number">20</span>) <span class="keyword">COMMENT</span> <span class="string">'一级分类名称'</span>,</span><br><span class="line"><span class="string">`order_count`</span> <span class="built_in">BIGINT</span>(<span class="number">20</span>) <span class="keyword">DEFAULT</span> <span class="literal">NULL</span> <span class="keyword">COMMENT</span> <span class="string">'订单个数'</span>,</span><br><span class="line"><span class="string">`order_amount`</span> <span class="built_in">DECIMAL</span>(<span class="number">16</span>,<span class="number">2</span>) <span class="keyword">DEFAULT</span> <span class="literal">NULL</span> <span class="keyword">COMMENT</span> <span class="string">'销售额'</span>,</span><br><span class="line"><span class="string">`avg_amount`</span> <span class="built_in">DECIMAL</span>(<span class="number">10</span>,<span class="number">2</span>) <span class="keyword">DEFAULT</span> <span class="literal">NULL</span> <span class="keyword">COMMENT</span> <span class="string">'平均客单价'</span>,</span><br><span class="line"><span class="string">`ds`</span> <span class="built_in">VARCHAR</span>(<span class="number">20</span>) <span class="keyword">NOT</span> <span class="literal">NULL</span> <span class="keyword">COMMENT</span> <span class="string">'日期'</span>,</span><br><span class="line">PRIMARY <span class="keyword">KEY</span> (<span class="string">`sku_id`</span>,<span class="string">`ds`</span>)</span><br><span class="line">) <span class="keyword">ENGINE</span>=<span class="keyword">INNODB</span> <span class="keyword">DEFAULT</span> <span class="keyword">CHARSET</span>=utf8 <span class="keyword">COMMENT</span>=<span class="string">'商品销售统计'</span>;</span><br></pre></td></tr></table></figure>

<p>3）查看创建好的 4 张表</p>
<p><img src="https://leinuo-blog.oss-cn-beijing.aliyuncs.com/images/typora/1594028374416.png" alt="1594028374416"></p>
<h3 id="8-2-创建商品-销售-数据同步节点"><a href="#8-2-创建商品-销售-数据同步节点" class="headerlink" title="8.2 创建商品 销售 数据同步节点"></a>8.2 创建商品 销售 数据同步节点</h3><p>将 ADS 层数据导出到 MaxCompute。<br>1）数据开发-&gt;数据集成-&gt;新建数据集成节点-&gt;数据同步</p>
<p><img src="https://leinuo-blog.oss-cn-beijing.aliyuncs.com/images/typora/1594028392881.png" alt="1594028392881"></p>
<p>2）新建节点 ads_trade_sku_exp</p>
<p><img src="https://leinuo-blog.oss-cn-beijing.aliyuncs.com/images/typora/1594028401374.png" alt="1594028401374"></p>
<p>3）建立连接 RDS 的数据源（由于和之前导入时的 rds 不是一个 databaseName 所以要新建一个）</p>
<p><img src="https://leinuo-blog.oss-cn-beijing.aliyuncs.com/images/typora/1594028411330.png" alt="1594028411330"></p>
<p>4）设定要导出的数据库</p>
<p><img src="https://leinuo-blog.oss-cn-beijing.aliyuncs.com/images/typora/1594028418254.png" alt="1594028418254"></p>
<p><img src="https://leinuo-blog.oss-cn-beijing.aliyuncs.com/images/typora/1594028423043.png" alt="1594028423043"></p>
<p>5）加入数据源后刷新菜单可以看到新的数据源。</p>
<p><img src="https://leinuo-blog.oss-cn-beijing.aliyuncs.com/images/typora/1594028438277.png" alt="1594028438277"></p>
<p>6）映射部分<br>注意 此处需注意如果想把分区字段导入到目标表中，需要手动添加该分区字段</p>
<p><img src="https://leinuo-blog.oss-cn-beijing.aliyuncs.com/images/typora/1594028447324.png" alt="1594028447324"></p>
<p>7）配置参数<br>点击调度配置-&gt; bizdate=${yyyymmdd}</p>
<h3 id="8-3-创建年龄统计表数据同步节点"><a href="#8-3-创建年龄统计表数据同步节点" class="headerlink" title="8.3 创建年龄统计表数据同步节点"></a>8.3 创建年龄统计表数据同步节点</h3><p>1）数据开发-&gt;数据集成-&gt;新建数据集成节点-&gt;数据同步<br>2）创建节点 ads_trade_age_exp</p>
<p><img src="https://leinuo-blog.oss-cn-beijing.aliyuncs.com/images/typora/1594028487282.png" alt="1594028487282"></p>
<p>3）配置年龄统计表导出详情</p>
<p><img src="https://leinuo-blog.oss-cn-beijing.aliyuncs.com/images/typora/1594028495360.png" alt="1594028495360"></p>
<p>4）配置参数<br>点击调度配置-&gt; bizdate=${yyyymmdd}</p>
<h3 id="8-4-创建省市统计表数据同步节点"><a href="#8-4-创建省市统计表数据同步节点" class="headerlink" title="8.4 创建省市统计表数据同步节点"></a>8.4 创建省市统计表数据同步节点</h3><p>1）数据开发-&gt;数据集成-&gt;新建数据集成节点-&gt;数据同步<br>2）创建节点 ads_trade_province_exp</p>
<p><img src="https://leinuo-blog.oss-cn-beijing.aliyuncs.com/images/typora/1594028511288.png" alt="1594028511288"></p>
<p>3）配置省市统计表导出详情</p>
<p><img src="https://leinuo-blog.oss-cn-beijing.aliyuncs.com/images/typora/1594028518736.png" alt="1594028518736"></p>
<p>4）配置参数<br>点击调度配置-&gt; bizdate=${yyyymmdd}</p>
<h3 id="8-5-创建渠道统计表数据同步节点"><a href="#8-5-创建渠道统计表数据同步节点" class="headerlink" title="8.5 创建渠道统计表数据同步节点"></a>8.5 创建渠道统计表数据同步节点</h3><p>1）数据开发-&gt;数据集成-&gt;新建数据集成节点-&gt;数据同步<br>2）创建节点 ads_uv_source_exp</p>
<p><img src="https://leinuo-blog.oss-cn-beijing.aliyuncs.com/images/typora/1594028532113.png" alt="1594028532113"></p>
<p>3）渠道日活统计</p>
<p><img src="https://leinuo-blog.oss-cn-beijing.aliyuncs.com/images/typora/1594028538553.png" alt="1594028538553"></p>
<p>4）配置参数<br>点击调度配置-&gt; bizdate=${yyyymmdd}</p>
<h3 id="8-6-作业调度"><a href="#8-6-作业调度" class="headerlink" title="8.6 作业调度"></a>8.6 作业调度</h3><p>1）把 ADS 层的数据流指向对应的数据导出节点。</p>
<p><img src="https://leinuo-blog.oss-cn-beijing.aliyuncs.com/images/typora/1594028553109.png" alt="1594028553109"></p>
<p><img src="https://leinuo-blog.oss-cn-beijing.aliyuncs.com/images/typora/1594028558229.png" alt="1594028558229"></p>
<p>2）执行调度策略</p>
<p><img src="https://leinuo-blog.oss-cn-beijing.aliyuncs.com/images/typora/1594028575821.png" alt="1594028575821"></p>
<p>3）流程跑完之后可以在 MySql RDS 中查看最后统计结果表中的数据 ，如下图</p>
<p><img src="https://leinuo-blog.oss-cn-beijing.aliyuncs.com/images/typora/1594028586210.png" alt="1594028586210"></p>
]]></content>
      <categories>
        <category>大数据</category>
        <category>大数据实战项目</category>
        <category>阿里云离线数仓</category>
        <category>5.数据导出与作业调度</category>
      </categories>
      <tags>
        <tag>大数据</tag>
      </tags>
  </entry>
  <entry>
    <title>6.数据可视化</title>
    <url>/2020/04/07/6.%E6%95%B0%E6%8D%AE%E5%8F%AF%E8%A7%86%E5%8C%96/</url>
    <content><![CDATA[<h2 id="第-9-章-数据可视化"><a href="#第-9-章-数据可视化" class="headerlink" title="第 9 章 数据可视化"></a>第 9 章 数据可视化</h2><h3 id="9-1-可视化工具"><a href="#9-1-可视化工具" class="headerlink" title="9.1 可视化工具"></a>9.1 可视化工具</h3><h4 id="9-1-1-DataV-简介"><a href="#9-1-1-DataV-简介" class="headerlink" title="9.1.1 DataV 简介"></a>9.1.1 DataV 简介</h4><p>DataV 框架是阿里云主推的大屏实时可视化工具。<br>1）官方入口地址：<br><a href="https://data.aliyun.com/visual/datav?spm=a2c0j.8190895.1kquk9v2l.3.e98b7d6aI6ZvXa" target="_blank" rel="noopener">https://data.aliyun.com/visual/datav?spm=a2c0j.8190895.1kquk9v2l.3.e98b7d6aI6ZvXa</a><br>2）官方模板案例</p>
<p><img src="https://leinuo-blog.oss-cn-beijing.aliyuncs.com/images/typora/1594028644031.png" alt="1594028644031"></p>
<p><img src="https://leinuo-blog.oss-cn-beijing.aliyuncs.com/images/typora/1594028652729.png" alt="1594028652729"></p>
<p><img src="https://leinuo-blog.oss-cn-beijing.aliyuncs.com/images/typora/1594028659371.png" alt="1594028659371"></p>
<h3 id="9-1-2-买-购买-QuickBI"><a href="#9-1-2-买-购买-QuickBI" class="headerlink" title="9.1.2 买 购买 QuickBI"></a>9.1.2 买 购买 QuickBI</h3><p>QuickBI 是阿里云推出的主要展示离线报表数据的可视化工具。<br>1）入口地址：<a href="https://data.aliyun.com/product/bi" target="_blank" rel="noopener">https://data.aliyun.com/product/bi</a></p>
<p><img src="https://leinuo-blog.oss-cn-beijing.aliyuncs.com/images/typora/1594028671949.png" alt="1594028671949"></p>
<p>2）标准版 30 天试用申请</p>
<p><img src="https://leinuo-blog.oss-cn-beijing.aliyuncs.com/images/typora/1594028678365.png" alt="1594028678365"></p>
<p>3）进入 Quick BI 标准版</p>
<p><img src="https://leinuo-blog.oss-cn-beijing.aliyuncs.com/images/typora/1594028686716.png" alt="1594028686716"></p>
<h3 id="9-2-各个渠道日活占比图"><a href="#9-2-各个渠道日活占比图" class="headerlink" title="9.2 各个渠道日活占比图"></a>9.2 各个渠道日活占比图</h3><h4 id="9-2-1-配置数据源"><a href="#9-2-1-配置数据源" class="headerlink" title="9.2.1 配置数据源"></a>9.2.1 配置数据源</h4><p>1）选择主面板下的数据源栏位</p>
<p><img src="https://leinuo-blog.oss-cn-beijing.aliyuncs.com/images/typora/1594028698072.png" alt="1594028698072"></p>
<p>2）进入工作空间，选择数据源，列表右上角选择【新建数据源】</p>
<p><img src="https://leinuo-blog.oss-cn-beijing.aliyuncs.com/images/typora/1594028708335.png" alt="1594028708335"></p>
<p>3）选择 MySql</p>
<p><img src="https://leinuo-blog.oss-cn-beijing.aliyuncs.com/images/typora/1594028715565.png" alt="1594028715565"></p>
<p>4）弹出输入框，按要求填写</p>
<p><img src="https://leinuo-blog.oss-cn-beijing.aliyuncs.com/images/typora/1594028721962.png" alt="1594028721962"></p>
<p>（1）显示名称：名称可以任意起，这里面起的是 gmall_adb<br>（2）数据库地址：去 RDS 控制台中查看【内网地址】</p>
<p><img src="https://leinuo-blog.oss-cn-beijing.aliyuncs.com/images/typora/1594028730074.png" alt="1594028730074"></p>
<p>（3）数据库：存储数仓分析后的结果数据库，这里面是 gmall_adb<br>（4）添加白名单</p>
<p><img src="https://leinuo-blog.oss-cn-beijing.aliyuncs.com/images/typora/1594028738098.png" alt="1594028738098"></p>
<p>（5）填写完成后，点击连接测试，通过后，进行后续操作<br>5）看到数据源列表里有对应的数据库和其下的数据表信息。</p>
<p><img src="https://leinuo-blog.oss-cn-beijing.aliyuncs.com/images/typora/1594028747093.png" alt="1594028747093"></p>
<p>9.2.2 配置数据集<br>每一个图表都要对应一个数据集，一个数据集也对应数据库的一张表。可以说数据集是<br>在 BI 系统中数据表转化为图表的一个中间形态。</p>
<p>1）在数据集中创建一个文件夹（用于存储数据集）</p>
<p><img src="https://leinuo-blog.oss-cn-beijing.aliyuncs.com/images/typora/1594028757927.png" alt="1594028757927"></p>
<p>2）创建数据集</p>
<p><img src="https://leinuo-blog.oss-cn-beijing.aliyuncs.com/images/typora/1594028764458.png" alt="1594028764458"></p>
<p><img src="https://leinuo-blog.oss-cn-beijing.aliyuncs.com/images/typora/1594028814631.png" alt="1594028814631"></p>
<p>3）点击数据集</p>
<p><img src="https://leinuo-blog.oss-cn-beijing.aliyuncs.com/images/typora/1594028819994.png" alt="1594028819994"></p>
<p>4）在数据集中找到 ds 字段，转换该维度类型为时间维度</p>
<p><img src="https://leinuo-blog.oss-cn-beijing.aliyuncs.com/images/typora/1594028830008.png" alt="1594028830008"></p>
<p>（1）转换好以后，ds 则成为时间维度，之后可以用来过滤查询。</p>
<p><img src="https://leinuo-blog.oss-cn-beijing.aliyuncs.com/images/typora/1594028843239.png" alt="1594028843239"></p>
<p>（2）记得在右上角进行最后的保存</p>
<p><img src="https://leinuo-blog.oss-cn-beijing.aliyuncs.com/images/typora/1594028853555.png" alt="1594028853555"></p>
<p>（3）点击保存-&gt;点击刷新预览</p>
<p><img src="https://leinuo-blog.oss-cn-beijing.aliyuncs.com/images/typora/1594028859769.png" alt="1594028859769"></p>
<h3 id="9-2-3-配置饼图仪表盘"><a href="#9-2-3-配置饼图仪表盘" class="headerlink" title="9.2.3 配置饼图仪表盘"></a>9.2.3 配置饼图仪表盘</h3><p>1）仪表板-&gt;新建文件夹</p>
<p><img src="https://leinuo-blog.oss-cn-beijing.aliyuncs.com/images/typora/1594028872288.png" alt="1594028872288"></p>
<p><img src="https://leinuo-blog.oss-cn-beijing.aliyuncs.com/images/typora/1594028877124.png" alt="1594028877124"></p>
<p>2）点击进入 20191008 文件夹-&gt;新建仪表板</p>
<p><img src="https://leinuo-blog.oss-cn-beijing.aliyuncs.com/images/typora/1594028884860.png" alt="1594028884860"></p>
<p>3）仪表板首页-&gt;删除线图-&gt;拖拽饼图到控制台</p>
<p><img src="https://leinuo-blog.oss-cn-beijing.aliyuncs.com/images/typora/1594028892213.png" alt="1594028892213"></p>
<p>4）点击请选择数据集</p>
<p><img src="https://leinuo-blog.oss-cn-beijing.aliyuncs.com/images/typora/1594028899449.png" alt="1594028899449"></p>
<p>5）右半部分选择对应的数据集</p>
<p><img src="https://leinuo-blog.oss-cn-beijing.aliyuncs.com/images/typora/1594028908212.png" alt="1594028908212"></p>
<p>6）把对应的维度和量度拖放到对应的位置-&gt;点击更新按钮</p>
<p><img src="https://leinuo-blog.oss-cn-beijing.aliyuncs.com/images/typora/1594028916497.png" alt="1594028916497"></p>
<p>维度：一般指类型、状态、性别、地区等码表键值等信息。<br>量度：一般指可以汇总求和的信息，比如交易额、数量、人次等。<br>过滤器中一般是指该仪表盘默认需要进行筛选的信息。这里咱们设为显示当日的数据。</p>
<p><img src="https://leinuo-blog.oss-cn-beijing.aliyuncs.com/images/typora/1594028926859.png" alt="1594028926859"></p>
<p><img src="https://leinuo-blog.oss-cn-beijing.aliyuncs.com/images/typora/1594028931397.png" alt="1594028931397"></p>
<p>7）具体饼图的一些显示样式可以去右侧【样式】标签中调整。</p>
<p><img src="https://leinuo-blog.oss-cn-beijing.aliyuncs.com/images/typora/1594028937833.png" alt="1594028937833"></p>
<p><img src="https://leinuo-blog.oss-cn-beijing.aliyuncs.com/images/typora/1594028941978.png" alt="1594028941978"></p>
<p>8）最后记得右上角保存</p>
<p><img src="https://leinuo-blog.oss-cn-beijing.aliyuncs.com/images/typora/1594028949695.png" alt="1594028949695"></p>
<h3 id="9-3-地区销售额分析"><a href="#9-3-地区销售额分析" class="headerlink" title="9.3 地区销售额分析"></a>9.3 地区销售额分析</h3><h3 id="9-3-1-配置数据集"><a href="#9-3-1-配置数据集" class="headerlink" title="9.3.1 配置数据集"></a>9.3.1 配置数据集</h3><p>1）在数据源中，创建地区数据集</p>
<p><img src="https://leinuo-blog.oss-cn-beijing.aliyuncs.com/images/typora/1594028965825.png" alt="1594028965825"></p>
<p><img src="https://leinuo-blog.oss-cn-beijing.aliyuncs.com/images/typora/1594028969185.png" alt="1594028969185"></p>
<p>2）点击地区数据集</p>
<p><img src="https://leinuo-blog.oss-cn-beijing.aliyuncs.com/images/typora/1594028975631.png" alt="1594028975631"></p>
<p>3）修改 ds 为时间维度</p>
<p><img src="https://leinuo-blog.oss-cn-beijing.aliyuncs.com/images/typora/1594028982135.png" alt="1594028982135"></p>
<p>4）把 province_name 转换为地理信息维度</p>
<p><img src="https://leinuo-blog.oss-cn-beijing.aliyuncs.com/images/typora/1594028989563.png" alt="1594028989563"></p>
<p>5）把 region_name 转换为地理信息维度的区域信息</p>
<p><img src="https://leinuo-blog.oss-cn-beijing.aliyuncs.com/images/typora/1594028995734.png" alt="1594028995734"></p>
<p>6）调整好后，可以看到维度的图标发生了变化。</p>
<p><img src="https://leinuo-blog.oss-cn-beijing.aliyuncs.com/images/typora/1594029005856.png" alt="1594029005856"></p>
<p>7）点击刷新预览，查看数据</p>
<p><img src="https://leinuo-blog.oss-cn-beijing.aliyuncs.com/images/typora/1594029021704.png" alt="1594029021704"></p>
<h4 id="9-3-2-配置地图仪表盘"><a href="#9-3-2-配置地图仪表盘" class="headerlink" title="9.3.2 配置地图仪表盘"></a>9.3.2 配置地图仪表盘</h4><p>1）新增仪表盘</p>
<p><img src="https://leinuo-blog.oss-cn-beijing.aliyuncs.com/images/typora/1594029033169.png" alt="1594029033169"></p>
<p>2）选择色彩地图</p>
<p><img src="https://leinuo-blog.oss-cn-beijing.aliyuncs.com/images/typora/1594029038898.png" alt="1594029038898"></p>
<p>3）右侧菜单，维度选择省份的地理维度，量度选择订单金额，过滤器选择天。</p>
<p><img src="https://leinuo-blog.oss-cn-beijing.aliyuncs.com/images/typora/1594029046218.png" alt="1594029046218"></p>
<p>4）点击右下角更新可以看到地图出现，右上角保存，把仪表盘保存起来。</p>
<p><img src="https://leinuo-blog.oss-cn-beijing.aliyuncs.com/images/typora/1594029054494.png" alt="1594029054494"></p>
<p><img src="https://leinuo-blog.oss-cn-beijing.aliyuncs.com/images/typora/1594029058042.png" alt="1594029058042"></p>
<h3 id="9-4-年龄段-销售额-占比分析"><a href="#9-4-年龄段-销售额-占比分析" class="headerlink" title="9.4 年龄段 销售额 占比分析"></a>9.4 年龄段 销售额 占比分析</h3><h3 id="9-4-1-配置数据集"><a href="#9-4-1-配置数据集" class="headerlink" title="9.4.1 配置数据集"></a>9.4.1 配置数据集</h3><p>1）创建数据集</p>
<p><img src="https://leinuo-blog.oss-cn-beijing.aliyuncs.com/images/typora/1594029068743.png" alt="1594029068743"></p>
<p><img src="https://leinuo-blog.oss-cn-beijing.aliyuncs.com/images/typora/1594029072168.png" alt="1594029072168"></p>
<p>2）点击数据集</p>
<p><img src="https://leinuo-blog.oss-cn-beijing.aliyuncs.com/images/typora/1594029079509.png" alt="1594029079509"></p>
<p>3）将 ds 转换为时间维度</p>
<p><img src="https://leinuo-blog.oss-cn-beijing.aliyuncs.com/images/typora/1594029085227.png" alt="1594029085227"></p>
<p>4）把年龄转化为维度</p>
<p><img src="https://leinuo-blog.oss-cn-beijing.aliyuncs.com/images/typora/1594029092547.png" alt="1594029092547"></p>
<p>5）添加计算字段<br>因为原始数据表是以年龄为维度，但是实际统计需要用年龄段进行统计，比如 10 岁到20 岁，20 岁到 30 岁等。所以要重新做一个“计算字段”</p>
<p><img src="https://leinuo-blog.oss-cn-beijing.aliyuncs.com/images/typora/1594029109572.png" alt="1594029109572"></p>
<p>要把年龄变成年龄段需要编写一个表达式，该表达式语法完全遵循 MySql 的语法和函数。<br>表达式：</p>
<figure class="highlight"><table><tr><td class="code"><pre><span class="line">concat(floor([age]/10)*10,'到',floor([age]/10+1)*10-1,'岁')</span><br></pre></td></tr></table></figure>

<p><img src="https://leinuo-blog.oss-cn-beijing.aliyuncs.com/images/typora/1594029123903.png" alt="1594029123903"></p>
<p>6）点击保存后，再刷新进行预览，可以看到新增的字段</p>
<p><img src="https://leinuo-blog.oss-cn-beijing.aliyuncs.com/images/typora/1594029161263.png" alt="1594029161263"></p>
<p><img src="https://leinuo-blog.oss-cn-beijing.aliyuncs.com/images/typora/1594029164141.png" alt="1594029164141"></p>
<h3 id="9-4-2-配置极坐标仪表盘"><a href="#9-4-2-配置极坐标仪表盘" class="headerlink" title="9.4.2 配置极坐标仪表盘"></a>9.4.2 配置极坐标仪表盘</h3><p>1）新增仪表盘</p>
<p><img src="https://leinuo-blog.oss-cn-beijing.aliyuncs.com/images/typora/1594029172725.png" alt="1594029172725"></p>
<p>2）选择极坐标仪表盘</p>
<p><img src="https://leinuo-blog.oss-cn-beijing.aliyuncs.com/images/typora/1594029180065.png" alt="1594029180065"></p>
<p>3）选择数据集</p>
<p><img src="https://leinuo-blog.oss-cn-beijing.aliyuncs.com/images/typora/1594029186214.png" alt="1594029186214"></p>
<p>4）拖拽对应的度量、维度和过滤器</p>
<p><img src="https://leinuo-blog.oss-cn-beijing.aliyuncs.com/images/typora/1594029192843.png" alt="1594029192843"></p>
<h3 id="9-5-热门商品分析"><a href="#9-5-热门商品分析" class="headerlink" title="9.5 热门商品分析"></a>9.5 热门商品分析</h3><h4 id="9-5-1-配置数据集"><a href="#9-5-1-配置数据集" class="headerlink" title="9.5.1 配置数据集"></a>9.5.1 配置数据集</h4><p>1）新增数据集</p>
<p><img src="https://leinuo-blog.oss-cn-beijing.aliyuncs.com/images/typora/1594029205721.png" alt="1594029205721"></p>
<p><img src="https://leinuo-blog.oss-cn-beijing.aliyuncs.com/images/typora/1594029208913.png" alt="1594029208913"></p>
<p>2）点击数据集</p>
<p><img src="https://leinuo-blog.oss-cn-beijing.aliyuncs.com/images/typora/1594029216530.png" alt="1594029216530"></p>
<p>3）转换 ds 为日期维度</p>
<p><img src="https://leinuo-blog.oss-cn-beijing.aliyuncs.com/images/typora/1594029222620.png" alt="1594029222620"></p>
<p>4）点击刷新预览</p>
<p><img src="https://leinuo-blog.oss-cn-beijing.aliyuncs.com/images/typora/1594029230158.png" alt="1594029230158"></p>
<h3 id="9-5-2-配置柱图仪表盘"><a href="#9-5-2-配置柱图仪表盘" class="headerlink" title="9.5.2 配置柱图仪表盘"></a>9.5.2 配置柱图仪表盘</h3><p>1）新增仪表盘</p>
<p><img src="https://leinuo-blog.oss-cn-beijing.aliyuncs.com/images/typora/1594029240701.png" alt="1594029240701"></p>
<p>2）选择柱图</p>
<p><img src="https://leinuo-blog.oss-cn-beijing.aliyuncs.com/images/typora/1594029247316.png" alt="1594029247316"></p>
<p>3）拖拽相应的度量、维度、过滤器</p>
<p><img src="https://leinuo-blog.oss-cn-beijing.aliyuncs.com/images/typora/1594029253295.png" alt="1594029253295"></p>
<p><img src="https://leinuo-blog.oss-cn-beijing.aliyuncs.com/images/typora/1594029257241.png" alt="1594029257241"></p>
<p>3）样式调整<br>（1）样式中选择 Y 轴</p>
<p><img src="https://leinuo-blog.oss-cn-beijing.aliyuncs.com/images/typora/1594029264122.png" alt="1594029264122"></p>
<p>（2）垂直左轴和右轴分别填写标题和单位</p>
<p><img src="https://leinuo-blog.oss-cn-beijing.aliyuncs.com/images/typora/1594029271024.png" alt="1594029271024"></p>
<p><img src="https://leinuo-blog.oss-cn-beijing.aliyuncs.com/images/typora/1594029273826.png" alt="1594029273826"></p>
<p>（3）缩略轴可以控制显示个数</p>
<p><img src="https://leinuo-blog.oss-cn-beijing.aliyuncs.com/images/typora/1594029284459.png" alt="1594029284459"></p>
<h4 id="9-5-3-查询控件"><a href="#9-5-3-查询控件" class="headerlink" title="9.5.3 查询控件"></a>9.5.3 查询控件</h4><p>1）加入一个查询控件<br>这个查询控件可以和其他图表控件进行绑定，从而达到更好的动态筛选效果。</p>
<p><img src="https://leinuo-blog.oss-cn-beijing.aliyuncs.com/images/typora/1594029297869.png" alt="1594029297869"></p>
<p>（1）双击需要筛选的条件字段，放到查询控件的查询源字段中，下图放入了一级分类名称、二级分类名称、三级分类名称。</p>
<p><img src="https://leinuo-blog.oss-cn-beijing.aliyuncs.com/images/typora/1594029309089.png" alt="1594029309089"></p>
<p>（2）设置查询绑定，在每个查询源字段的齿轮上可以设定对应的图表控件。</p>
<p><img src="https://leinuo-blog.oss-cn-beijing.aliyuncs.com/images/typora/1594029317509.png" alt="1594029317509"></p>
<p>（3）选择要绑定的图表</p>
<p><img src="https://leinuo-blog.oss-cn-beijing.aliyuncs.com/images/typora/1594029323717.png" alt="1594029323717"></p>
<p>2）实际的效果就是查询控件中选择某个分类，点击查询，下方的图表会自动筛选数据。</p>
<p><img src="https://leinuo-blog.oss-cn-beijing.aliyuncs.com/images/typora/1594029331339.png" alt="1594029331339"></p>
<h3 id="9-6-分享仪表盘"><a href="#9-6-分享仪表盘" class="headerlink" title="9.6 分享仪表盘"></a>9.6 分享仪表盘</h3><h4 id="9-6-1-访问链接"><a href="#9-6-1-访问链接" class="headerlink" title="9.6.1 访问链接"></a>9.6.1 访问链接</h4><p>即使仪表盘制作完成，但是如果只停留在开发页面是不适合提供给数据分析人员、运营<br>人员或者管理层用户使用的。这些数据分析用户不需要进行开发，只需要从固定的只读展示<br>页面进行查看就好了。</p>
<p>1）在仪表盘列表里的小眼睛就是该仪表盘的只读页面。</p>
<p><img src="https://leinuo-blog.oss-cn-beijing.aliyuncs.com/images/typora/1594029348331.png" alt="1594029348331"></p>
<p>2）点击查看，可以得到只读页面，那么上方的路径地址就是可以发布出去的访问路径。</p>
<p><img src="https://leinuo-blog.oss-cn-beijing.aliyuncs.com/images/typora/1594029360020.png" alt="1594029360020"></p>
<p>3）权限管理<br>如果用其他阿里云账号访问该路径的页面却需要提交权限申请。</p>
<p><img src="https://leinuo-blog.oss-cn-beijing.aliyuncs.com/images/typora/1594029369306.png" alt="1594029369306"></p>
<h4 id="9-6-2-权限赋予"><a href="#9-6-2-权限赋予" class="headerlink" title="9.6.2 权限赋予"></a>9.6.2 权限赋予</h4><p>1）权限赋予有两种途径：<br>（1）一种是通过该界面提交申请，由管理员在审核页面进行审批，从而让其他用户获得访问权限。</p>
<p><img src="https://leinuo-blog.oss-cn-beijing.aliyuncs.com/images/typora/1594029384282.png" alt="1594029384282"></p>
<p>（2）另一种是在仪表盘列表中的【分享】按钮</p>
<p><img src="https://leinuo-blog.oss-cn-beijing.aliyuncs.com/images/typora/1594029394371.png" alt="1594029394371"></p>
<p>在分享界面中填写，要授予的用户的阿里云账号及访问期限即可。</p>
<p><img src="https://leinuo-blog.oss-cn-beijing.aliyuncs.com/images/typora/1594029400929.png" alt="1594029400929"></p>
<h4 id="9-7-数据门户（高级版）-（可选）"><a href="#9-7-数据门户（高级版）-（可选）" class="headerlink" title="9.7 数据门户（高级版） （可选）"></a>9.7 数据门户（高级版） （可选）</h4><p>门户也叫数据产品，是通过菜单形式组织的仪表板的集合。通过数据门户可以制作复杂的带导航菜单的专题类分析。</p>
<p><img src="https://leinuo-blog.oss-cn-beijing.aliyuncs.com/images/typora/1594029416147.png" alt="1594029416147"></p>
<h4 id="9-7-1-新建数据门户"><a href="#9-7-1-新建数据门户" class="headerlink" title="9.7.1 新建数据门户"></a>9.7.1 新建数据门户</h4><p>1）单击工作空间 &gt; 数据门户，进入数据门户管理页面。<br>2）单击新建数据门户，进入数据门户编辑页面。</p>
<p>3）在页面设置标签页中，设置页面的标题、上传 logo、设置皮肤颜色、设置导航栏颜色以<br>及编辑页脚。</p>
<p><img src="https://leinuo-blog.oss-cn-beijing.aliyuncs.com/images/typora/1594029428987.png" alt="1594029428987"></p>
<p>4）门户整体风格，不做特别改变可以直接保存。</p>
<p><img src="https://leinuo-blog.oss-cn-beijing.aliyuncs.com/images/typora/1594029437098.png" alt="1594029437098"></p>
<p>5）保存完毕后关掉【页面设置】。</p>
<p><img src="https://leinuo-blog.oss-cn-beijing.aliyuncs.com/images/typora/1594029444354.png" alt="1594029444354"></p>
<p>6）然后进行【菜单设置】<br>在这个菜单设置中，主要是把每个菜单对应的仪表盘关联起来，方便用户查询。</p>
<p><img src="https://leinuo-blog.oss-cn-beijing.aliyuncs.com/images/typora/1594029455798.png" alt="1594029455798"></p>
<p><img src="https://leinuo-blog.oss-cn-beijing.aliyuncs.com/images/typora/1594029458838.png" alt="1594029458838"></p>
<p>7）保存起来，然后右上角【保存】起来。就可以看到这个门户保存成功了。</p>
<p><img src="https://leinuo-blog.oss-cn-beijing.aliyuncs.com/images/typora/1594029465842.png" alt="1594029465842"></p>
<h4 id="9-7-2-使用说明"><a href="#9-7-2-使用说明" class="headerlink" title="9.7.2 使用说明"></a>9.7.2 使用说明</h4><p>1 ）页面配置<br>在建立数据门户的页面中设置</p>
<p><img src="https://leinuo-blog.oss-cn-beijing.aliyuncs.com/images/typora/1594029481807.png" alt="1594029481807"></p>
<p>下方有个页面别名的链接，这个链接是可以发布出来，让数据分析师、运营人员进行访问的地址。<br>2）发布对外访问 IP 地址</p>
<p><img src="https://leinuo-blog.oss-cn-beijing.aliyuncs.com/images/typora/1594029497256.png" alt="1594029497256"></p>
<p><img src="https://leinuo-blog.oss-cn-beijing.aliyuncs.com/images/typora/1594029501482.png" alt="1594029501482"></p>
<h3 id="9-7-3-人员权限管理"><a href="#9-7-3-人员权限管理" class="headerlink" title="9.7.3 人员权限管理"></a>9.7.3 人员权限管理</h3><p>由于数据门户的参与者不单单是开发人员，还有分析师、运营等人员，所以多读写权限要有一定的要求。<br>1）在工作空间右上角点击【配置面板】的齿轮图标。</p>
<p><img src="https://leinuo-blog.oss-cn-beijing.aliyuncs.com/images/typora/1594029516327.png" alt="1594029516327"></p>
<p>2）添加组织成员</p>
<p><img src="https://leinuo-blog.oss-cn-beijing.aliyuncs.com/images/typora/1594029522098.png" alt="1594029522098"></p>
<p>3）需要用阿里云账号填写（淘宝、支付宝账号不行）。</p>
<p><img src="https://leinuo-blog.oss-cn-beijing.aliyuncs.com/images/typora/1594029529544.png" alt="1594029529544"></p>
<p>4）赋予角色身份<br>（1）点击左侧菜单【工作空间管理】，下方【工作空间成员】</p>
<p><img src="https://leinuo-blog.oss-cn-beijing.aliyuncs.com/images/typora/1594029605364.png" alt="1594029605364"></p>
<p>（2）在弹出框中选择对应的角色</p>
<p><img src="https://leinuo-blog.oss-cn-beijing.aliyuncs.com/images/typora/1594029612540.png" alt="1594029612540"></p>
]]></content>
      <categories>
        <category>大数据</category>
        <category>大数据实战项目</category>
        <category>阿里云离线数仓</category>
        <category>6.数据可视化</category>
      </categories>
      <tags>
        <tag>大数据</tag>
      </tags>
  </entry>
  <entry>
    <title>7.协同工作</title>
    <url>/2020/04/07/7.%E5%8D%8F%E5%90%8C%E5%B7%A5%E4%BD%9C/</url>
    <content><![CDATA[<h2 id="第-10-章-协同工作"><a href="#第-10-章-协同工作" class="headerlink" title="第 10 章 协同工作"></a>第 10 章 协同工作</h2><p>​    阿里云存在多个服务，显然是无法一个人完成的，需要多个人协同开发。<br>​    但是所有的服务器又都是有一个账号统一管理（一般是公司的账号），这个账号非常关键不可能让所有的开发人员人手一个，那么如何让这些程序员一起参与开发呢？<br>​    这就要用到 RAM 子账户。</p>
<h3 id="10-1-RAM-简介"><a href="#10-1-RAM-简介" class="headerlink" title="10.1 RAM 简介"></a>10.1 RAM 简介</h3><p>​    RAM（Resource Access Management）是阿里云为客户提供的用户身份管理与资源访问控制服务。</p>
<p>​    RAM 允许在一个云账号下创建并管理多个身份，并允许给单个身份或一组身份分配不<br>同的权限，从而实现不同用户拥有不同的云资源访问权限。</p>
<h3 id="10-2-RAM-管理界面"><a href="#10-2-RAM-管理界面" class="headerlink" title="10.2 RAM 管理界面"></a>10.2 RAM 管理界面</h3><p>1）访问地址：<a href="https://ram.console.aliyun.com/overview" target="_blank" rel="noopener">https://ram.console.aliyun.com/overview</a><br>2）或者在你登录阿里云后，点击右上角头像后菜单中的【访问控制】选项。</p>
<p><img src="https://leinuo-blog.oss-cn-beijing.aliyuncs.com/images/typora/1594029701368.png" alt="1594029701368"></p>
<p>3）立即开通访问控制 RAM</p>
<p><img src="https://leinuo-blog.oss-cn-beijing.aliyuncs.com/images/typora/1594029708070.png" alt="1594029708070"></p>
<p><img src="https://leinuo-blog.oss-cn-beijing.aliyuncs.com/images/typora/1594029715565.png" alt="1594029715565"></p>
<p><img src="https://leinuo-blog.oss-cn-beijing.aliyuncs.com/images/typora/1594029719647.png" alt="1594029719647"></p>
<h3 id="10-2-1-新增用户"><a href="#10-2-1-新增用户" class="headerlink" title="10.2.1 新增用户"></a>10.2.1 新增用户</h3><p>1）来到 RAM 访问控制-&gt;用户-&gt;新建用户</p>
<p><img src="https://leinuo-blog.oss-cn-beijing.aliyuncs.com/images/typora/1594029729451.png" alt="1594029729451"></p>
<p>2）配置新建用户</p>
<p><img src="https://leinuo-blog.oss-cn-beijing.aliyuncs.com/images/typora/1594029737740.png" alt="1594029737740"></p>
<p>3）新建用户成功</p>
<p><img src="https://leinuo-blog.oss-cn-beijing.aliyuncs.com/images/typora/1594029747299.png" alt="1594029747299"></p>
<h4 id="10-2-2-配置用户-信息"><a href="#10-2-2-配置用户-信息" class="headerlink" title="10.2.2 配置用户 信息"></a>10.2.2 配置用户 信息</h4><p>1）点击用户列表中的该用户</p>
<p><img src="https://leinuo-blog.oss-cn-beijing.aliyuncs.com/images/typora/1594029761774.png" alt="1594029761774"></p>
<p>2）编辑基本信息</p>
<p><img src="https://leinuo-blog.oss-cn-beijing.aliyuncs.com/images/typora/1594029767649.png" alt="1594029767649"></p>
<p>补充手机号码和邮箱</p>
<p><img src="https://leinuo-blog.oss-cn-beijing.aliyuncs.com/images/typora/1594029773830.png" alt="1594029773830"></p>
<p>3）增加 AccessKey：</p>
<p>在认证管理中增加该用户的访问密钥（某些情况下要用到）</p>
<p><img src="https://leinuo-blog.oss-cn-beijing.aliyuncs.com/images/typora/1594029786683.png" alt="1594029786683"></p>
<h3 id="10-2-3-赋权限"><a href="#10-2-3-赋权限" class="headerlink" title="10.2.3 赋权限"></a>10.2.3 赋权限</h3><p>1）在用户列表中点击权限管理</p>
<p><img src="https://leinuo-blog.oss-cn-beijing.aliyuncs.com/images/typora/1594029797241.png" alt="1594029797241"></p>
<p>2）可以各种权限列表中找到对应的服务器权限</p>
<p><img src="https://leinuo-blog.oss-cn-beijing.aliyuncs.com/images/typora/1594029804423.png" alt="1594029804423"></p>
<h3 id="10-3-如何登录"><a href="#10-3-如何登录" class="headerlink" title="10.3 如何登录"></a>10.3 如何登录</h3><h4 id="10-3-1-登录地址"><a href="#10-3-1-登录地址" class="headerlink" title="10.3.1 登录地址"></a>10.3.1 登录地址</h4><p>1）来到 RAM 访问控制</p>
<p><img src="https://leinuo-blog.oss-cn-beijing.aliyuncs.com/images/typora/1594029817200.png" alt="1594029817200"></p>
<p>2）把路径复制下来用浏览器打开<br><a href="https://signin.aliyun.com/1902761552218725.onaliyun.com/login.htm" target="_blank" rel="noopener">https://signin.aliyun.com/1902761552218725.onaliyun.com/login.htm</a></p>
<p><img src="https://leinuo-blog.oss-cn-beijing.aliyuncs.com/images/typora/1594029829197.png" alt="1594029829197"></p>
<p><img src="https://leinuo-blog.oss-cn-beijing.aliyuncs.com/images/typora/1594029833798.png" alt="1594029833798"></p>
<p>3）该用户确实可以登录</p>
<p><img src="https://leinuo-blog.oss-cn-beijing.aliyuncs.com/images/typora/1594029847450.png" alt="1594029847450"></p>
<p>4）但是会发现其实看不到已经有的项目</p>
<p><img src="https://leinuo-blog.oss-cn-beijing.aliyuncs.com/images/typora/1594029854747.png" alt="1594029854747"></p>
<h3 id="10-3-2-用户管理"><a href="#10-3-2-用户管理" class="headerlink" title="10.3.2 用户管理"></a>10.3.2 用户管理</h3><p>针对 DataWorks 的项目进行权限授予<br>1）在 DataWorks 中，点击上方的项目配置按钮</p>
<p><img src="https://leinuo-blog.oss-cn-beijing.aliyuncs.com/images/typora/1594029875994.png" alt="1594029875994"></p>
<p>2）添加项目成员</p>
<p><img src="https://leinuo-blog.oss-cn-beijing.aliyuncs.com/images/typora/1594029892337.png" alt="1594029892337"></p>
<p>3）选中要添加的成员并给予开发权限</p>
<p><img src="https://leinuo-blog.oss-cn-beijing.aliyuncs.com/images/typora/1594029899656.png" alt="1594029899656"></p>
<p>4）然后再回到 dev 用户的 Dataworks 界面中就可以看到对应的权限了。</p>
<p><img src="https://leinuo-blog.oss-cn-beijing.aliyuncs.com/images/typora/1594029911435.png" alt="1594029911435"></p>
<p>5）进入数据开发页面中，如果出现该页面，点击【这里】完善密钥</p>
<p><img src="https://leinuo-blog.oss-cn-beijing.aliyuncs.com/images/typora/1594029930582.png" alt="1594029930582"></p>
<p>（1）修改 AccessKey 信息</p>
<p><img src="https://leinuo-blog.oss-cn-beijing.aliyuncs.com/images/typora/1594029936537.png" alt="1594029936537"></p>
<p>（2）输入对应的密钥</p>
<p><img src="https://leinuo-blog.oss-cn-beijing.aliyuncs.com/images/typora/1594029942431.png" alt="1594029942431"></p>
<p>（3）登录完成</p>
]]></content>
      <categories>
        <category>大数据</category>
        <category>大数据实战项目</category>
        <category>阿里云离线数仓</category>
        <category>7.协同工作</category>
      </categories>
      <tags>
        <tag>大数据</tag>
      </tags>
  </entry>
  <entry>
    <title>Flume监控之Ganglia</title>
    <url>/2020/02/06/Flume%E7%9B%91%E6%8E%A7%E4%B9%8BGanglia/</url>
    <content><![CDATA[<h2 id="Ganglia-的安装与部署"><a href="#Ganglia-的安装与部署" class="headerlink" title="Ganglia 的安装与部署"></a>Ganglia 的安装与部署</h2><p>1) 安装 httpd 服务与 php</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[leinuo@hadoop102 flume]$ sudo yum -y install httpd php</span><br></pre></td></tr></table></figure>

<p>2) 安装其他依赖</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[leinuo@hadoop102 flume]$ sudo yum -y install rrdtool perl-rrdtool rrdtool-devel</span><br><span class="line">[leinuo@hadoop102 flume]$ sudo yum -y install apr-devel</span><br></pre></td></tr></table></figure>

<p>3) 安装 ganglia</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[leinuo@hadoop102 flume]$ sudo rpm -Uvh http://dl.fedoraproject.org/pub/epel/6/x86_64/epel-release-6-8.noarch.rpm</span><br><span class="line">[leinuo@hadoop102 flume]$ sudo yum -y install ganglia-gmetad</span><br><span class="line">[leinuo@hadoop102 flume]$ sudo yum -y install ganglia-web</span><br><span class="line">[leinuo@hadoop102 flume]$ sudo yum install -y ganglia-gmond</span><br></pre></td></tr></table></figure>

<p>4) 修改配置文件/etc/httpd/conf.d/ganglia.conf</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[leinuo@hadoop102 flume]$ sudo vim /etc/httpd/conf.d/ganglia.conf</span><br></pre></td></tr></table></figure>

<p>修改为*的配置 ：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> Ganglia monitoring system php web frontend</span></span><br><span class="line">Alias /ganglia /usr/share/ganglia</span><br><span class="line">&lt;Location /ganglia&gt;</span><br><span class="line">Order deny,allow</span><br><span class="line">Deny from all</span><br><span class="line">Allow from all               * </span><br><span class="line"><span class="meta">#</span><span class="bash"> Allow from 127.0.0.1       *</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> Allow from ::1             *</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> Allow from .example.com</span></span><br><span class="line">&lt;/Location&gt;</span><br></pre></td></tr></table></figure>

<p>5) 修改配置文件/etc/ganglia/gmetad.conf</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[leinuo@hadoop102 flume]$ sudo vim /etc/ganglia/gmetad.conf</span><br><span class="line">修改为：</span><br><span class="line">data_source "hadoop102" 192.168.25.102</span><br></pre></td></tr></table></figure>

<p>6) 修改配置文件/etc/ganglia/gmond.conf</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[leinuo@hadoop102 flume]$ sudo vim /etc/ganglia/gmond.conf</span><br></pre></td></tr></table></figure>

<p>修改为：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">cluster &#123;</span><br><span class="line">name = "hadoop102"                 *</span><br><span class="line">owner = "unspecified"</span><br><span class="line">latlong = "unspecified"</span><br><span class="line">url = "unspecified"</span><br><span class="line">&#125;</span><br><span class="line">udp_send_channel &#123;</span><br><span class="line"><span class="meta">#</span><span class="bash">bind_hostname = yes <span class="comment"># Highly recommended, soon to be default.</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> This option tells gmond to use a <span class="built_in">source</span></span></span><br><span class="line">address</span><br><span class="line"><span class="meta">#</span><span class="bash"> that resolves to the machine<span class="string">'s hostname.</span></span></span><br><span class="line">Without</span><br><span class="line"><span class="meta">#</span><span class="bash"> this, the metrics may appear to come from any</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> interface and the DNS names associated with</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> those IPs will be used to create the RRDs.</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> mcast_join = 239.2.11.71</span></span><br><span class="line">host = 192.168.1.102                 *</span><br><span class="line">port = 8649</span><br><span class="line">ttl = 1</span><br><span class="line">&#125;</span><br><span class="line">udp_recv_channel &#123;</span><br><span class="line"><span class="meta">#</span><span class="bash"> mcast_join = 239.2.11.71            *</span></span><br><span class="line">port = 8649</span><br><span class="line">bind = 192.168.25.102                 *</span><br><span class="line">retry_bind = true</span><br><span class="line"><span class="meta">#</span><span class="bash"> Size of the UDP buffer. If you are handling lots of metrics you really</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> should bump it up to e.g. 10MB or even higher.</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> buffer = 10485760</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>7) 修改配置文件/etc/selinux/config</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[leinuo@hadoop102 flume]$ sudo vim /etc/selinux/config</span><br></pre></td></tr></table></figure>

<p>修改为：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> This file controls the state of SELinux on the system.</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> SELINUX= can take one of these three values:</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> enforcing - SELinux security policy is enforced.</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> permissive - SELinux prints warnings instead of enforcing.</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> disabled - No SELinux policy is loaded.</span></span><br><span class="line">SELINUX=disabled           </span><br><span class="line"><span class="meta">#</span><span class="bash"> SELINUXTYPE= can take one of these two values:</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> targeted - Targeted processes are protected,</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> mls - Multi Level Security protection.</span></span><br><span class="line">SELINUXTYPE=targeted</span><br></pre></td></tr></table></figure>

<p><strong>尖叫提示</strong>：selinux 本次生效关闭必须重启，如果此时不想重启，可以临时生效之：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[leinuo@hadoop102 flume]$ sudo setenforce 0</span><br></pre></td></tr></table></figure>

<p>5) 启动 ganglia</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[leinuo@hadoop102 flume]$ sudo service httpd start</span><br><span class="line">[leinuo@hadoop102 flume]$ sudo service gmetad start</span><br><span class="line">[leinuo@hadoop102 flume]$ sudo service gmond start</span><br></pre></td></tr></table></figure>

<p>6) 打开网页浏览 ganglia 页面<br><a href="http://192.168.25.102/ganglia" target="_blank" rel="noopener">http://192.168.25.102/ganglia</a><br><strong>尖叫提示</strong>：如果完成以上操作依然出现权限不足错误，请修改/var/lib/ganglia 目录的权限</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[leinuo@hadoop102 flume]$ sudo chmod -R 777 /var/lib/ganglia</span><br></pre></td></tr></table></figure>

<h2 id="操作-Flume-测试监控"><a href="#操作-Flume-测试监控" class="headerlink" title="操作 Flume 测试监控"></a>操作 Flume 测试监控</h2><p>1) 修改/opt/module/flume/conf 目录的 下的 flume-env.sh 配置：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">JAVA_OPTS="-Dflume.monitoring.type=ganglia</span><br><span class="line">-Dflume.monitoring.hosts=192.168.25.102:8649</span><br><span class="line">-Xms100m</span><br><span class="line">-Xmx200m"</span><br></pre></td></tr></table></figure>

<p>2) 启动 Flume 任务</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[leinuo@hadoop102 flume]$ bin/flume-ng agent \</span><br><span class="line">--conf conf/ \</span><br><span class="line">--name a1 \</span><br><span class="line">--conf-file job/flume-telnet-logger.conf \</span><br><span class="line">-Dflume.root.logger==INFO,console \</span><br><span class="line">-Dflume.monitoring.type=ganglia \</span><br><span class="line">-Dflume.monitoring.hosts=192.168.25.102:8649</span><br></pre></td></tr></table></figure>

<p>3) 发送数据观察 ganglia 监测图</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[leinuo@hadoop102 flume]$ telnet localhost 44444</span><br></pre></td></tr></table></figure>

<p>样式如图:</p>
<p><img src="https://leinuo-blog.oss-cn-beijing.aliyuncs.com/images/typora/1590221698405.png" alt="1590221698405"></p>
<p>图例说明：</p>
<p><img src="https://leinuo-blog.oss-cn-beijing.aliyuncs.com/images/typora/1590221737003.png" alt="1590221737003"></p>
]]></content>
      <categories>
        <category>大数据</category>
        <category>大数据项目部署</category>
        <category>Flume</category>
        <category>Flume监控之Ganglia</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>Flume</tag>
      </tags>
  </entry>
  <entry>
    <title>Hadoop-lzo压缩配置</title>
    <url>/2019/12/06/Hadoop-lzo%E5%8E%8B%E7%BC%A9%E9%85%8D%E7%BD%AE/</url>
    <content><![CDATA[<h2 id="LZO-压缩配置"><a href="#LZO-压缩配置" class="headerlink" title="LZO 压缩配置"></a>LZO 压缩配置</h2><h3 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h3><p>a. LZO本身是不支持分片的，但是我们给LZO压缩的文件加上索引，就支持分片了</p>
<p>b. Linux本身是不支持LZO压缩的，所以我们需要下载安装软件包，其中包括三个：lzo,lzop,hdoop-gpl-packaging.</p>
<p>c. hdoop-gpl-packaging的主要作用就是给压缩的LZO文件创建索引，否则LZO是不支持分片的，无论文件有多大，都只能有一个map</p>
<p>snappy 速度要比LZO速度快，但不支持切片</p>
<p>1）hadoop 本身并不支持 lzo 压缩，故需要使用 twitter 提供的 hadoop-lzo 开源组件。hadoop-lzo<br>需依赖 hadoop 和 lzo 进行编译，编译步骤如下。</p>
<h2 id="集群每一个节点都要安装lzo-本地lib包！"><a href="#集群每一个节点都要安装lzo-本地lib包！" class="headerlink" title="集群每一个节点都要安装lzo 本地lib包！"></a><strong>集群每一个节点都要安装lzo 本地lib包！</strong></h2><h3 id="安装lzop-native-library"><a href="#安装lzop-native-library" class="headerlink" title="安装lzop native library"></a>安装lzop native library</h3><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">yum -y install gcc-c++ lzo-devel zlib-devel autoconf automake libtool</span><br><span class="line"><span class="meta">#</span><span class="bash"> 下载安装并编译LZO</span></span><br><span class="line">wget www.oberhumer.com/opensource/lzo/download/lzo-2.10.tar.gz</span><br><span class="line">tar -zxvf lzo-2.10.tar.gz -C /opt/module/</span><br><span class="line">cd lzo-2.10</span><br><span class="line">export CFLAGS=-m64</span><br><span class="line">mkdir -p /usr/local/hadoop/lzo/ </span><br><span class="line">./configure prefix=/usr/local/hadoop/lzo/  #如果你想指定具体的安装目录的话，可以添加参数–prefix==your_path</span><br><span class="line">make &amp;&amp; make install  #这个过程要注意权限问题，所以我建议你还是使用root用户来操作</span><br></pre></td></tr></table></figure>

<h3 id="安装maven"><a href="#安装maven" class="headerlink" title="安装maven"></a>安装maven</h3><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">cd /opt/software</span><br><span class="line">wget http://archive.apache.org/dist/maven/maven-3/3.5.2/binaries/apache-maven-3.5.2-bin.tar.gz</span><br><span class="line">tar -zxvf apache-maven-3.5.2-bin.tar.gz -C /opt/module/</span><br></pre></td></tr></table></figure>

<p>将maven配置到环境变量当中去</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">vim /etc/profile</span><br><span class="line"><span class="meta">#</span><span class="bash">MAVEN_HOME</span></span><br><span class="line">export MAVEN_HOME=/opt/module/apache-maven-3.5.2</span><br><span class="line">export PATH=$PATH:$MAVEN_HOME/bin</span><br><span class="line">source /etc/profile</span><br><span class="line">mvn -v  # 有信息则说明安装正确</span><br></pre></td></tr></table></figure>

<h3 id="安装hadoop-lzo"><a href="#安装hadoop-lzo" class="headerlink" title="安装hadoop-lzo"></a>安装hadoop-lzo</h3><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">wget https://github.com/twitter/hadoop-lzo/archive/master.zip</span><br><span class="line">unzip master </span><br><span class="line">mv hadoop-lzo-master /opt/module/</span><br><span class="line">vim hadoop-lzo-master/pom.xml    #修改hadoop版本号</span><br><span class="line">cd hadoop-lzo-master/</span><br><span class="line">export CFLAGS=-m64</span><br><span class="line">export CXXFLAGS=-m64</span><br><span class="line">export C_INCLUDE_PATH=/usr/local/hadoop/lzo/include #这里是指定你lzo-2.06的地址里面的include目录，这就是为什么我们要安装配置lzo-2.10的原因了</span><br><span class="line">export LIBRARY_PATH=/usr/local/hadoop/lzo/lib #同上</span><br><span class="line">mvn clean package -Dmaven.test.skip=true #使用maven编译</span><br></pre></td></tr></table></figure>

<p>2）将编译好后的 hadoop-lzo-0.4.20.jar 放入 hadoop-2.7.2/share/hadoop/common/</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">cd target/native/Linux-amd64-64</span><br><span class="line">tar -cBf - -C lib . | tar -xBvf - -C ~</span><br><span class="line">cp ~/libgplcompression* $HADOOP_HOME/lib/native/ #将这个包里面的所有东西都cp到hadoop目录下的lib下的native里面，最好你是编译过的hadoop，否则你的native里面是空的</span><br><span class="line">cp /opt/module/hadoop-lzo-master/target/hadoop-lzo-0.4.21-SNAPSHOT.jar $HADOOP_HOME/share/hadoop/common/ #这个非常重要，你能否正常使用lzo就看这个了</span><br><span class="line">ls -l /root/libgplcompression.*</span><br></pre></td></tr></table></figure>

<p>3）同步 hadoop-lzo-0.4.20.jar 到 hadoop103、hadoop104</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[leinuo@hadoop102 ~]$ cd $HADOOP_HOME/share/hadoop/common/</span><br><span class="line">[leinuo@hadoop102 common]$ xsync hadoop-lzo-0.4.21-SNAPSHOT.jar</span><br></pre></td></tr></table></figure>

<p>4） 配置hadoop-env.sh </p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">vim $HADOOP_HOME/etc/hadoop/hadoop-env.sh </span><br><span class="line"><span class="meta">#</span><span class="bash"> 添加下面内容</span></span><br><span class="line">export LD_LIBRARY_PATH=/usr/local/hadoop/lzo/lib</span><br></pre></td></tr></table></figure>

<p>  配置 core-site.xml </p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">vim $HADOOP_HOME/etc/hadoop/core-site.xml</span><br></pre></td></tr></table></figure>

<p>加入如下配置</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>io.compression.codecs<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span></span><br><span class="line">org.apache.hadoop.io.compress.GzipCodec,</span><br><span class="line">org.apache.hadoop.io.compress.DefaultCodec,</span><br><span class="line">org.apache.hadoop.io.compress.BZip2Codec,</span><br><span class="line">org.apache.hadoop.io.compress.SnappyCodec,</span><br><span class="line">com.hadoop.compression.lzo.LzoCodec,</span><br><span class="line">com.hadoop.compression.lzo.LzopCodec</span><br><span class="line"><span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>io.compression.codec.lzo.class<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>com.hadoop.compression.lzo.LzoCodec<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p>配置 mapred-site.xml </p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">vim $HADOOP_HOME/etc/hadoop/mapred-site.xml</span><br></pre></td></tr></table></figure>

<p>添加如下配置</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="comment">&lt;!-- lzo 压缩 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>mapred.child.env<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>LD_LIBRARY_PATH=/usr/local/hadoop/lzo/lib<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.map.output.compress<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.map.output.compress.codec<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>com.hadoop.compression.lzo.LzoCodec<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p>5）同步到 hadoop103、hadoop104</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">cd $HADOOP_HOME/etc/hadoop/</span><br><span class="line"></span><br><span class="line">xsync hadoop-env.sh</span><br><span class="line">xsync core-site.xml</span><br><span class="line">xsync mapred-site.xml</span><br></pre></td></tr></table></figure>

<p>6）启动及查看集群</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[leinuo@hadoop102 hadoop-2.7.2]$ sbin/start-dfs.sh</span><br><span class="line">[leinuo@hadoop103 hadoop-2.7.2]$ sbin/start-yarn.sh</span><br></pre></td></tr></table></figure>

<h2 id="LZO-创建索引"><a href="#LZO-创建索引" class="headerlink" title="LZO 创建索引"></a>LZO 创建索引</h2><p>1）创建 LZO 文件的索引，LZO 压缩文件的可切片特性依赖于其索引，故我们需要手动为LZO 压缩文件创建索引。若无索引，则 LZO 文件的切片只有一个。</p>
<p>2）测试<br>（1）将 bigtable.lzo（150M）上传到集群的根目录</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">hdfs dfs -mkdir /input</span><br><span class="line">hdfs dfs -put bigtable.lzo /input</span><br><span class="line">hdfs dfs -text /input/bigtable.lzo</span><br></pre></td></tr></table></figure>

<p>（2）执行 wordcount 程序</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">hadoop jar $HADOOP_HOME/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.6.jar wordcount /input /output1</span><br><span class="line">--------------------------------------</span><br><span class="line">20/05/21 20:11:20 INFO mapreduce.JobSubmitter: number of splits:1</span><br></pre></td></tr></table></figure>

<p>（3）对上传的 LZO 文件建索引</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">hadoop jar $HADOOP_HOME/share/hadoop/common/hadoop-lzo-0.4.21-SNAPSHOT.jar com.hadoop.compression.lzo.DistributedLzoIndexer /input/bigtable.lzo</span><br></pre></td></tr></table></figure>

<p>（4）再次执行 WordCount 程序</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 不带参数</span></span><br><span class="line">hadoop jar $HADOOP_HOME/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.6.jar wordcount  /input /output2</span><br><span class="line">-------------------------------------------------</span><br><span class="line">20/05/21 20:14:18 INFO mapreduce.JobSubmitter: number of splits:2</span><br><span class="line">程序结束用时1分39秒</span><br></pre></td></tr></table></figure>

]]></content>
      <categories>
        <category>大数据</category>
        <category>大数据项目经验</category>
        <category>LZO</category>
        <category>Hadoop-lzo压缩配置</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>LZO</tag>
      </tags>
  </entry>
  <entry>
    <title>HDFS基准测试&amp;参数调优</title>
    <url>/2019/11/23/HDFS%E5%9F%BA%E5%87%86%E6%B5%8B%E8%AF%95%E4%B8%8E%E5%8F%82%E6%95%B0%E8%B0%83%E4%BC%98/</url>
    <content><![CDATA[<h3 id="基准测试"><a href="#基准测试" class="headerlink" title="基准测试"></a>基准测试</h3><p>1） 测试 HDFS 写性能<br>测试内容：向 HDFS 集群写 10 个 128M 的文件</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">hadoop jar /opt/module/hadoop-2.7.6/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.6-tests.jar TestDFSIO -write -nrFiles 10 -fileSize 128MB</span><br></pre></td></tr></table></figure>

<p><img src="https://leinuo-blog.oss-cn-beijing.aliyuncs.com/images/typora/1590064620016.png" alt="1590064620016"></p>
<p>2）测试 HDFS 读性能<br>测试内容：读取 HDFS 集群 10 个 128M 的文件</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">hadoop jar /opt/module/hadoop-2.7.6/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.6-tests.jar TestDFSIO -read -nrFiles 10 -fileSize 128MB</span><br></pre></td></tr></table></figure>

<p><img src="https://leinuo-blog.oss-cn-beijing.aliyuncs.com/images/typora/1590064603377.png" alt="1590064603377"></p>
<p>可以用top命令查看CUP利用率</p>
<p>3）删除测试生成数据</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">hadoop jar /opt/module/hadoop-2.7.6/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.6-tests.jar TestDFSIO -clean</span><br></pre></td></tr></table></figure>

<p>4）使用 Sort 程序评测 MapReduce（自己玩的话慎用！容易把机子搞崩）</p>
<p>（1）使用 RandomWriter 来产生随机数，每个节点运行 10 个 Map 任务，每个 Map 产生大约 1G 大小的二进制随机数</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">hadoop jar /opt/module/hadoop-2.7.6/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.6.jar randomwriter random-data</span><br></pre></td></tr></table></figure>

<p>（2）执行 Sort 程序</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">hadoop jar /opt/module/hadoop-2.7.6/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.6.jar sort random-data  sorted-data</span><br></pre></td></tr></table></figure>

<p>（3）验证数据是否真正排好序了</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">hadoop jar /opt/module/hadoop-2.7.6/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.6.jar testmapredsort -sortInput random-data -sortOutput sorted-data</span><br></pre></td></tr></table></figure>

<h3 id="Hadoop-参数-调优"><a href="#Hadoop-参数-调优" class="headerlink" title="Hadoop 参数 调优"></a>Hadoop 参数 调优</h3><p>1）HDFS 参数调优 <strong>hdfs-site.xml</strong><br><strong>dfs.namenode.handler.count=20 * log2(Cluster Size)</strong>，比如集群规模为 8 台时，此参数设置为 60 NameNode 有一个工作线程池，用来处理不同 DataNode 的并发心跳以及客户端并发的元数据操作。对于大集群或者有大量客户端的集群来说，通常需要增大参数dfs.namenode.handler.count 的默认值 10。设置该值的一般原则是将其设置为集群大小的自然对数乘以 20，即 20logN，N 为集群大小。</p>
<p>2）YARN 参数调优 <strong>yarn-site.xml</strong><br>（1）情景描述：总共 7 台机器，每天几亿条数据，数据源-&gt;Flume-&gt;Kafka-&gt;HDFS-&gt;Hive 面临问题：数据统计主要用 HiveSQL，没有数据倾斜，小文件已经做了合并处理，开启了 JVM 重用，而且 IO 没有阻塞，内存用了不到 50%。但是还是跑的非常慢，而且数据量洪峰过来时，整个集群都会宕掉。基于这种情况有没有优化方案。<br>（2）解决办法：<br>内存利用率不够。这个一般是 Yarn 的 2 个配置造成的，单个任务可以申请的最大内存大小，和Hadoop 单个节点可用内存大小。调节这两个参数能提高系统内存的利用率。<br>（a）<strong>yarn.nodemanager.resource.memory-mb</strong><br>表示<strong>该节点</strong>上 YARN <strong>可使用</strong>的<strong>物理内存总量</strong>，默认是 8192（MB），注意，如果你的节点内存资源不够 8GB，则需要调减小这个值，而 YARN 不会智能的探测节点的物理内存总量。<br>（b）<strong>yarn.scheduler.maximum-allocation-mb</strong><br>单个任务可申请的<strong>最多物理内存量</strong>，默认是 8192（MB）。<br>3）Hadoop 宕机<br>（1）如果 MR 造成系统宕机。此时要<strong>控制 Yarn 同时运行的任务数</strong>，和每个任务申请的最大内存。调整参数：yarn.scheduler.maximum-allocation-mb（单个任务可申请的最多物理内存量，默认是 8192MB）<br>（2）如果<strong>写入文件过量</strong>造成 NameNode 宕机。那么<strong>调高 Kafka 的存储大小</strong>，<strong>控制从 Kafka到 HDFS 的写入速度</strong>。高峰期的时候用 Kafka 进行缓存，高峰期过去数据同步会自动跟上。</p>
]]></content>
      <categories>
        <category>大数据</category>
        <category>大数据项目经验</category>
        <category>HDFS</category>
        <category>HDFS基准测试&amp;参数调优</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>HDFS</tag>
      </tags>
  </entry>
  <entry>
    <title>Hive SQL语法总结</title>
    <url>/2019/08/26/HiveSQL%E8%AF%AD%E6%B3%95%E6%80%BB%E7%BB%93/</url>
    <content><![CDATA[<h1 id="Hive-SQL语法总结"><a href="#Hive-SQL语法总结" class="headerlink" title="Hive SQL语法总结"></a>Hive SQL语法总结</h1><p>Hive是一个数据仓库基础的应用工具，在Hadoop中用来处理结构化数据.</p>
<p>Hive 查询操作过程严格遵守Hadoop MapReduce 的作业执行模型，Hive 将用户的Hive SQL 语句通过解释器转换为MapReduce 作业提交到Hadoop 集群上，Hadoop 监控作业执行过程，然后返回作业执行结果给用户。Hive 并非为联机事务处理而设计，Hive 并不提供实时的查询和基于行级的数据更新操作。Hive 的最佳使用场合是<strong>大数据集的批处理作业</strong>。 </p>
<h2 id="创建数据库"><a href="#创建数据库" class="headerlink" title="创建数据库"></a>创建数据库</h2><p> <strong>CREATE</strong> DATABASE <strong>name</strong>; </p>
<p> 显示查看操作命令 </p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">show</span> <span class="keyword">tables</span>; <span class="comment">--显示表</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">show</span> <span class="keyword">databases</span>; <span class="comment">--显示数据库</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">show</span> <span class="keyword">partitions</span> table_name; <span class="comment">--显示表名为table_name的表的所有分区</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">show</span> functions ; <span class="comment">--显示所有函数</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">describe</span> <span class="keyword">extended</span> table_name col_name; <span class="comment">--查看表中字段</span></span><br></pre></td></tr></table></figure>

<h2 id="DDL-Data-Defination-Language"><a href="#DDL-Data-Defination-Language" class="headerlink" title="DDL(Data Defination Language)"></a>DDL(Data Defination Language)</h2><p> 数据库定义语言 </p>
<p> 创建表结构 </p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> [<span class="keyword">EXTERNAL</span>] <span class="keyword">TABLE</span> [<span class="keyword">IF</span> <span class="keyword">NOT</span> <span class="keyword">EXISTS</span>] table_name</span><br><span class="line"></span><br><span class="line">[(col_name data_type [<span class="keyword">COMMENT</span> col_comment], ...)]</span><br><span class="line"></span><br><span class="line">[<span class="keyword">COMMENT</span> table_comment]</span><br><span class="line"></span><br><span class="line">[PARTITIONED <span class="keyword">BY</span> (col_name data_type [<span class="keyword">COMMENT</span> col_comment], ...)]</span><br><span class="line"></span><br><span class="line">[CLUSTERED <span class="keyword">BY</span> (col_name, col_name, ...)</span><br><span class="line"></span><br><span class="line">[SORTED <span class="keyword">BY</span> (col_name [<span class="keyword">ASC</span>|<span class="keyword">DESC</span>], ...)] <span class="keyword">INTO</span> num_buckets BUCKETS]</span><br><span class="line"></span><br><span class="line">[<span class="keyword">ROW</span> <span class="keyword">FORMAT</span> row_format]</span><br><span class="line"></span><br><span class="line">[<span class="keyword">STORED</span> <span class="keyword">AS</span> file_format]  <span class="comment">-- 默认文件存储格式有textfile、sequencefile、rcfile等</span></span><br><span class="line"></span><br><span class="line">[LOCATION hdfs_path]     <span class="comment">-- hdfs 路径</span></span><br></pre></td></tr></table></figure>

<ul>
<li>CREATE TABLE 创建一个指定名字的表。如果相同名字的表已经存在，则抛出异常；用户可以用 IF NOT EXIST 选项来忽略这个异常</li>
<li>EXTERNAL 关键字可以让用户创建一个外部表，在建表的同时指定一个指向实际数据的路径（LOCATION）</li>
<li>LIKE 允许用户复制现有的表结构，但是不复制数据</li>
<li>COMMENT可以为表与字段增加描述</li>
<li>ROW FORMAT 设置行数据分割格式</li>
<li>虽然单独rcfile的列运算不一定总是存在的，但是rcfile的高压缩率确实减少文件大小，因此实际应用中，<strong>rcfile总是成为不二的选择，达观数据平台在选择文件存储格式时也大量选择了rcfile方案。</strong> </li>
</ul>
<h4 id="STORED-AS"><a href="#STORED-AS" class="headerlink" title="STORED AS"></a>STORED AS</h4><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">SEQUENCEFILE</span><br><span class="line"></span><br><span class="line">| TEXTFILE</span><br><span class="line"></span><br><span class="line">| RCFILE</span><br><span class="line"></span><br><span class="line">| INPUTFORMAT input_format_classname OUTPUTFORMAT</span><br><span class="line"></span><br><span class="line">output_format_classname</span><br></pre></td></tr></table></figure>

<p>如果文件数据是纯文本，可以使用 STORED <strong>AS</strong> TEXTFILE。</p>
<p>如果数据需要压缩，使用 STORED <strong>AS SEQUENCE</strong> 。</p>
<p> 创建简单表： </p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> person(<span class="keyword">name</span> <span class="keyword">STRING</span>,age <span class="built_in">INT</span>);</span><br></pre></td></tr></table></figure>

<p> 创建外部表: </p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">EXTERNAL</span> <span class="keyword">TABLE</span> page_view(viewTime <span class="built_in">INT</span>, userid <span class="built_in">BIGINT</span>,</span><br><span class="line"></span><br><span class="line">page_url <span class="keyword">STRING</span>, referrer_url <span class="keyword">STRING</span>,</span><br><span class="line"></span><br><span class="line">ip <span class="keyword">STRING</span> <span class="keyword">COMMENT</span> <span class="string">'IP Address of the User'</span>,</span><br><span class="line"></span><br><span class="line">country <span class="keyword">STRING</span> <span class="keyword">COMMENT</span> <span class="string">'country of origination'</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">COMMENT</span> <span class="string">'这里写表的描述信息'</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">ROW</span> <span class="keyword">FORMAT</span> <span class="keyword">DELIMITED</span> <span class="keyword">FIELDS</span> <span class="keyword">TERMINATED</span> <span class="keyword">BY</span> <span class="string">'\054'</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">STORED</span> <span class="keyword">AS</span> TEXTFILE</span><br><span class="line"></span><br><span class="line">LOCATION <span class="string">'&lt;hdfs_location&gt;'</span>;</span><br></pre></td></tr></table></figure>

<p> 创建分区表： </p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> par_table(viewTime <span class="built_in">INT</span>, userid <span class="built_in">BIGINT</span>,</span><br><span class="line"></span><br><span class="line">page_url <span class="keyword">STRING</span>, referrer_url <span class="keyword">STRING</span>,</span><br><span class="line"></span><br><span class="line">ip <span class="keyword">STRING</span> <span class="keyword">COMMENT</span> <span class="string">'IP Address of the User'</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">COMMENT</span> <span class="string">'This is the page view table'</span></span><br><span class="line"></span><br><span class="line">PARTITIONED <span class="keyword">BY</span>(<span class="built_in">date</span> <span class="keyword">STRING</span>, pos <span class="keyword">STRING</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">ROW</span> <span class="keyword">FORMAT</span> <span class="keyword">DELIMITED</span> <span class="string">'\t'</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">FIELDS</span> <span class="keyword">TERMINATED</span> <span class="keyword">BY</span> <span class="string">'\n'</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">STORED</span> <span class="keyword">AS</span> SEQUENCEFILE;</span><br></pre></td></tr></table></figure>

<p> 创建分桶表： </p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> par_table(viewTime <span class="built_in">INT</span>, userid <span class="built_in">BIGINT</span>,</span><br><span class="line"></span><br><span class="line">page_url <span class="keyword">STRING</span>, referrer_url <span class="keyword">STRING</span>,</span><br><span class="line"></span><br><span class="line">ip <span class="keyword">STRING</span> <span class="keyword">COMMENT</span> <span class="string">'IP Address of the User'</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">COMMENT</span> <span class="string">'This is the page view table'</span></span><br><span class="line"></span><br><span class="line">PARTITIONED <span class="keyword">BY</span>(<span class="built_in">date</span> <span class="keyword">STRING</span>, pos <span class="keyword">STRING</span>)</span><br><span class="line"></span><br><span class="line">CLUSTERED <span class="keyword">BY</span>(userid) SORTED <span class="keyword">BY</span>(viewTime) <span class="keyword">INTO</span> <span class="number">32</span> BUCKETS</span><br><span class="line"></span><br><span class="line"><span class="keyword">ROW</span> <span class="keyword">FORMAT</span> <span class="keyword">DELIMITED</span> <span class="string">'\t'</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">FIELDS</span> <span class="keyword">TERMINATED</span> <span class="keyword">BY</span> <span class="string">'\n'</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">STORED</span> <span class="keyword">AS</span> SEQUENCEFILE;</span><br></pre></td></tr></table></figure>

<p> 创建带索引字段的表： </p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> invites (foo <span class="built_in">INT</span>, bar <span class="keyword">STRING</span>) PARTITIONED <span class="keyword">BY</span> (dindex <span class="keyword">STRING</span>);</span><br></pre></td></tr></table></figure>



<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="comment">--显示所有表： </span></span><br><span class="line"><span class="keyword">SHOW</span> <span class="keyword">TABLES</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">--按正则表达式显示表：</span></span><br><span class="line"><span class="keyword">SHOW</span> <span class="keyword">TABLES</span> <span class="string">'.*s'</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">--表中添加一个字段：</span></span><br><span class="line"><span class="keyword">ALTER</span> <span class="keyword">TABLE</span> pokes <span class="keyword">ADD</span> <span class="keyword">COLUMNS</span> (new_col <span class="built_in">INT</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">--添加一个字段并为其添加注释：</span></span><br><span class="line"><span class="keyword">ALTER</span> <span class="keyword">TABLE</span> invites <span class="keyword">ADD</span> <span class="keyword">COLUMNS</span> (new_col2 <span class="built_in">INT</span> <span class="keyword">COMMENT</span> <span class="string">'a comment'</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 删除列：</span></span><br><span class="line"><span class="keyword">ALTER</span> <span class="keyword">TABLE</span> <span class="keyword">test</span> <span class="keyword">REPLACE</span> <span class="keyword">COLUMNS</span>(<span class="keyword">id</span> <span class="built_in">BIGINT</span>, <span class="keyword">name</span> <span class="keyword">STRING</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">--更改表名：</span></span><br><span class="line"><span class="keyword">ALTER</span> <span class="keyword">TABLE</span> <span class="keyword">events</span> <span class="keyword">RENAME</span> <span class="keyword">TO</span> new_events;</span><br><span class="line"></span><br><span class="line"><span class="comment">--增加、删除分区</span></span><br><span class="line"></span><br><span class="line"><span class="comment">--增加：</span></span><br><span class="line"><span class="keyword">ALTER</span> <span class="keyword">TABLE</span> table_name <span class="keyword">ADD</span> [<span class="keyword">IF</span> <span class="keyword">NOT</span> <span class="keyword">EXISTS</span>] partition_spec [ LOCATION <span class="string">'location1'</span> ] partition_spec [ LOCATION <span class="string">'location2'</span> ] ...</span><br><span class="line"></span><br><span class="line">partition_spec:</span><br><span class="line"></span><br><span class="line">: <span class="keyword">PARTITION</span> (partition_col = partition_col_value, partition_col = partiton_col_value, ...)</span><br><span class="line"></span><br><span class="line">   </span><br><span class="line"><span class="comment">--删除：</span></span><br><span class="line"><span class="keyword">ALTER</span> <span class="keyword">TABLE</span> table_name <span class="keyword">DROP</span> partition_spec, partition_spec,...</span><br><span class="line"></span><br><span class="line">   </span><br><span class="line"></span><br><span class="line"><span class="comment">--改变表的文件格式与组织：</span></span><br><span class="line"><span class="keyword">ALTER</span> <span class="keyword">TABLE</span> table_name <span class="keyword">SET</span> FILEFORMAT file_format</span><br><span class="line"><span class="keyword">ALTER</span> <span class="keyword">TABLE</span> table_name CLUSTERED <span class="keyword">BY</span>(userid) SORTED <span class="keyword">BY</span>(viewTime) <span class="keyword">INTO</span> num_buckets BUCKETS <span class="comment">--这个命令修改了表的物理存储属性</span></span><br><span class="line"></span><br><span class="line">   </span><br><span class="line"></span><br><span class="line"><span class="comment">--创建和删除视图：</span></span><br><span class="line"></span><br><span class="line"><span class="comment">--创建视图：</span></span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">VIEW</span> [<span class="keyword">IF</span> <span class="keyword">NOT</span> <span class="keyword">EXISTS</span>] view_name [ (column_name [<span class="keyword">COMMENT</span> column_comment], ...) ][<span class="keyword">COMMENT</span> view_comment][TBLPROPERTIES (property_name = property_value, ...)] <span class="keyword">AS</span> <span class="keyword">SELECT</span>；</span><br><span class="line"></span><br><span class="line"><span class="comment">--删除视图：</span></span><br><span class="line"><span class="keyword">DROP</span> <span class="keyword">VIEW</span> view_name；</span><br></pre></td></tr></table></figure>

<h2 id="DML（Data-manipulation-language）"><a href="#DML（Data-manipulation-language）" class="headerlink" title="DML（Data manipulation language）"></a>DML（Data manipulation language）</h2><p> 数据操作语言，主要是数据库增删改三种操作，DML包括：INSERT插入、UPDATE新、DELETE删除。 </p>
<p> 向数据表内加载文件： </p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">LOAD</span> <span class="keyword">DATA</span> [<span class="keyword">LOCAL</span>] INPATH <span class="string">'filepath'</span> [OVERWRITE] <span class="keyword">INTO</span> <span class="keyword">TABLE</span> tablename [<span class="keyword">PARTITION</span> (partcol1=val1, partcol2=val2 ...)]</span><br><span class="line"></span><br><span class="line"><span class="comment">--load操作只是单纯的复制/移动操作，将数据文件移动到Hive表对应的位置。</span></span><br><span class="line"></span><br><span class="line"><span class="comment">--加载本地</span></span><br><span class="line"><span class="keyword">LOAD</span> <span class="keyword">DATA</span> <span class="keyword">LOCAL</span> INPATH <span class="string">'./examples/files/kv1.txt'</span> OVERWRITE <span class="keyword">INTO</span> <span class="keyword">TABLE</span> pokes;</span><br><span class="line"></span><br><span class="line">   </span><br><span class="line"><span class="comment">--加载HDFS数据，同时给定分区信息</span></span><br><span class="line"><span class="keyword">LOAD</span> <span class="keyword">DATA</span> INPATH <span class="string">'/user/myname/kv2.txt'</span> OVERWRITE <span class="keyword">INTO</span> <span class="keyword">TABLE</span> invites <span class="keyword">PARTITION</span> (ds=<span class="string">'2008-08-15'</span>);</span><br></pre></td></tr></table></figure>

<p> 将查询结果插入到Hive表： </p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">INSERT</span> OVERWRITE <span class="keyword">TABLE</span> tablename1 [<span class="keyword">PARTITION</span> (partcol1=val1, partcol2=val2 ...)] select_statement1 <span class="keyword">FROM</span> from_statement;</span><br><span class="line"></span><br><span class="line">   </span><br><span class="line"></span><br><span class="line"><span class="comment">--多插入模式：</span></span><br><span class="line">FROM from_statement</span><br><span class="line"></span><br><span class="line"><span class="keyword">INSERT</span> OVERWRITE <span class="keyword">TABLE</span> tablename1 [<span class="keyword">PARTITION</span> (partcol1=val1, partcol2=val2 ...)] select_statement1</span><br><span class="line"></span><br><span class="line">[<span class="keyword">INSERT</span> OVERWRITE <span class="keyword">TABLE</span> tablename2 [<span class="keyword">PARTITION</span> ...] select_statement2] ...</span><br><span class="line"></span><br><span class="line">   </span><br><span class="line"></span><br><span class="line"><span class="comment">--自动分区模式</span></span><br><span class="line"><span class="keyword">INSERT</span> OVERWRITE <span class="keyword">TABLE</span> tablename <span class="keyword">PARTITION</span> (partcol1[=val1], partcol2[=val2] ...) select_statement <span class="keyword">FROM</span> from_statement;</span><br></pre></td></tr></table></figure>

<p> 将查询结果插入到HDFS文件系统中： </p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">INSERT</span> OVERWRITE [<span class="keyword">LOCAL</span>] <span class="keyword">DIRECTORY</span> directory1 <span class="keyword">SELECT</span> ... <span class="keyword">FROM</span> ...</span><br><span class="line"></span><br><span class="line"><span class="keyword">FROM</span> from_statement</span><br><span class="line"></span><br><span class="line"><span class="keyword">INSERT</span> OVERWRITE [<span class="keyword">LOCAL</span>] <span class="keyword">DIRECTORY</span> directory1 select_statement1</span><br><span class="line"></span><br><span class="line">[<span class="keyword">INSERT</span> OVERWRITE [<span class="keyword">LOCAL</span>] <span class="keyword">DIRECTORY</span> directory2 select_statement2]</span><br></pre></td></tr></table></figure>

<p> INSERT INTO </p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> <span class="keyword">TABLE</span> tablename1 [<span class="keyword">PARTITION</span> (partcol1=val1, partcol2=val2 ...)] select_statement1 <span class="keyword">FROM</span> from_statement;</span><br></pre></td></tr></table></figure>

<p>insert overwrite和insert into的区别：</p>
<ul>
<li>insert overwrite 会覆盖已经存在的数据，假如原始表使用overwrite 上述的数据，先现将原始表的数据remove，再插入新数据。</li>
<li>insert into 只是简单的插入，不考虑原始表的数据，直接追加到表中。最后表的数据是原始数据和新插入数据。</li>
</ul>
<h2 id="DQL（data-query-language）数据查询语言-select操作"><a href="#DQL（data-query-language）数据查询语言-select操作" class="headerlink" title="DQL（data query language）数据查询语言 select操作"></a>DQL（data query language）数据查询语言 select操作</h2><p> SELECT查询结构： </p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> [<span class="keyword">ALL</span> | <span class="keyword">DISTINCT</span>] select_expr, select_expr, ...</span><br><span class="line"></span><br><span class="line"><span class="keyword">FROM</span> table_reference</span><br><span class="line"></span><br><span class="line">[<span class="keyword">WHERE</span> where_condition]</span><br><span class="line"></span><br><span class="line">[<span class="keyword">GROUP</span> <span class="keyword">BY</span> col_list [<span class="keyword">HAVING</span> condition]]</span><br><span class="line"></span><br><span class="line">[ CLUSTER <span class="keyword">BY</span> col_list</span><br><span class="line"></span><br><span class="line">| [<span class="keyword">DISTRIBUTE</span> <span class="keyword">BY</span> col_list] [<span class="keyword">SORT</span> <span class="keyword">BY</span>| <span class="keyword">ORDER</span> <span class="keyword">BY</span> col_list]</span><br><span class="line"></span><br><span class="line">]</span><br><span class="line"></span><br><span class="line">[<span class="keyword">LIMIT</span> <span class="built_in">number</span>]</span><br></pre></td></tr></table></figure>

<ul>
<li>使用ALL和DISTINCT选项区分对重复记录的处理。默认是ALL，表示查询所有记录DISTINCT表示去掉重复的记录</li>
<li>Where 条件 类似我们传统SQL的where 条件</li>
<li>ORDER BY 全局排序，只有一个Reduce任务</li>
<li>SORT BY 只在本机做排序</li>
<li>LIMIT限制输出的个数和输出起始位置</li>
</ul>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="comment">--将查询数据输出至目录： </span></span><br><span class="line"><span class="keyword">INSERT</span> OVERWRITE <span class="keyword">DIRECTORY</span> <span class="string">'/tmp/hdfs_out'</span> <span class="keyword">SELECT</span> a.* <span class="keyword">FROM</span> invites a <span class="keyword">WHERE</span> a.ds=<span class="string">'&lt;DATE&gt;'</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">--将查询结果输出至本地目录：</span></span><br><span class="line"><span class="keyword">INSERT</span> OVERWRITE <span class="keyword">LOCAL</span> <span class="keyword">DIRECTORY</span> <span class="string">'/tmp/local_out'</span> <span class="keyword">SELECT</span> a.* <span class="keyword">FROM</span> pokes a;</span><br></pre></td></tr></table></figure>

<p> 将一个表的结果插入到另一个表： </p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">FROM invites a <span class="keyword">INSERT</span> OVERWRITE <span class="keyword">TABLE</span> <span class="keyword">events</span> <span class="keyword">SELECT</span> a.bar, <span class="keyword">count</span>(<span class="number">1</span>) <span class="keyword">WHERE</span> a.foo &gt; <span class="number">0</span> <span class="keyword">GROUP</span> <span class="keyword">BY</span> a.bar;</span><br><span class="line"></span><br><span class="line"><span class="keyword">INSERT</span> OVERWRITE <span class="keyword">TABLE</span> <span class="keyword">events</span> <span class="keyword">SELECT</span> a.bar, <span class="keyword">count</span>(<span class="number">1</span>) <span class="keyword">FROM</span> invites a <span class="keyword">WHERE</span> a.foo &gt; <span class="number">0</span> <span class="keyword">GROUP</span> <span class="keyword">BY</span> a.bar;</span><br><span class="line"></span><br><span class="line">JOIN</span><br><span class="line"></span><br><span class="line">FROM pokes t1 JOIN invites t2 ON (t1.bar = t2.bar) <span class="keyword">INSERT</span> OVERWRITE <span class="keyword">TABLE</span> <span class="keyword">events</span> <span class="keyword">SELECT</span> t1.bar, t1.foo, t2.foo;</span><br></pre></td></tr></table></figure>

<p> 将多表数据插入到同一表中 </p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">FROM src</span><br><span class="line"></span><br><span class="line"><span class="keyword">INSERT</span> OVERWRITE <span class="keyword">TABLE</span> dest1 <span class="keyword">SELECT</span> src.* <span class="keyword">WHERE</span> src.key &lt; <span class="number">100</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">INSERT</span> OVERWRITE <span class="keyword">TABLE</span> dest2 <span class="keyword">SELECT</span> src.key, src.value <span class="keyword">WHERE</span> src.key &gt;= <span class="number">100</span> <span class="keyword">and</span> src.key &lt; <span class="number">200</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">INSERT</span> OVERWRITE <span class="keyword">TABLE</span> dest3 <span class="keyword">PARTITION</span>(ds=<span class="string">'2008-04-08'</span>, hr=<span class="string">'12'</span>) <span class="keyword">SELECT</span> src.key <span class="keyword">WHERE</span> src.key &gt;= <span class="number">200</span> <span class="keyword">and</span> src.key &lt; <span class="number">300</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">INSERT</span> OVERWRITE <span class="keyword">LOCAL</span> <span class="keyword">DIRECTORY</span> <span class="string">'/tmp/dest4.out'</span> <span class="keyword">SELECT</span> src.value <span class="keyword">WHERE</span> src.key &gt;= <span class="number">300</span>;</span><br></pre></td></tr></table></figure>

<p>Hive 只支持等值连接（equality joins）、外连接（outer joins）和（left semi joins）。Hive 不支持所有非等值的连接，因为非等值连接非常难转化到 map/reduce 任务。</p>
<ul>
<li><p>LEFT，RIGHT和FULL OUTER关键字用于处理join中空记录的情况</p>
</li>
<li><p>LEFT SEMI JOIN 是 IN/EXISTS 子查询的一种更高效的实现</p>
</li>
<li><p>join 时，每次 map/reduce 任务的逻辑是这样的：reducer 会缓存 join 序列中除了最后一个表的所有表的记录，再通过最后一个表将结果序列化到文件系统</p>
</li>
<li><p>实际应用过程中应尽量使用小表join大表</p>
</li>
</ul>
<h5 id="join查询时应注意的点："><a href="#join查询时应注意的点：" class="headerlink" title="join查询时应注意的点："></a>join查询时应注意的点：</h5><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment">--只支持等值连接</span></span><br><span class="line"><span class="keyword">SELECT</span> a.* <span class="keyword">FROM</span> a <span class="keyword">JOIN</span> b <span class="keyword">ON</span> (a.id = b.id)</span><br><span class="line"></span><br><span class="line"><span class="keyword">SELECT</span> a.* <span class="keyword">FROM</span> a <span class="keyword">JOIN</span> b <span class="keyword">ON</span> (a.id = b.id <span class="keyword">AND</span> a.department = b.department)</span><br><span class="line"></span><br><span class="line">   </span><br><span class="line"></span><br><span class="line"><span class="comment">--可以 join 多个表</span></span><br><span class="line"><span class="keyword">SELECT</span> a.val, b.val, c.val <span class="keyword">FROM</span> a <span class="keyword">JOIN</span> b</span><br><span class="line"><span class="keyword">ON</span> (a.key = b.key1) <span class="keyword">JOIN</span> c <span class="keyword">ON</span> (c.key = b.key2)</span><br><span class="line"></span><br><span class="line">   </span><br><span class="line"></span><br><span class="line"><span class="comment">--如果join中多个表的 join key 是同一个，则 join 会被转化为单个 map/reduce 任务</span></span><br><span class="line"></span><br><span class="line">   </span><br><span class="line"></span><br><span class="line"><span class="comment">-- LEFT，RIGHT和FULL OUTER关键字</span></span><br><span class="line"></span><br><span class="line"><span class="comment">--左外连接</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">SELECT</span> a.val, b.val <span class="keyword">FROM</span> a <span class="keyword">LEFT</span> <span class="keyword">OUTER</span> <span class="keyword">JOIN</span> b <span class="keyword">ON</span> (a.key=b.key)</span><br><span class="line"></span><br><span class="line"><span class="comment">--右外链接</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">SELECT</span> a.val, b.val <span class="keyword">FROM</span> a <span class="keyword">RIGHT</span> <span class="keyword">OUTER</span> <span class="keyword">JOIN</span> b <span class="keyword">ON</span> (a.key=b.key)</span><br><span class="line"></span><br><span class="line"><span class="comment">--满外连接</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">SELECT</span> a.val, b.val <span class="keyword">FROM</span> a <span class="keyword">FULL</span> <span class="keyword">OUTER</span> <span class="keyword">JOIN</span> b <span class="keyword">ON</span> (a.key=b.key)</span><br><span class="line"></span><br><span class="line">   </span><br><span class="line"></span><br><span class="line"><span class="comment">-- LEFT SEMI JOIN关键字</span></span><br><span class="line"></span><br><span class="line"><span class="comment">--LEFT SEMI JOIN 的限制是， JOIN 子句中右边的表只能在 ON 子句中设置过滤条件，在 WHERE 子句、SELECT 子句或其他地方过滤都不行</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">SELECT</span> a.key, a.value</span><br><span class="line"></span><br><span class="line"><span class="keyword">FROM</span> a</span><br><span class="line"></span><br><span class="line"><span class="keyword">WHERE</span> a.key <span class="keyword">in</span></span><br><span class="line"></span><br><span class="line">(<span class="keyword">SELECT</span> b.key</span><br><span class="line"></span><br><span class="line"><span class="keyword">FROM</span> B);</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">--可以被写为：</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">SELECT</span> a.key, a.val</span><br><span class="line"></span><br><span class="line"><span class="keyword">FROM</span> a <span class="keyword">LEFT</span> <span class="keyword">SEMI</span> <span class="keyword">JOIN</span> b <span class="keyword">on</span> (a.key = b.key)</span><br><span class="line"></span><br><span class="line">   </span><br><span class="line"></span><br><span class="line"><span class="keyword">UNION</span> 与 <span class="keyword">UNION</span> <span class="keyword">ALL</span></span><br><span class="line"></span><br><span class="line"><span class="comment">--用来合并多个select的查询结果，需要保证select中字段须一致</span></span><br><span class="line"></span><br><span class="line">select_statement <span class="keyword">UNION</span> <span class="keyword">ALL</span> select_statement <span class="keyword">UNION</span> <span class="keyword">ALL</span> select_statement ...</span><br><span class="line"></span><br><span class="line"><span class="comment">--UNION 和 UNION ALL的区别</span></span><br><span class="line"></span><br><span class="line"><span class="comment">--UNION只会查询到两个表中不同的数据，相同的部分不会被查出</span></span><br><span class="line"></span><br><span class="line"><span class="comment">--UNION ALL会把两个表的所有数据都查询出</span></span><br></pre></td></tr></table></figure>

]]></content>
      <categories>
        <category>大数据</category>
        <category>大数据之数据计算</category>
        <category>Hive</category>
        <category>Hive SQL语法总结</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>Hive</tag>
      </tags>
  </entry>
  <entry>
    <title>Kafka压力测试</title>
    <url>/2020/03/07/Kafka%E5%8E%8B%E5%8A%9B%E6%B5%8B%E8%AF%95/</url>
    <content><![CDATA[<h2 id="项目经验之-Kafka-压力测试"><a href="#项目经验之-Kafka-压力测试" class="headerlink" title="项目经验之 Kafka 压力测试"></a>项目经验之 Kafka 压力测试</h2><p>1）Kafka 压测<br>用 Kafka 官方自带的脚本，对 Kafka 进行压测。Kafka 压测时，可以查看到哪个地方出现了瓶颈（CPU，内存，网络 IO）。一般都是网络 IO 达到瓶颈。<br>kafka-consumer-perf-test.sh<br>kafka-producer-perf-test.sh</p>
<p>2）Kafka Producer 压力测试</p>
<p>（1）在/opt/module/kafka/bin 目录下面有这两个文件。我们来测试一下</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[leinuo@hadoop102 kafka]$ bin/kafka-producer-perf-test.sh --topic test --record-size 100 --num-records 100000 --throughput -1 --producer-props bootstrap.servers=hadoop102:9092,hadoop103:9092,hadoop104:9092</span><br></pre></td></tr></table></figure>

<p>说明：</p>
<p>record-size 是一条信息有多大，单位是字节。</p>
<p>num-records 是总共发送多少条信息。<br>throughput 是每秒多少条信息，设成-1，表示不限流，可测出生产者最大吞吐量。</p>
<p>（2）Kafka 会打印下面的信息</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">100000 records sent, 24740.227610 records/sec (2.36 MB/sec), 1183.89 ms avg latency, 1622.00 ms max latency, 1367 ms 50th, 1553 ms 95th, 1593 ms 99th, 1617 ms 99.9th.</span><br></pre></td></tr></table></figure>

<p>参数解析：本例中一共写入 10w 条消息，吞吐量为 2.36 MB/sec，每次写入的平均延迟为 1183.89 毫秒，最大的延迟为 1622.00 毫秒。</p>
<p>3）Kafka Consumer 压力测试</p>
<p>Consumer 的测试，如果这四个指标（IO，CPU，内存，网络）都不能改变，<strong>考虑增加分区数来提升性能。</strong></p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">[leinuo@hadoop102 kafka]$ bin/kafka-consumer-perf-test.sh --zookeeper hadoop102:2181 --topic test --fetch-size 10000 --messages 10000000 --threads 1</span><br></pre></td></tr></table></figure>

<p>参数说明：<br>–zookeeper 指定 zookeeper 的链接信息<br>–topic 指定 topic 的名称<br>–fetch-size 指定每次 fetch 的数据的大小<br>–messages 总共要消费的消息个数<br>测试结果说明：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">start.time, end.time, data.consumed.in.MB, MB.sec, data.consumed.in.nMsg, nMsg.sec</span><br><span class="line">2019-02-19 20:29:07:566, 2019-02-19 20:29:12:170, 9.5368, 2.0714,100010,21722.4153</span><br><span class="line">开始测试时间，测试结束数据，共消费数据 9.5368MB，吞吐量 2.0714MB/s ，共消费100010 条，平均每秒消费 21722.4153 条。</span><br></pre></td></tr></table></figure>

<h3 id="项目之-经验之-Kafka-机器数量"><a href="#项目之-经验之-Kafka-机器数量" class="headerlink" title="项目之 经验之 Kafka 机器数量"></a>项目之 经验之 Kafka 机器数量</h3><p>Kafka 机器数量（经验公式）=2 X（峰值生产速度 X 副本数/100）+1<br>先拿到峰值生产速度，再根据设定的副本数，就能预估出需要部署 Kafka 的数量。<br>比如我们的峰值生产速度是 50M/s。副本数为 2。<br>Kafka 机器数量=2<em>（50</em>2/100）+ 1=3 台</p>
<h3 id="使用kafka-server-stop-sh命令关闭kafka服务，发现无法停止"><a href="#使用kafka-server-stop-sh命令关闭kafka服务，发现无法停止" class="headerlink" title="使用kafka-server-stop.sh命令关闭kafka服务，发现无法停止"></a>使用kafka-server-stop.sh命令关闭kafka服务，发现无法停止</h3><p>主要是因为zookeeper停止后，kafka就不会再继续运行了</p>
<p>所以可以 kill -9 强制关闭 </p>
<p> <strong>下面修改kafka-server-stop.sh</strong><br>将 </p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">PIDS=$(ps ax | grep -i 'kafka\.Kafka' | grep java | grep -v grep | awk '&#123;print $1&#125;')</span><br></pre></td></tr></table></figure>

<p>修改为</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">PIDS=$(jps -lm | grep -i 'kafka.Kafka'| awk '&#123;print $1&#125;')</span><br></pre></td></tr></table></figure>

<p> <strong>命令详解：</strong>使用jps -lm命令列出所有的java进程，然后通过管道，利用grep -i ‘kafka.Kafka’命令将kafka进程筛出来，最后再接一管道命令，利用awk将进程号取出来。 </p>
<p>而kafka-server-stop.sh 里的kill 不是强制关闭，kill -s 比较温和 不容易数据丢失</p>
]]></content>
      <categories>
        <category>大数据</category>
        <category>大数据项目经验</category>
        <category>Kafka</category>
        <category>Kafka压力测试</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>Kafka</tag>
      </tags>
  </entry>
  <entry>
    <title>Kafka Eagle安装及使用简介</title>
    <url>/2020/02/10/KafkaEagle%E5%AE%89%E8%A3%85%E5%8F%8A%E4%BD%BF%E7%94%A8%E7%AE%80%E4%BB%8B/</url>
    <content><![CDATA[<h2 id="Kafka-Eagle安装及使用简介"><a href="#Kafka-Eagle安装及使用简介" class="headerlink" title="Kafka Eagle安装及使用简介"></a>Kafka Eagle安装及使用简介</h2><h3 id="1-安装包拷贝至服务器解压安装包"><a href="#1-安装包拷贝至服务器解压安装包" class="headerlink" title="1.安装包拷贝至服务器解压安装包"></a>1.安装包拷贝至服务器解压安装包</h3><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">tar -zxvf kafka-eagle-bin-1.2.4.tar.gz -C /opt/module</span><br></pre></td></tr></table></figure>

<h3 id="2-设置全局变量，设置JDK的全局变量JAVA-HOME和本应用的KE-HOME"><a href="#2-设置全局变量，设置JDK的全局变量JAVA-HOME和本应用的KE-HOME" class="headerlink" title="2. 设置全局变量，设置JDK的全局变量JAVA_HOME和本应用的KE_HOME"></a>2. 设置全局变量，设置JDK的全局变量JAVA_HOME和本应用的KE_HOME</h3><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">--使用root权限编辑环境变量</span><br><span class="line">sudo vi /etc/profile</span><br><span class="line"> </span><br><span class="line">--打开后加上</span><br><span class="line"> </span><br><span class="line">export JAVA_HOME=/opt/module/jdk1.8.0_181</span><br><span class="line">export KE_HOME=/kafka/kafka-eagle-bin-1.2.4/kafka-eagle-web-1.2.4</span><br><span class="line">export PATH=$PATH:$KE_HOME/bin:$JAVA_HOME/bin</span><br><span class="line"></span><br><span class="line">source /etc/profile</span><br></pre></td></tr></table></figure>

<h3 id="3-进入kafka-eagle的conf目录下修改配置文件，配置基础配置"><a href="#3-进入kafka-eagle的conf目录下修改配置文件，配置基础配置" class="headerlink" title="3.进入kafka-eagle的conf目录下修改配置文件，配置基础配置"></a>3.进入kafka-eagle的conf目录下修改配置文件，配置基础配置</h3><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">修改配置文件</span></span><br><span class="line">vim system-config.properties</span><br><span class="line"> </span><br><span class="line"><span class="meta">#</span><span class="bash">配置文件详情介绍</span></span><br><span class="line"> </span><br><span class="line"><span class="meta">#</span><span class="bash"> * kafkazookeeper节点配置属性多个可以添加一个，cluster1 </span></span><br><span class="line">kafka.eagle.zk.cluster.alias=cluster1</span><br><span class="line">cluster1.zk.list=hadoop102:2181 </span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">#####################################</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> * zk 线程数量</span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">#####################################</span></span></span><br><span class="line">kafka.zk.limit.size=25</span><br><span class="line"> </span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">#####################################</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> * kafka eagle 的端口</span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">#####################################</span></span></span><br><span class="line">kafka.eagle.webui.port=8048</span><br><span class="line"> </span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">#####################################</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> kafka offset storage</span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">#####################################</span></span></span><br><span class="line">cluster1.kafka.eagle.offset.storage=kafka</span><br><span class="line"> </span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">#####################################</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> <span class="built_in">enable</span> kafka 开启图表</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 及开始sql查询</span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">#####################################</span></span></span><br><span class="line">kafka.eagle.metrics.charts=true</span><br><span class="line"> </span><br><span class="line">kafka.eagle.sql.fix.error=true</span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">#####################################</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 提醒的email</span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">#####################################</span></span></span><br><span class="line">kafka.eagle.mail.enable=true</span><br><span class="line">kafka.eagle.mail.sa=alert_sa</span><br><span class="line">kafka.eagle.mail.username=alert_sa@163.com</span><br><span class="line">kafka.eagle.mail.password=mqslimczkdqabbbh</span><br><span class="line">kafka.eagle.mail.server.host=smtp.163.com</span><br><span class="line">kafka.eagle.mail.server.port=25</span><br><span class="line"> </span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">#####################################</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 删除kafka topic 的token</span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">#####################################</span></span></span><br><span class="line">kafka.eagle.topic.token=keadmin</span><br><span class="line"> </span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">#####################################</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> kafka sasl authenticate</span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">#####################################</span></span></span><br><span class="line">kafka.eagle.sasl.enable=false</span><br><span class="line">kafka.eagle.sasl.protocol=SASL_PLAINTEXT</span><br><span class="line">kafka.eagle.sasl.mechanism=PLAIN</span><br><span class="line"> </span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">#####################################</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> kafka jdbc 地址注意可以自己安装数据mysql也可以自带的</span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">#####################################</span></span></span><br><span class="line">kafka.eagle.driver=org.sqlite.JDBC</span><br><span class="line">kafka.eagle.url=jdbc:sqlite:/kafka/kafka-eagle-bin-1.2.4/kafka-eagle-web-1.2.4/db/ke.db</span><br><span class="line">kafka.eagle.username=root</span><br><span class="line">kafka.eagle.password=smartloli</span><br></pre></td></tr></table></figure>

<h3 id="4-文件配置完毕后开始启动-，启动要进入bin目录下"><a href="#4-文件配置完毕后开始启动-，启动要进入bin目录下" class="headerlink" title="4.文件配置完毕后开始启动 ，启动要进入bin目录下"></a>4.文件配置完毕后开始启动 ，启动要进入bin目录下</h3><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">进入bin目录后会看到 ke.sh 文件先修改文件的权限</span></span><br><span class="line">sudo chomd 777 ke.sh</span><br><span class="line"><span class="meta">#</span><span class="bash">启动命令</span></span><br><span class="line">./ke.sh start</span><br></pre></td></tr></table></figure>

<p> 启动成功后的控制台输出 </p>
<p><img src="https://leinuo-blog.oss-cn-beijing.aliyuncs.com/images/typora/20180926095313859.png" alt="img"> </p>
<p> 部署完成结束 </p>
<p> 我们下面介绍下他的页面功能 </p>
<p> <img src="https://leinuo-blog.oss-cn-beijing.aliyuncs.com/images/typora/20180926095601457.png" alt="img"> </p>
<p> 功能界面 </p>
<p> <img src="https://leinuo-blog.oss-cn-beijing.aliyuncs.com/images/typora/20180926100006424.png" alt="img"> </p>
<p> 我们来着重的说下kafkasql查询的功能其他的功能点点就知道什么意思和怎么使用了 </p>
<p> <img src="https://leinuo-blog.oss-cn-beijing.aliyuncs.com/images/typora/20180926100849641.png" alt="img"> </p>
<p> <img src="https://leinuo-blog.oss-cn-beijing.aliyuncs.com/images/typora/2018092610092585.png" alt="img"> </p>
]]></content>
      <categories>
        <category>大数据</category>
        <category>大数据项目部署</category>
        <category>Kafka</category>
        <category>Kafka Eagle安装及使用简介</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>Kafka</tag>
      </tags>
  </entry>
</search>
